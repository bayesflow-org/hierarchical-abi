{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flat Gaussian with compositional score matching\n",
    "\n",
    "\n",
    "In this notebook, we will use the compositional score matching to learn the posterior of a flat Gaussian model.\n",
    "The problem is defined as follows:\n",
    "- The prior is a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "- The simulator/likelihood is a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "- We have an analytical solution for the posterior.\n",
    "- We set the dimension of the problem to $D=10$."
   ],
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import CompositionalScoreModel, SDE, weighting_function, train_score_model, sde_sampling, adaptive_sampling, probability_ode_solving, langevin_sampling, generate_diffusion_time, count_parameters, euler_maruyama_sampling\n",
    "from problems.gaussian_flat import GaussianProblem, Prior, Simulator, visualize_simulation_output, generate_synthetic_data, \\\n",
    "    sample_posterior, posterior_contraction, analytical_posterior_median"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cpu\")",
   "id": "70c931844e56c698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior()\n",
    "simulator_test = Simulator()\n",
    "\n",
    "# test the simulator\n",
    "prior_test = prior.sample(2)\n",
    "sim_test = simulator_test(prior_test, n_obs=1000)\n",
    "visualize_simulation_output(sim_test['observable'])"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "max_number_of_obs = 1  # larger than one means we condition the score on multiple observations\n",
    "\n",
    "dataset = GaussianProblem(\n",
    "    n_data=10000,\n",
    "    prior=prior,\n",
    "    online_learning=True,\n",
    "    max_number_of_obs=max_number_of_obs\n",
    ")\n",
    "dataset_valid = GaussianProblem(\n",
    "    n_data=batch_size*2,\n",
    "    prior=prior,\n",
    "    max_number_of_obs=max_number_of_obs\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "score_model = CompositionalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_x=prior.D,\n",
    "    hidden_dim=64,\n",
    "    n_blocks=3,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    time_embed_dim=16,\n",
    "    use_film=False,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior\n",
    ")\n",
    "#score_model.name = score_model.name + '_same_sim_budget'\n",
    "count_parameters(score_model)\n",
    "print(score_model.name)\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "ded414838e34143f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 epochs=500, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Mean Train')\n",
    "plt.plot(loss_history[:, 1], label='Mean Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')\n",
    "plt.show()"
   ],
   "id": "ab5eef7cdb5ec27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", weights_only=True, map_location=torch.device(torch_device)))\n",
    "score_model.eval();"
   ],
   "id": "75a70bd9458b2655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize Loss",
   "id": "cb1892f554ec96fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_target = {}\n",
    "loss_list_score = {}\n",
    "loss_list_error_w_global = {}\n",
    "loss_list_error = {}\n",
    "\n",
    "score_model.to(torch_device)\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=100, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        loss_list_target[t.item()] = 0\n",
    "        loss_list_score[t.item()] = 0\n",
    "        loss_list_error_w_global[t.item()] = 0\n",
    "        loss_list_error[t.item()] = 0\n",
    "\n",
    "        for theta_global_batch, _, x_batch in dataloader_valid:\n",
    "            theta_global_batch = theta_global_batch.to(torch_device)\n",
    "            x_batch = x_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon_global = torch.randn_like(theta_global_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_global_batch.shape[0], 1), t,\n",
    "                                  dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            snr = score_model.sde.get_snr(t=t_tensor)\n",
    "            alpha, sigma = score_model.sde.kernel(log_snr=snr)\n",
    "            z_global = alpha * theta_global_batch + sigma * epsilon_global\n",
    "\n",
    "            # predict from perturbed theta\n",
    "            pred_epsilon_global = score_model(theta_global=z_global, time=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score_global = score_model(theta_global=z_global, time=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score_global = score_model.sde.grad_log_kernel(x=z_global,\n",
    "                                                                x0=theta_global_batch, t=t_tensor)\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target_global = -epsilon_global / sigma\n",
    "                pred_target_global = -pred_epsilon_global / sigma\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                target_global = epsilon_global\n",
    "                pred_target_global = pred_epsilon_global\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target_global = alpha*epsilon_global - sigma * theta_global_batch\n",
    "                pred_target_global = alpha*pred_epsilon_global - sigma * theta_global_batch\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target_global = theta_global_batch\n",
    "                pred_target_global = (z_global - pred_epsilon_global * sigma) / alpha\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_target_global - target_global), dim=-1))\n",
    "            loss = loss_global\n",
    "            loss_list_target[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_score_global - true_score_global), dim=-1))\n",
    "            loss = loss_global\n",
    "            loss_list_score[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            w = weighting_function(t_tensor, sde=score_model.sde,\n",
    "                                   weighting_type=score_model.weighting_type, prediction_type=score_model.prediction_type)\n",
    "            loss_global = torch.mean(w * torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_list_error_w_global[t.item()] += loss_global.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss = loss_global\n",
    "            loss_list_error[t.item()] += loss.item()"
   ],
   "id": "3e3644f3daaa5484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_target = pd.DataFrame(loss_list_target.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_global = pd.DataFrame(loss_list_error_w_global.items(), columns=['Time', 'Loss'])\n",
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "# compute snr\n",
    "snr = score_model.sde.get_snr(diffusion_time).cpu()\n",
    "#upper_bound_loss = (np.sqrt(2) + 1) / (std.numpy()**2)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, sharex=True, figsize=(16, 3), tight_layout=True)\n",
    "ax[0].plot(df_target['Time'], np.log(df_target['Loss']), label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], np.log(df_score['Loss']), label='Score Loss')\n",
    "#ax[1].plot(df_score['Time'], df_score['Loss'] / upper_bound_loss, label='Score Loss')\n",
    "ax[1].plot(diffusion_time.cpu(), snr, label='log snr', alpha=0.5)\n",
    "ax[2].plot(df_error_w_global['Time'], np.log(df_error_w_global['Loss']), label='Weighted Loss (as in Optimization)')\n",
    "ax[3].plot(df_error['Time'], np.log(df_error['Loss']), label='Loss on Error')\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Log Loss')\n",
    "    a.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/losses_diffusion_time.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time.cpu(),\n",
    "         weighting_function(diffusion_time, sde=score_model.sde, weighting_type=score_model.weighting_type,\n",
    "                            prediction_type=score_model.prediction_type).cpu(),\n",
    "         label='weighting')\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "75adebcae950de41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_size = 1000  # number of observations\n",
    "valid_prior_global, valid_data = generate_synthetic_data(prior, n_samples=100, data_size=data_size,\n",
    "                                                         normalize=False, random_seed=0)\n",
    "param_names = ['$D_{' + str(i+1) + '}$' for i in range(prior.D)]\n",
    "n_post_samples = 100\n",
    "score_model.current_number_of_obs = 1\n",
    "score_model.sde.s_shift_cosine = 0"
   ],
   "id": "28d63d6aa09d9245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(valid_data)",
   "id": "d7da728796fa5bde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_posterior_single = lambda vd: sample_posterior(\n",
    "    vd,\n",
    "    prior_sigma=prior.scale,\n",
    "    sigma=prior.simulator.scale,\n",
    "    n_samples=n_post_samples\n",
    ")\n",
    "posterior_global_samples_true = np.array([sample_posterior_single(vd) for vd in valid_data])"
   ],
   "id": "484b478c3e324105",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_global_samples_true, np.array(valid_prior_global), param_names=param_names)\n",
    "diagnostics.plot_sbc_ecdf(posterior_global_samples_true, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=param_names);"
   ],
   "id": "f20719f971c73408",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t1_value = 1/( data_size //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    'size': 10,\n",
    "    #'damping_factor': lambda t: t1_value + (t0_value - t1_value) * 0.5 * (1 + torch.cos(torch.pi * t)),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * score_model.sde.kernel(log_snr=score_model.sde.get_snr(t))[1],\n",
    "    'damping_factor': lambda t: t1_value,\n",
    "    #'damping_factor': lambda t: t0_value * np.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "}\n",
    "plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "plt.show()"
   ],
   "id": "fd3d0805fc460d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = langevin_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   diffusion_steps=200, langevin_steps=5, step_size_factor=0.05,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "9a0ee345ded1f7d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_langevin_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_langevin_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "118c0de3d3ab0b1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = sde_sampling(score_model, valid_data, n_post_samples=n_post_samples, diffusion_steps=100,\n",
    "                                              method=['euler', 'milstein_grad_free', 'srk1w1'][0],\n",
    "                                           device=torch_device, verbose=True)"
   ],
   "id": "e906aed050d0714b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "874d1359427acaa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = sde_sampling(score_model, valid_data, n_post_samples=n_post_samples, diffusion_steps=100,\n",
    "                                              method=['euler', 'milstein_grad_free', 'srk1w1'][0],\n",
    "                                              mini_batch_arg=mini_batch_arg,\n",
    "                                              device=torch_device, verbose=True)"
   ],
   "id": "1a691225901a6b91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                difference=True, param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "cc9389a3bdf1a30d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                   #e_rel=0.6, #e_abs = 0.01 * np.sqrt(prior.D),\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "ab55fadb21762acc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                difference=True, param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.plot_z_score_contraction(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                            param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/z_score_global_adaptive_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "509691ae9e76856a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = probability_ode_solving(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                         #run_sampling_in_parallel=False,\n",
    "                                                         method=['RK45', 'RK23', 'Radau', 'LSODA'][0],\n",
    "                                                         device=torch_device, verbose=True)"
   ],
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_ode{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_ode{score_model.current_number_of_obs}.png')"
   ],
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step Size for different Grid Sizes\n",
    "\n",
    "- we compare score model with only one condition, and with $k$-conditions\n",
    "- we show that the scaling in the number of needed sampling steps only depends on the Bayesian Units used\n",
    "- error reduces when using more conditions, but since network size stays the same, increases at some point again\n",
    "- we show how mini batching effects the posterior\n",
    "\n",
    "Metrics:\n",
    "- MMD between true and estimated posterior samples\n",
    "- RMSE between the medians of true and estimated posterior samples\n",
    "- Posterior contraction: (1 - var_empirical_posterior / var_prior) / (1 - var_true_posterior / var_prior), and using the mean variances over all parameters"
   ],
   "id": "be293168e0abdda5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def gaussian_kernel(x, y, sigma):\n",
    "    \"\"\"Compute Gaussian kernel between two sets of samples.\"\"\"\n",
    "    x = np.atleast_2d(x)\n",
    "    y = np.atleast_2d(y)\n",
    "    sq_dists = np.sum((x[:, None, :] - y[None, :, :]) ** 2, axis=2)\n",
    "    return np.exp(-sq_dists / (2 * sigma ** 2))\n",
    "\n",
    "def compute_mmd(x, y, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Maximum Mean Discrepancy (MMD) between two sets of samples.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Samples from distribution P, shape (n, d).\n",
    "        y (np.ndarray): Samples from distribution Q, shape (m, d).\n",
    "        sigma (float): Bandwidth for the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated MMD^2 value.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Compute kernel matrices\n",
    "    K_xx = gaussian_kernel(x, x, sigma)\n",
    "    K_yy = gaussian_kernel(y, y, sigma)\n",
    "    K_xy = gaussian_kernel(x, y, sigma)\n",
    "\n",
    "    # Compute MMD^2\n",
    "    mmd_squared = (np.mean(K_xx) + np.mean(K_yy) - 2 * np.mean(K_xy))\n",
    "    return mmd_squared"
   ],
   "id": "2d16e9b8f5efd78f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure we generate enough synthetic data samples.\n",
    "n_samples_data = 10\n",
    "n_post_samples = 100\n",
    "score_model.current_number_of_obs = 1\n",
    "max_steps = 5000\n",
    "\n",
    "# Set up your data sizes and mini-batch parameters.\n",
    "data_sizes = np.array([100, 1000, 10000]) #, 100000, 200000, 250000])  # todo: up to 250.000\n",
    "mini_batch = [1, 10, 100, 1000, None]\n",
    "\n",
    "# List to store results.\n",
    "results = []\n",
    "\n",
    "# Iterate over data sizes.\n",
    "for n in data_sizes:\n",
    "    damping_factor = 1. / (n // score_model.current_number_of_obs)\n",
    "    # Generate synthetic data with enough samples\n",
    "    true_params, test_data = generate_synthetic_data(prior, n_samples=n_samples_data, data_size=n,\n",
    "                                                      normalize=False, random_seed=0)\n",
    "    # Iterate over mini-batch settings.\n",
    "    for mb in mini_batch:\n",
    "        # Skip mini-batch settings that are larger than or equal to the data size.\n",
    "        if mb is not None and mb >= n:\n",
    "            continue\n",
    "\n",
    "        # Run adaptive sampling.\n",
    "        test_samples, list_steps = adaptive_sampling(score_model, test_data, conditions=None,\n",
    "                                                     n_post_samples=n_post_samples,\n",
    "                                                     mini_batch_arg={'size': mb, 'damping_factor': damping_factor} if mb is not None else None,\n",
    "                                                     max_evals=max_steps*2,\n",
    "                                                     t_end=0, random_seed=0, device=torch_device,\n",
    "                                                     return_steps=True)\n",
    "        # Sample the true posterior.\n",
    "        true_samples = np.stack([sample_posterior(x, prior_sigma=prior.scale,\n",
    "                                        sigma=prior.simulator.scale, n_samples=n_post_samples) for x in test_data], axis=0)\n",
    "        true_median = np.array([analytical_posterior_median(x, prior_std=prior.scale, likelihood_std=prior.simulator.scale)\n",
    "                                for x in test_data])\n",
    "\n",
    "        # Compute metrics.\n",
    "        mmd = [compute_mmd(test_samples[i], true_samples[i]) for i in range(n_samples_data)]\n",
    "\n",
    "        median = np.median(test_samples, axis=1)\n",
    "        median_error = median - true_median\n",
    "        rmse = np.sqrt(np.mean(median_error**2, axis=-1))\n",
    "\n",
    "        contractions = 1 - np.var(test_samples, axis=1) / (prior.scale**2)\n",
    "        true_contraction = posterior_contraction(prior_std=prior.scale, likelihood_std=prior.simulator.scale, n_obs=n)\n",
    "        rel_contraction = (contractions / true_contraction[np.newaxis]).mean(axis=-1)\n",
    "\n",
    "        # Print current metrics.\n",
    "        print(f\"Data Size: {n}, Mini Batch: {mb}, \"\n",
    "              f\"MMD: {np.mean(mmd)}, Median RMSE: {np.mean(rmse)}, Relative Contraction: {np.mean(rel_contraction)}\")\n",
    "\n",
    "        # Save results into a dictionary.\n",
    "        for i in range(n_samples_data):\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": i,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"n_steps\": len(list_steps),\n",
    "                \"list_steps\": np.array(list_steps),\n",
    "                \"mmd\": mmd[i],\n",
    "                \"median\": median[i],\n",
    "                \"median_rmse\": rmse[i],\n",
    "                \"contractions\": contractions[i],\n",
    "                \"rel_contraction\": rel_contraction[i]\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the results list.\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(f'plots/{score_model.name}/df_results.csv')"
   ],
   "id": "4936602cfa640068",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by both data_size and mini_batch to compute mean and standard deviation of n_steps.\n",
    "grouped_bar = df_results.groupby(['data_size', 'mini_batch'])['n_steps'].agg(['mean','std']).reset_index()\n",
    "\n",
    "# Determine unique data sizes and mini_batch values.\n",
    "data_sizes = sorted(grouped_bar['data_size'].unique())\n",
    "# Order mini_batch values\n",
    "mini_batch_values = sorted(grouped_bar['mini_batch'].unique())\n",
    "\n",
    "# Set up bar plot parameters.\n",
    "n_groups = len(data_sizes)\n",
    "n_bars = len(mini_batch_values)\n",
    "bar_width = 0.8 / n_bars  # total width is 0.8 for each group\n",
    "x = np.arange(n_groups)  # x locations for groups\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "# Plot a bar for each mini_batch value within each data_size group.\n",
    "for i, mb in enumerate(mini_batch_values):\n",
    "    # Filter the grouped DataFrame for the current mini_batch value.\n",
    "    subset = grouped_bar[grouped_bar['mini_batch'] == mb]\n",
    "    means = []\n",
    "    stds = []\n",
    "    for ds in data_sizes:\n",
    "        # Find the row corresponding to this data size and mini_batch.\n",
    "        row = subset[subset['data_size'] == ds]\n",
    "        if not row.empty:\n",
    "            means.append(row['mean'].values[0])\n",
    "            stds.append(row['std'].values[0])\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            stds.append(0)\n",
    "\n",
    "    # Define label\n",
    "    label = f'Mini Batch {int(mb)}'\n",
    "    # Compute positions for the bars.\n",
    "    positions = x + i * bar_width\n",
    "    ax.bar(positions, means, width=bar_width, yerr=stds, capsize=5, label=label)\n",
    "\n",
    "# Set the x-axis ticks and labels so that groups are centered.\n",
    "ax.set_xticks(x + bar_width*(n_bars-1)/2)\n",
    "ax.set_xticklabels(data_sizes)\n",
    "ax.set_xlabel('Data Size')\n",
    "ax.set_ylabel('Number of Steps')\n",
    "ax.set_title('Number of Steps by Data Size and Mini Batch')\n",
    "#ax.set_yscale('log')\n",
    "ax.legend(title='Mini Batch')\n",
    "plt.show()"
   ],
   "id": "c9e04f9642e2b9a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------------------\n",
    "# Plot 1: Bar plot of n_steps for the full-batch  case.\n",
    "# ------------------------------\n",
    "\n",
    "# Filter the full-batch rows\n",
    "df_full = df_results[df_results['data_size'] == df_results['mini_batch']]\n",
    "\n",
    "# Group by data_size and compute mean and standard deviation of n_steps.\n",
    "grouped_full = df_full.groupby('data_size')['n_steps'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "plt.bar(grouped_full['data_size'], grouped_full['mean'],\n",
    "        yerr=grouped_full['std'], capsize=5, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Data Size')\n",
    "plt.ylabel('Number of Steps')\n",
    "plt.title('Number of Steps (Full Batch) per Data Size')\n",
    "plt.xticks(grouped_full['data_size'])\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 2: Errorbar plots for MMD, RMSE, and Contraction vs. mini batch size.\n",
    "# ------------------------------\n",
    "\n",
    "# Filter rows with a mini_batch value (skip full-batch rows).\n",
    "df_mb = df_results[df_results['mini_batch'].notnull()].copy()\n",
    "# Convert mini_batch to float (if not already) to allow proper plotting on the x-axis.\n",
    "df_mb['mini_batch'] = df_mb['mini_batch'].astype(float)\n",
    "\n",
    "# Define the metrics to plot: key is dataframe column, value is label for y-axis.\n",
    "metrics = {\n",
    "    'mmd': 'MMD',\n",
    "    'median_rmse': 'RMSE (Posterior Median)',\n",
    "    'rel_contraction': 'Relative Posterior Contraction'\n",
    "}\n",
    "\n",
    "# Identify the unique data sizes (to plot different lines per data size).\n",
    "unique_data_sizes = sorted(df_mb['data_size'].unique())\n",
    "\n",
    "# Create one figure per metric.\n",
    "for metric, metric_label in metrics.items():\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    for ds in unique_data_sizes:\n",
    "        # Select the rows for this particular data size.\n",
    "        df_sub = df_mb[df_mb['data_size'] == ds]\n",
    "        # Group by mini_batch size to get mean and std of the metric.\n",
    "        grouped = df_sub.groupby('mini_batch')[metric].agg(['mean', 'std']).reset_index()\n",
    "        plt.errorbar(grouped['mini_batch'], grouped['mean'], yerr=grouped['std'],\n",
    "                     marker='o', capsize=5, label=f'{ds}')\n",
    "    plt.xlabel('Mini Batch Size')\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.title(f'{metric_label} vs Mini Batch Size')\n",
    "    plt.legend(title='Data Size')\n",
    "    # Using a logarithmic scale for the x-axis since mini-batch sizes vary widely.\n",
    "    plt.xscale('log')\n",
    "    plt.show()"
   ],
   "id": "8bb9571ed8915879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Figure 1: Full Batch (mini_batch is None) for data_id 0\n",
    "# ------------------------------\n",
    "# Filter for valid_id 0 and full-batch runs\n",
    "df_full = df_results[(df_results['data_id'] == 0) & (df_results['data_size'] == df_results['mini_batch'])]\n",
    "\n",
    "plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "for ds in sorted(df_full['data_size'].unique()):\n",
    "    # Extract the row for this data_size (should be a single row per combination)\n",
    "    row = df_full[df_full['data_size'] == ds]\n",
    "    if not row.empty:\n",
    "        # Extract the list of step sizes (assumed to be a list or array)\n",
    "        steps_list = row.iloc[0]['list_steps']\n",
    "        # Plot step size vs iteration\n",
    "        plt.plot(range(len(steps_list)), steps_list, label=f\"{ds}\", alpha=.75)\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Step Size\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"Step Size Over Time for Full Batch\")\n",
    "plt.legend(title=\"Data Size\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Figure 2: Mini Batching (mini_batch is not None) for data_id 0\n",
    "# ------------------------------\n",
    "# Filter for data_id 0 and only mini-batch runs.\n",
    "df_mb = df_results[(df_results['data_id'] == 0) & (df_results['mini_batch'].notnull())]\n",
    "\n",
    "# Get the sorted unique mini_batch values\n",
    "mini_batch_values = sorted(df_mb['mini_batch'].unique())\n",
    "n_subplots = len(mini_batch_values)\n",
    "\n",
    "# Create subplots (one per mini_batch value)\n",
    "fig, axes = plt.subplots(1, n_subplots, figsize=(4 * n_subplots, 3), sharey=True, tight_layout=True)\n",
    "\n",
    "# In case there's only one subplot, wrap axes in a list for uniformity.\n",
    "if n_subplots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, mb in zip(axes, mini_batch_values):\n",
    "    # Filter for the current mini_batch value\n",
    "    df_mb_subset = df_mb[df_mb['mini_batch'] == mb]\n",
    "    for ds in sorted(df_mb_subset['data_size'].unique()):\n",
    "        row = df_mb_subset[df_mb_subset['data_size'] == ds]\n",
    "        if not row.empty:\n",
    "            steps_list = row.iloc[0]['list_steps']\n",
    "            ax.plot(range(len(steps_list)), steps_list, label=f\"{ds}\", alpha=.75)\n",
    "\n",
    "    ax.set_title(f\"Step Size Over Time for Mini Batch = {mb}\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Step Size\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(title=\"Data Size\")\n",
    "plt.show()"
   ],
   "id": "1387b4c88ce4a638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up your data sizes and mini-batch parameters.\n",
    "data_sizes = np.array([1, 10, 100, 1000])\n",
    "n_conditions = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "# List to store results.\n",
    "results = []\n",
    "\n",
    "# Iterate over data sizes.\n",
    "for n in data_sizes:\n",
    "    # Generate synthetic data with enough samples\n",
    "    true_params, test_data = generate_synthetic_data(prior, n_samples=n_samples_data, data_size=n,\n",
    "                                                      normalize=False, random_seed=0)\n",
    "    # Iterate over number of conditions\n",
    "    for current_number_of_obs in n_conditions:\n",
    "        if current_number_of_obs > n:\n",
    "            continue\n",
    "        score_model.current_number_of_obs = current_number_of_obs\n",
    "\n",
    "        # Run adaptive sampling.\n",
    "        test_samples, list_steps = adaptive_sampling(score_model, test_data, conditions=None,\n",
    "                                                     n_post_samples=n_post_samples,\n",
    "                                                     mini_batch_arg=None,\n",
    "                                                     max_evals=max_steps*2,\n",
    "                                                     t_end=0, random_seed=0, device=torch_device,\n",
    "                                                     return_steps=True)\n",
    "        # Sample the true posterior.\n",
    "        true_samples = np.stack([sample_posterior(x, prior_sigma=prior.scale,\n",
    "                                        sigma=prior.simulator.scale, n_samples=n_post_samples) for x in test_data], axis=0)\n",
    "        true_median = np.array([analytical_posterior_median(x, prior_std=prior.scale, likelihood_std=prior.simulator.scale)\n",
    "                                for x in test_data])\n",
    "\n",
    "        # Compute metrics.\n",
    "        mmd = [compute_mmd(test_samples[i], true_samples[i]) for i in range(n_samples_data)]\n",
    "\n",
    "        median = np.median(test_samples, axis=1)\n",
    "        median_error = median - true_median\n",
    "        rmse = np.sqrt(np.mean(median_error**2, axis=-1))\n",
    "\n",
    "        contractions = 1 - np.var(test_samples, axis=1) / (prior.scale**2)\n",
    "        true_contraction = posterior_contraction(prior_std=prior.scale, likelihood_std=prior.simulator.scale, n_obs=n)\n",
    "        rel_contraction = (contractions / true_contraction[np.newaxis]).mean(axis=-1)\n",
    "\n",
    "        # Print current metrics.\n",
    "        print(f\"Data Size: {n}, #Conditions: {current_number_of_obs}, \"\n",
    "              f\"MMD: {np.mean(mmd)}, Median RMSE: {np.mean(rmse)}, Relative Contraction: {np.mean(rel_contraction)}\")\n",
    "\n",
    "        # Save results into a dictionary.\n",
    "        for i in range(n_samples_data):\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": i,\n",
    "                \"current_number_of_obs\": current_number_of_obs,\n",
    "                \"n_steps\": len(list_steps),\n",
    "                \"list_steps\": np.array(list_steps),\n",
    "                \"mmd\": mmd[i],\n",
    "                \"median\": median[i],\n",
    "                \"median_rmse\": rmse[i],\n",
    "                \"contractions\": contractions[i],\n",
    "                \"rel_contraction\": rel_contraction[i]\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the results list.\n",
    "df_results_conditions = pd.DataFrame(results)\n",
    "df_results_conditions.to_csv(f'plots/{score_model.name}/df_results_conditions.csv')"
   ],
   "id": "ff0f92e8cf106531",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by both data_size and current_number_of_obs to compute mean and standard deviation of n_steps.\n",
    "grouped_bar = df_results_conditions.groupby(['data_size', 'current_number_of_obs'])['n_steps'].agg(['mean','std']).reset_index()\n",
    "\n",
    "# Determine unique data sizes and current_number_of_obs values.\n",
    "data_sizes = sorted(grouped_bar['data_size'].unique())\n",
    "# Order current_number_of_obs values\n",
    "n_conditions_values = sorted(grouped_bar['current_number_of_obs'].unique())\n",
    "\n",
    "# Set up bar plot parameters.\n",
    "n_groups = len(data_sizes)\n",
    "n_bars = len(n_conditions_values)\n",
    "bar_width = 0.8 / n_bars  # total width is 0.8 for each group\n",
    "x = np.arange(n_groups)  # x locations for groups\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "# Plot a bar for each current_number_of_obs value within each data_size group.\n",
    "for i, mb in enumerate(n_conditions_values):\n",
    "    # Filter the grouped DataFrame for the current current_number_of_obs value.\n",
    "    subset = grouped_bar[grouped_bar['current_number_of_obs'] == mb]\n",
    "    means = []\n",
    "    stds = []\n",
    "    for ds in data_sizes:\n",
    "        # Find the row corresponding to this data size and current_number_of_obs.\n",
    "        row = subset[subset['data_size'] == ds]\n",
    "        if not row.empty:\n",
    "            means.append(row['mean'].values[0])\n",
    "            stds.append(row['std'].values[0])\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            stds.append(0)\n",
    "\n",
    "    # Define label.\n",
    "    label = f'#Conditions {int(mb)}'\n",
    "    # Compute positions for the bars.\n",
    "    positions = x + i * bar_width\n",
    "    ax.bar(positions, means, width=bar_width, yerr=stds, capsize=5, label=label)\n",
    "\n",
    "# Set the x-axis ticks and labels so that groups are centered.\n",
    "ax.set_xticks(x + bar_width*(n_bars-1)/2)\n",
    "ax.set_xticklabels(data_sizes)\n",
    "ax.set_xlabel('Data Size')\n",
    "ax.set_ylabel('Number of Steps')\n",
    "ax.set_title('Number of Steps by Data Size and #Conditions')\n",
    "#ax.set_yscale('log')\n",
    "ax.legend(title='#Conditions')\n",
    "plt.show()"
   ],
   "id": "e9d3473440f9090",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------------------\n",
    "# Plot 2: Errorbar plots for MMD, RMSE, and Contraction vs. mini batch size.\n",
    "# ------------------------------\n",
    "\n",
    "# Filter rows with a current_number_of_obs value (skip full-batch rows).\n",
    "df_mb = df_results_conditions[df_results_conditions['current_number_of_obs'].notnull()].copy()\n",
    "# Convert mini_batch to float (if not already) to allow proper plotting on the x-axis.\n",
    "df_mb['current_number_of_obs'] = df_mb['current_number_of_obs'].astype(float)\n",
    "\n",
    "# Define the metrics to plot: key is dataframe column, value is label for y-axis.\n",
    "metrics = {\n",
    "    'mmd': 'MMD',\n",
    "    'median_rmse': 'RMSE (Posterior Median)',\n",
    "    'rel_contraction': 'Relative Posterior Contraction'\n",
    "}\n",
    "\n",
    "# Identify the unique data sizes (to plot different lines per data size).\n",
    "unique_data_sizes = sorted(df_mb['data_size'].unique())\n",
    "\n",
    "# Create one figure per metric.\n",
    "for metric, metric_label in metrics.items():\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    for ds in unique_data_sizes:\n",
    "        # Select the rows for this particular data size.\n",
    "        df_sub = df_mb[df_mb['data_size'] == ds]\n",
    "        # Group by current_number_of_obs size to get mean and std of the metric.\n",
    "        grouped = df_sub.groupby('current_number_of_obs')[metric].agg(['mean', 'std']).reset_index()\n",
    "        plt.errorbar(grouped['current_number_of_obs'], grouped['mean'], yerr=grouped['std'],\n",
    "                     marker='o', capsize=5, label=f'{ds}')\n",
    "    plt.xlabel('#Conditions')\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.title(f'{metric_label} vs #Conditions')\n",
    "    plt.legend(title='Data Size')\n",
    "    # Using a logarithmic scale for the x-axis since mini-batch sizes vary widely.\n",
    "    plt.xscale('log')\n",
    "    plt.show()"
   ],
   "id": "bf676365a24ab206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Contraction",
   "id": "b55ff4c321fe4226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot contraction factor\n",
    "prior_std = prior.scale\n",
    "likelihood_std = prior.simulator.scale\n",
    "\n",
    "# contraction =  1 - posterior_var / prior_var\n",
    "contractions = [posterior_contraction(prior_std, likelihood_std, n).mean() for n in range(1, 101)]\n",
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "plt.plot(contractions)\n",
    "plt.title('Analytical Posterior Contraction')\n",
    "plt.xlabel('Observations')\n",
    "plt.ylabel('Factor')\n",
    "plt.show()"
   ],
   "id": "fedee06a7f2e4c18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "63b4b50965abd2b3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
