{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flat Gaussian with compositional score matching\n",
    "\n",
    "\n",
    "In this notebook, we will use the compositional score matching to learn the posterior of a flat Gaussian model.\n",
    "The problem is defined as follows:\n",
    "- The prior is a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "- The simulator/likelihood is a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "- We have an analytical solution for the posterior.\n",
    "- We set the dimension of the problem to $D=10$."
   ],
   "id": "85bd336b13322aeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import ScoreModel, SDE, train_score_model, sde_sampling, \\\n",
    "    adaptive_sampling, probability_ode_solving, langevin_sampling\n",
    "from diffusion_model.helper_networks import GaussianFourierProjection\n",
    "from problems.gaussian_flat import GaussianProblem, Prior, Simulator, visualize_simulation_output, \\\n",
    "    generate_synthetic_data, \\\n",
    "    sample_posterior, posterior_contraction, kl_divergence"
   ],
   "id": "a8d358253f4577c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cuda\")",
   "id": "1c384ccdb1a1593d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior()\n",
    "simulator_test = Simulator()\n",
    "\n",
    "# test the simulator\n",
    "prior_test = prior.sample(2)\n",
    "sim_test = simulator_test(prior_test, n_obs=1000)\n",
    "visualize_simulation_output(sim_test['observable'])"
   ],
   "id": "8ce39dd1ed4c87fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "number_of_obs = 1  # multiple obs means we amortize over number of conditions\n",
    "\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "dataset = GaussianProblem(\n",
    "    n_data=10000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    online_learning=True,\n",
    "    number_of_obs=number_of_obs\n",
    ")\n",
    "dataset_valid = GaussianProblem(\n",
    "    n_data=1000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for test in dataloader:\n",
    "    print(test[-2].shape)\n",
    "    break"
   ],
   "id": "2ddcf035d790e88f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "time_embedding = nn.Sequential(\n",
    "    GaussianFourierProjection(8),\n",
    "    nn.Linear(8,8),\n",
    "    nn.Mish()\n",
    ")\n",
    "\n",
    "score_model = ScoreModel(\n",
    "    input_dim_theta=prior.n_params_global,\n",
    "    input_dim_x=10,\n",
    "    time_embedding=time_embedding,\n",
    "    hidden_dim=256,\n",
    "    n_blocks=5,\n",
    "    max_number_of_obs=max(number_of_obs) if isinstance(number_of_obs, list) else number_of_obs,\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior,\n",
    "    name_prefix='test_'\n",
    ")\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "b68c74f062ac0fc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 epochs=1000, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Training', color=\"#132a70\", lw=2.0, alpha=0.9)\n",
    "plt.plot(loss_history[:, 1], label='Validation', linestyle=\"--\", marker=\"o\", color='black')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('Training epoch #')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')"
   ],
   "id": "96114310ae948916",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", weights_only=True,\n",
    "                                       map_location=torch.device(torch_device)))\n",
    "score_model.eval();"
   ],
   "id": "98249e0fb958fd88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "92c880b8eadc167e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_size = 10  # number of observations\n",
    "obs_n_time_steps = 0\n",
    "valid_prior_global, valid_data = generate_synthetic_data(prior, n_samples=100, data_size=data_size,\n",
    "                                                         normalize=False, random_seed=0)\n",
    "param_names = ['$D_{' + str(i+1) + '}$' for i in range(prior.D)]\n",
    "n_post_samples = 100\n",
    "score_model.current_number_of_obs = 1\n",
    "score_model.sde.s_shift_cosine = 0"
   ],
   "id": "a84a190b5ba55794",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(valid_data)",
   "id": "f6c66521f0395fea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_posterior_single = lambda vd: sample_posterior(\n",
    "    vd,\n",
    "    prior_sigma=prior.scale,\n",
    "    sigma=prior.simulator.scale,\n",
    "    n_samples=n_post_samples\n",
    ")\n",
    "posterior_global_samples_true = np.array([sample_posterior_single(vd) for vd in valid_data])"
   ],
   "id": "a64cfec0c5a42723",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_global_samples_true, np.array(valid_prior_global), variable_names=param_names)\n",
    "diagnostics.calibration_ecdf(posterior_global_samples_true, np.array(valid_prior_global),\n",
    "                             difference=True, variable_names=param_names);"
   ],
   "id": "b1f94d38d6f28f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 10\n",
    "t1_value = 0.8 #mini_batch_size /( data_size //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    'size': mini_batch_size,\n",
    "    #'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'damping_factor_prior': 1\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "57a394ded7ecd677",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = langevin_sampling(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                   n_post_samples=n_post_samples,\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   diffusion_steps=300, langevin_steps=5, step_size_factor=0.05,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "45adef55d36baa78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_langevin_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_langevin_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "6ad18b2a9631d7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = sde_sampling(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                              n_post_samples=n_post_samples, diffusion_steps=300,\n",
    "                                              method=['euler', 'milstein_grad_free', 'srk1w1'][1],\n",
    "                                           device=torch_device, verbose=True)"
   ],
   "id": "ede9815fbf9bf4a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "841a18666e1a1f92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = sde_sampling(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                              n_post_samples=n_post_samples, diffusion_steps=1000,\n",
    "                                              method=['euler', 'milstein_grad_free', 'srk1w1'][1],\n",
    "                                              mini_batch_arg=mini_batch_arg,\n",
    "                                              device=torch_device, verbose=True)"
   ],
   "id": "ccc8ad776804ef3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "14d6573897c0ea35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 10\n",
    "t1_value = 0.1 #mini_batch_size /( data_size //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    #'size': mini_batch_size,\n",
    "    'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t)\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "c4d0c6fe061d45bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                   n_post_samples=n_post_samples,\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "320c9c07cae3e323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.z_score_contraction(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                            variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/z_score_global_adaptive_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "diagnostics.calibration_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'].mean()"
   ],
   "id": "9ddbdfdbd2092d74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = probability_ode_solving(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                         n_post_samples=n_post_samples,\n",
    "                                                         #run_sampling_in_parallel=False,\n",
    "                                                         device=torch_device, verbose=True)"
   ],
   "id": "6b2b8290071b60d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_ode{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_ode{score_model.current_number_of_obs}.png')"
   ],
   "id": "7d258bd693fed9a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step Size for different Grid Sizes\n",
    "\n",
    "- we compare score model with only one condition, and with $k$-conditions\n",
    "- we show that the scaling in the number of needed sampling steps only depends on the Bayesian Units used\n",
    "- error reduces when using more conditions, but since network size stays the same, increases at some point again\n",
    "- we show how mini batching effects the posterior\n",
    "\n",
    "Metrics:\n",
    "- KL divergence between true and estimated posterior samples\n",
    "- RMSE between the medians of true and estimated posterior samples\n",
    "- Posterior contraction: (1 - var_empirical_posterior / var_prior) / (1 - var_true_posterior / var_prior), and using the mean variances over all parameters"
   ],
   "id": "79af63ec5f78de3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure we generate enough synthetic data samples.\n",
    "n_samples_data = 100\n",
    "n_post_samples = 100\n",
    "score_model.current_number_of_obs = 1\n",
    "max_steps = 10000\n",
    "variables_of_interest = ['mini_batch', 'cosine_shift', 'damping_factor_t']\n",
    "\n",
    "variables_of_interest.append('n_conditions')\n",
    "variable_of_interest = variables_of_interest[1]\n",
    "print(variable_of_interest)\n",
    "\n",
    "mini_batch = ['10%']\n",
    "n_conditions = [1]\n",
    "cosine_shifts = [0]\n",
    "d_factors = [1]  # using the d factor depending on the mini batch size\n",
    "data_sizes = np.array([1, 10, 100, 1000, 10000, 100000])\n",
    "\n",
    "if variable_of_interest == 'mini_batch':\n",
    "    # Set up your data sizes and mini-batch parameters.\n",
    "    mini_batch = [1, 10, 100, 1000, 10000, None]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest == 'n_conditions':\n",
    "    n_conditions = [1, 5, 10, 20, 50, 100]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest == 'cosine_shift':\n",
    "    cosine_shifts = [0, -1, 1, 2, 5, 10]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest in ['damping_factor', 'damping_factor_prior', 'damping_factor_t']:\n",
    "    d_factors = [0.0001, 0.001, 0.01, 0.1, 0.5, 0.75, 0.9, 1]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "else:\n",
    "    raise ValueError('Unknown variable_of_interest')\n",
    "\n",
    "df_path = f'plots/{score_model.name}/df_results_{variable_of_interest}.csv'\n",
    "#df_path = f'results/gaussian_flat/results_{variable_of_interest}.csv'\n",
    "if os.path.exists(df_path):\n",
    "    # Load CSV\n",
    "    df_results = pd.read_csv(df_path, index_col=0)\n",
    "    # Convert string representations back to lists\n",
    "    #df_results['list_steps'] = df_results['list_steps'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    if variable_of_interest == 'damping_factor_prior':\n",
    "        df_results['damping_factor_prior'] = df_results['damping_factor']\n",
    "    elif variable_of_interest == 'damping_factor_t':\n",
    "        df_results['damping_factor_t'] = df_results['damping_factor']\n",
    "else:\n",
    "    df_results = None"
   ],
   "id": "6e77b7230045ac07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List to store results.\n",
    "results = []\n",
    "reached_max_evals = []\n",
    "\n",
    "# Iterate over data sizes.\n",
    "for n in data_sizes:\n",
    "    # Generate synthetic data with enough samples\n",
    "    true_params, test_data = generate_synthetic_data(prior, n_samples=n_samples_data, data_size=n,\n",
    "                                                     normalize=False, random_seed=0)\n",
    "    true_params = true_params.numpy()\n",
    "    # Iterate over experimental setting\n",
    "    for mb, nc, cs, d_factor in itertools.product(mini_batch, n_conditions, cosine_shifts, d_factors):\n",
    "        # Skip mini-batch settings that are larger than or equal to the data size.\n",
    "        if mb == '10%':\n",
    "            mb = max(int(n * 0.1), 1)\n",
    "        if mb is not None and mb >= n:\n",
    "            continue\n",
    "        if nc > n:\n",
    "            continue\n",
    "\n",
    "        skip = False\n",
    "        for max_reached in reached_max_evals:\n",
    "            if max_reached[0] <= n:  # check if for a smaller data size we already failed\n",
    "                if max_reached[2] == nc and max_reached[3] == cs and max_reached[4] == d_factor:\n",
    "                    # all conditions are the same, only mini batch size is different\n",
    "                    if max_reached[1] is None:\n",
    "                        pass\n",
    "                    elif mb is None or max_reached[1] < mb:\n",
    "                        print(f'smaller mini batch size already failed, skipping {nc}, {cs}')\n",
    "                        skip = True\n",
    "                        break\n",
    "                elif max_reached[2] == nc and max_reached[3] == cs and max_reached[4] < d_factor:\n",
    "                    # all conditions are the same (assuming mini-batching does not change)\n",
    "                    # check if smaller damping factor already failed\n",
    "                    print(f'smaller damping factor already failed, skipping {nc}, {cs}')\n",
    "                    skip = True\n",
    "                    break\n",
    "        if skip:\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": -1,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"damping_factor\": d_factor,\n",
    "                'n_conditions': nc,\n",
    "                'cosine_shift': cs,\n",
    "                \"n_steps\": max_steps,\n",
    "                #\"list_steps\": np.nan,\n",
    "                \"kl\": np.nan,\n",
    "                \"median\": np.nan,\n",
    "                \"median_rmse\": np.nan,\n",
    "                \"c_error\": np.nan,\n",
    "                \"contractions\": np.nan,\n",
    "                \"rel_contraction\": np.nan,\n",
    "            })\n",
    "            df_results = pd.DataFrame(results)\n",
    "            # Convert lists to strings for CSV storage\n",
    "            #df_results['list_steps'] = df_results['list_steps'].apply(lambda x: str(x))\n",
    "            df_results.to_csv(df_path)\n",
    "            continue\n",
    "\n",
    "        print(f\"Data Size: {n}, Mini Batch: {mb}, Conditions: {nc}, Cosine shift: {cs}, Damping Factor: {d_factor}\")\n",
    "        # Set current number of conditions\n",
    "        score_model.current_number_of_obs = nc\n",
    "\n",
    "        # Set cosine shit\n",
    "        score_model.sde.s_shift_cosine = cs\n",
    "\n",
    "        # Damping factor\n",
    "        if variable_of_interest == 'damping_factor_t':\n",
    "            t0_value = 1\n",
    "            t1_value = d_factor\n",
    "            damping_factor = lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t)\n",
    "            if mb is None:\n",
    "                mini_batch_arg = {'damping_factor': damping_factor}\n",
    "            else:\n",
    "                mini_batch_arg = {'size': mb, 'damping_factor': damping_factor}\n",
    "        elif variable_of_interest == 'damping_factor_prior':\n",
    "            damping_factor = lambda t: torch.ones_like(t) * d_factor\n",
    "            if mb is None:\n",
    "                mini_batch_arg = {'damping_factor_prior': damping_factor}\n",
    "            else:\n",
    "                mini_batch_arg = {'size': mb, 'damping_factor_prior': damping_factor}\n",
    "        else:\n",
    "            damping_factor = lambda t: torch.ones_like(t) * d_factor\n",
    "            if mb is None:\n",
    "                mini_batch_arg = {'damping_factor': damping_factor, 'damping_factor_prior': 1}\n",
    "            else:\n",
    "                mini_batch_arg = {'size': mb, 'damping_factor': damping_factor, 'damping_factor_prior': 1}\n",
    "\n",
    "        # Run adaptive sampling.\n",
    "        try:\n",
    "            test_samples, list_steps = adaptive_sampling(score_model, test_data, conditions=None,\n",
    "                                                         obs_n_time_steps=obs_n_time_steps,\n",
    "                                                         n_post_samples=n_post_samples,\n",
    "                                                         mini_batch_arg=mini_batch_arg,\n",
    "                                                         max_evals=max_steps*2,\n",
    "                                                         t_end=0, random_seed=0, device=torch_device,\n",
    "                                                         run_sampling_in_parallel=False,  # can actually be faster\n",
    "                                                         return_steps=True)\n",
    "        except torch.OutOfMemoryError as e:\n",
    "            print(e)\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": -1,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"damping_factor\": d_factor,\n",
    "                'n_conditions': nc,\n",
    "                'cosine_shift': cs,\n",
    "                \"n_steps\": max_steps,\n",
    "                #\"list_steps\": np.nan,\n",
    "                \"kl\": np.nan,\n",
    "                \"median\": np.nan,\n",
    "                \"median_rmse\": np.nan,\n",
    "                \"c_error\": np.nan,\n",
    "                \"contractions\": np.nan,\n",
    "                \"rel_contraction\": np.nan,\n",
    "            })\n",
    "            df_results = pd.DataFrame(results)\n",
    "            # Convert lists to strings for CSV storage\n",
    "            #df_results['list_steps'] = df_results['list_steps'].apply(lambda x: str(x))\n",
    "            df_results.to_csv(df_path)\n",
    "            continue\n",
    "\n",
    "        # Sample the true posterior.\n",
    "        true_samples = np.stack([sample_posterior(x, prior_sigma=prior.scale,\n",
    "                                                  sigma=prior.simulator.scale, n_samples=n_post_samples) for x in test_data], axis=0)\n",
    "\n",
    "        # Compute metrics.\n",
    "        if test_samples.shape[1] > 1:\n",
    "            kl = [kl_divergence(test_data[i], test_samples[i], prior=prior) for i in range(n_samples_data)]\n",
    "        else:\n",
    "            kl = [np.nan for i in range(n_samples_data)]\n",
    "\n",
    "        rmse = diagnostics.root_mean_squared_error(test_samples, true_params)['values'].mean()\n",
    "        c_error = diagnostics.calibration_error(test_samples, true_params)['values'].mean()\n",
    "\n",
    "        contractions = diagnostics.posterior_contraction(test_samples, true_params)['values'].mean()\n",
    "        true_contraction = posterior_contraction(prior_std=prior.scale, likelihood_std=prior.simulator.scale, n_obs=n).mean()\n",
    "        rel_contraction = (contractions / true_contraction)\n",
    "\n",
    "        # Number of steps\n",
    "        if np.isnan(test_samples).any():\n",
    "            n_steps = np.inf\n",
    "            reached_max_evals.append((n, mb, nc, cs, d_factor))\n",
    "        else:\n",
    "            n_steps = np.mean([len(ls) for ls in list_steps])\n",
    "            if n_steps >= max_steps:\n",
    "                # others will also fail to converge\n",
    "                reached_max_evals.append((n, mb, nc, cs, d_factor))\n",
    "\n",
    "        # Print current metrics.\n",
    "        print(f\"KL: {np.mean(kl)}, #Steps: {n_steps}\")\n",
    "\n",
    "        # Save results into a dictionary.\n",
    "        for i in range(n_samples_data):  # might be less than the actual data points because inference failed\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": i,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"damping_factor\": d_factor,\n",
    "                'n_conditions': nc,\n",
    "                'cosine_shift': cs,\n",
    "                \"n_steps\": n_steps,\n",
    "                #\"list_steps\": np.where(np.isnan(list_steps[0]), None, list_steps[0]).tolist(),  # only for the first sample\n",
    "                \"kl\": kl[i],\n",
    "                \"median\": np.median(test_samples, axis=1)[i],\n",
    "                \"median_rmse\": rmse,\n",
    "                \"c_error\": c_error,\n",
    "                \"contractions\": contractions,\n",
    "                \"rel_contraction\": rel_contraction\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the results list. Save intermediate results\n",
    "        df_results = pd.DataFrame(results)\n",
    "        # Convert lists to strings for CSV storage\n",
    "        #df_results['list_steps'] = df_results['list_steps'].apply(lambda x: str(x))\n",
    "        df_results.to_csv(df_path)\n",
    "\n",
    "# Convert string representations back to lists\n",
    "#df_results['list_steps'] = df_results['list_steps'].apply(lambda x: ast.literal_eval(x))"
   ],
   "id": "52ef14e87fcdf1b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "df_dictionary = pd.DataFrame([\n",
    "    {'data_size': 100000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 1.0},\n",
    "    {'data_size': 100000, 'mini_batch': 100, 'n_steps': max_steps, 'damping_factor': 1.0},\n",
    "    {'data_size': 100000, 'mini_batch': 1000, 'n_steps': max_steps, 'damping_factor': 1.0},\n",
    "    {'data_size': 10000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 1.0},\n",
    "    {'data_size': 10000, 'mini_batch': 100, 'n_steps': max_steps, 'damping_factor': 1.0},\n",
    "    {'data_size': 100000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 0.9},\n",
    "    {'data_size': 10000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 0.9},\n",
    "    {'data_size': 100000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 0.75},\n",
    "    {'data_size': 10000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 0.75},\n",
    "    {'data_size': 100000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 0.5},\n",
    "    {'data_size': 10000, 'mini_batch': 10, 'n_steps': max_steps, 'damping_factor': 0.5},\n",
    "])\n",
    "df_results = pd.concat([df_results, df_dictionary], ignore_index=True)"
   ],
   "id": "fc8637ea3cb4d1d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colors = ['#a6cee3', '#1f77b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a']\n",
    "\n",
    "metrics = {\n",
    "    'kl': 'KL Divergence',\n",
    "    'median_rmse': 'RMSE',\n",
    "    'rel_contraction': 'Relative Posterior Contraction',\n",
    "    'c_error': 'Calibration Error'\n",
    "}\n",
    "\n",
    "experiment_names = {\n",
    "    'damping_factor': 'Damping Factor',\n",
    "    'damping_factor_t': 'Damping Factor\\nTime Dependent',\n",
    "    'damping_factor_prior': 'Damping Factor Prior',\n",
    "    'n_conditions': 'Number of Conditions',\n",
    "    'cosine_shift': 'Cosine Shift',\n",
    "    'data_size': 'Data Size',\n",
    "    'mini_batch': 'Mini Batch Size'\n",
    "}"
   ],
   "id": "8f08384b1c55ae5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by both second_variable_of_interest and variable_of_interest to compute mean and standard deviation of n_steps.\n",
    "grouped_bar = df_results.groupby([second_variable_of_interest, variable_of_interest])['n_steps'].agg(['mean','std']).reset_index()\n",
    "\n",
    "# Determine unique second_variable_of_interest and variable_of_interest values.\n",
    "second_variable_of_interest_values = sorted(grouped_bar[second_variable_of_interest].unique())\n",
    "variable_batch_values = sorted(grouped_bar[variable_of_interest].unique())\n",
    "\n",
    "# Set up errorbar plot parameters.\n",
    "n_groups = len(second_variable_of_interest_values)\n",
    "n_series = len(variable_batch_values)\n",
    "x = np.arange(n_groups)  # base x locations for groups\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "# Plot an errorbar for each variable_of_interest value within each second_variable_of_interest group.\n",
    "for i, mb in enumerate(variable_batch_values):\n",
    "    subset = grouped_bar[grouped_bar[variable_of_interest] == mb]\n",
    "    means = []\n",
    "    stds = []\n",
    "    for ds in second_variable_of_interest_values:\n",
    "        row = subset[subset[second_variable_of_interest] == ds]\n",
    "        if not row.empty:\n",
    "            means.append(row['mean'].values[0])\n",
    "            stds.append(row['std'].values[0])\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            stds.append(0)\n",
    "\n",
    "    # Use 'o-' for markers connected by lines.\n",
    "    ax.errorbar(x, means, yerr=stds, fmt='o-', capsize=5, label=f'{mb}', alpha=0.75, color=colors[i])\n",
    "\n",
    "ax.axhline(max_steps, color='k', linestyle='--')\n",
    "ax.text(0.1, max_steps-3500, f\"Maximal Number of Steps\", fontsize=8, color='k')\n",
    "\n",
    "# Center the x-axis ticks and label them.\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(second_variable_of_interest_values)\n",
    "ax.set_xlabel(experiment_names[second_variable_of_interest])\n",
    "ax.set_ylabel('Number of Steps')\n",
    "#ax.set_title(f'Number of Steps by {experiment_names[second_variable_of_interest]} and {experiment_names[variable_of_interest]}')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(title=experiment_names[variable_of_interest], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_n_steps.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "7811e58d362cf68e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if variable_batch_values == 'mini_batch_size':\n",
    "    # ------------------------------\n",
    "    # Plot 1: Bar plot of n_steps for the full-batch  case.\n",
    "    # ------------------------------\n",
    "\n",
    "    # Filter the full-batch rows\n",
    "    df_full = df_results[df_results['data_size'] == df_results['mini_batch']]\n",
    "\n",
    "    # Group by data_size and compute mean and standard deviation of n_steps.\n",
    "    grouped_full = df_full.groupby('data_size')['n_steps'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    plt.bar(grouped_full['data_size'], grouped_full['mean'],\n",
    "            yerr=grouped_full['std'], capsize=5, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Number of Steps')\n",
    "    plt.title('Number of Steps (Full Batch) per Data Size')\n",
    "    plt.xticks(grouped_full['data_size'])\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_steps_full_batch.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 2: Errorbar plots for MMD, RMSE, and Contraction vs. variable_of_interest.\n",
    "# ------------------------------\n",
    "\n",
    "# Filter rows with a variable_of_interest value (skip full-batch rows).\n",
    "df_mb = df_results[df_results[variable_of_interest].notnull()].copy()\n",
    "# Convert mini_batch to float (if not already) to allow proper plotting on the x-axis.\n",
    "df_mb[variable_of_interest] = df_mb[variable_of_interest].astype(float)\n",
    "\n",
    "# Define the metrics to plot: key is dataframe column, value is label for y-axis.\n",
    "y_limits = {\n",
    "    #'kl': (0, 100),\n",
    "    'median_rmse': (-0.1, 1),\n",
    "    'rel_contraction': (-0.1, 1.2),\n",
    "    'c_error': (-0.05, 0.55)\n",
    "}\n",
    "\n",
    "# Identify the unique data sizes (to plot different lines per data size).\n",
    "unique_second_variable_of_interest = sorted(df_mb[second_variable_of_interest].unique())\n",
    "\n",
    "# Create one figure per metric.\n",
    "for metric, metric_label in metrics.items():\n",
    "    plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "    for i, ds in enumerate(unique_second_variable_of_interest):\n",
    "        # Select the rows for this particular data size.\n",
    "        df_sub = df_mb[(df_mb[second_variable_of_interest] == ds) & (df_mb['n_steps'] != max_steps)]\n",
    "        # Group by variable_of_interest size to get mean and std of the metric.\n",
    "        grouped = df_sub.groupby(variable_of_interest)[metric].agg(['mean', 'std']).reset_index()\n",
    "        if not np.isfinite(grouped['mean']).all() or grouped.empty:\n",
    "            continue\n",
    "        plt.errorbar(grouped[variable_of_interest], grouped['mean'], yerr=grouped['std'],\n",
    "                     marker='o', capsize=5, label=f'{ds}', alpha=0.75, color=colors[i])\n",
    "    plt.xlabel(experiment_names[variable_of_interest])\n",
    "    plt.ylabel(metric_label)\n",
    "    #plt.title(f'{metric_label} vs {experiment_names[variable_of_interest]}')\n",
    "    plt.legend(title=experiment_names[second_variable_of_interest], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    # Using a logarithmic scale for the x-axis since mini-batch sizes vary widely.\n",
    "    if metric == 'rel_contraction':\n",
    "        plt.axhline(1, linestyle='--', color='k')\n",
    "        if variable_of_interest == 'mini_batch':\n",
    "            plt.text(1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "        elif variable_of_interest == 'n_conditions':\n",
    "            plt.text(1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "        elif variable_of_interest == 'cosine_shift':\n",
    "            plt.text(1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "        elif variable_of_interest == 'damping_factor_t':\n",
    "            plt.text(0.1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "\n",
    "    if variable_of_interest == 'mini_batch':\n",
    "        plt.xscale('log')\n",
    "    if metric == 'kl':\n",
    "        plt.yscale('log')\n",
    "    else:\n",
    "        plt.ylim(y_limits[metric])\n",
    "    plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_{metric}.png', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "255202e619229f14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "if variable_of_interest == 'mini_batch':\n",
    "    # Figure 1: Full Batch (mini_batch is None) for data_id 0\n",
    "    # ------------------------------\n",
    "    # Filter for valid_id 0 and full-batch runs\n",
    "    df_full = df_results[(df_results['data_id'] == 0) & (df_results['data_size'] == df_results['mini_batch'])]\n",
    "\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    for ds in sorted(df_full['data_size'].unique()):\n",
    "        # Extract the row for this data_size (should be a single row per combination)\n",
    "        row = df_full[df_full['data_size'] == ds]\n",
    "        if not row.empty:\n",
    "            # Extract the list of step sizes (assumed to be a list or array)\n",
    "            steps_list = row.iloc[0]['list_steps']\n",
    "            # Plot step size vs iteration\n",
    "            plt.plot(range(len(steps_list)), steps_list, label=f\"{ds}\", alpha=.75)\n",
    "\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Step Size\")\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Step Size Over Time for Full Batch\")\n",
    "    plt.legend(title=\"Data Size\")\n",
    "    plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_full_batch_step_size.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Figure 2\n",
    "# ------------------------------\n",
    "# Filter for data_id 0 and only mini-batch runs.\n",
    "df_mb = df_results[(df_results['data_id'] == 0) & (df_results[variable_of_interest].notnull())]\n",
    "\n",
    "# Get the sorted unique mini_batch values\n",
    "variable_batch_values = sorted(df_mb[variable_of_interest].unique())\n",
    "n_subplots = len(variable_batch_values)\n",
    "\n",
    "# Create subplots (one per mini_batch value)\n",
    "fig, axes = plt.subplots(1, n_subplots, figsize=(4 * n_subplots, 3), sharey=True, tight_layout=True)\n",
    "\n",
    "# In case there's only one subplot, wrap axes in a list for uniformity.\n",
    "if n_subplots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "group_unique = df_mb[second_variable_of_interest].unique()\n",
    "for ax, mb in zip(axes, variable_batch_values):\n",
    "    # Filter for the current mini_batch value\n",
    "    df_mb_subset = df_mb[df_mb[variable_of_interest] == mb]\n",
    "    for i, ds in enumerate(sorted(group_unique)):\n",
    "        row = df_mb_subset[df_mb_subset[second_variable_of_interest] == ds]\n",
    "        if not row.empty:\n",
    "            steps_list = row.iloc[0]['list_steps']\n",
    "            ax.plot(range(len(steps_list)), steps_list, label=f\"{ds}\", color=colors[i])\n",
    "\n",
    "    ax.set_title(f\"{experiment_names[variable_of_interest]} = {mb}\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Step Size\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(title=experiment_names[second_variable_of_interest], loc='lower right')\n",
    "plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_step_size.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "5862da768354824a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Number of Steps",
   "id": "c51ad2fe2c97dd5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "variable_of_interest = 'mini_batch'\n",
    "df_path = f'results/gaussian_flat/results_{variable_of_interest}.csv'\n",
    "if os.path.exists(df_path):\n",
    "    # Load CSV\n",
    "    df_results_mini = pd.read_csv(df_path, index_col=0)\n",
    "    # Convert string representations back to lists\n",
    "    #df_results_mini['list_steps'] = df_results_mini['list_steps'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    if variable_of_interest == 'damping_factor_prior':\n",
    "        df_results_mini['damping_factor_prior'] = df_results_mini['damping_factor']\n",
    "    elif variable_of_interest == 'damping_factor_t':\n",
    "        df_results_mini['damping_factor_t'] = df_results_mini['damping_factor']\n",
    "else:\n",
    "    df_results_mini = None\n",
    "\n",
    "variable_of_interest = 'n_conditions'\n",
    "df_path2 = f'results/gaussian_flat/results_{variable_of_interest}.csv'\n",
    "if os.path.exists(df_path2):\n",
    "    # Load CSV\n",
    "    df_results_cond = pd.read_csv(df_path2, index_col=0)\n",
    "    # Convert string representations back to lists\n",
    "    #df_results_cond['list_steps'] = df_results_cond['list_steps'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "    if variable_of_interest == 'damping_factor_prior':\n",
    "        df_results_cond['damping_factor_prior'] = df_results_cond['damping_factor']\n",
    "    elif variable_of_interest == 'damping_factor_t':\n",
    "        df_results_cond['damping_factor_t'] = df_results_cond['damping_factor']\n",
    "else:\n",
    "    df_results_cond = None"
   ],
   "id": "2cd956af35cc4a3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_dictionary = pd.DataFrame([\n",
    "    {'data_size': 100000, 'mini_batch': 10, 'n_steps': max_steps},\n",
    "    {'data_size': 100000, 'mini_batch': 100, 'n_steps': max_steps},\n",
    "    {'data_size': 100000, 'mini_batch': 1000, 'n_steps': max_steps},\n",
    "    {'data_size': 10000, 'mini_batch': 10, 'n_steps': max_steps},\n",
    "    {'data_size': 10000, 'mini_batch': 100, 'n_steps': max_steps},\n",
    "    {'data_size': 100000, 'mini_batch': 1000, 'n_steps': max_steps}\n",
    "])\n",
    "df_results = pd.concat([df_results_mini, df_dictionary], ignore_index=True)"
   ],
   "id": "60c37561639e5503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_steps_raw = df_results_mini[['n_steps', 'data_size']].groupby('data_size').agg(['mean', 'std'])\n",
    "n_steps_raw"
   ],
   "id": "adf8895ff66d55db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axis = plt.subplots(ncols=2, figsize=(10, 3), sharex=True, sharey=True, tight_layout=True)\n",
    "\n",
    "second_variable_of_interest = 'data_size'\n",
    "axis[0].errorbar(n_steps_raw.index, n_steps_raw.n_steps.values[:, 0], yerr=n_steps_raw.n_steps.values[:, 1],\n",
    "                 fmt='o-', capsize=5, alpha=0.75, color=colors[0])\n",
    "\n",
    "for df_results, ax, variable_of_interest in zip([df_results_cond],\n",
    "                                                [axis[-1]],\n",
    "                                                ['n_conditions']):\n",
    "\n",
    "    # Group by both second_variable_of_interest and variable_of_interest to compute mean and standard deviation of n_steps.\n",
    "    grouped_bar = df_results.groupby([second_variable_of_interest, variable_of_interest])['n_steps'].agg(['mean','std']).reset_index()\n",
    "\n",
    "    # Determine unique second_variable_of_interest and variable_of_interest values.\n",
    "    second_variable_of_interest_values = sorted(grouped_bar[second_variable_of_interest].unique())\n",
    "    variable_batch_values = sorted(grouped_bar[variable_of_interest].unique())\n",
    "\n",
    "    # Set up errorbar plot parameters.\n",
    "    n_groups = len(second_variable_of_interest_values)\n",
    "    n_series = len(variable_batch_values)\n",
    "    x = np.arange(n_groups)  # base x locations for groups\n",
    "\n",
    "    # Plot an errorbar for each variable_of_interest value within each second_variable_of_interest group.\n",
    "    for i, mb in enumerate(variable_batch_values):\n",
    "        subset = grouped_bar[grouped_bar[variable_of_interest] == mb]\n",
    "        means = []\n",
    "        stds = []\n",
    "        for ds in second_variable_of_interest_values:\n",
    "            row = subset[subset[second_variable_of_interest] == ds]\n",
    "            if not row.empty:\n",
    "                means.append(row['mean'].values[0])\n",
    "                stds.append(row['std'].values[0])\n",
    "            else:\n",
    "                means.append(np.nan)\n",
    "                stds.append(0)\n",
    "\n",
    "        means = np.array(means)\n",
    "        means[means > max_steps] = max_steps\n",
    "\n",
    "        # Center the x-axis ticks and label them.\n",
    "        if variable_of_interest == 'n_conditions':\n",
    "            ax.errorbar(second_variable_of_interest_values / mb, means,\n",
    "                        yerr=stds, fmt='o-', capsize=5, label=f'{mb}', alpha=0.75, color=colors[i])\n",
    "            #if i == 0:\n",
    "            #    axis[0].errorbar(second_variable_of_interest_values / mb, means,\n",
    "            #            yerr=stds, fmt='o-', capsize=5, label=f'{mb}', alpha=0.75, color=colors[i])\n",
    "        else:\n",
    "            # Use 'o-' for markers connected by lines.\n",
    "            ax.errorbar(second_variable_of_interest_values, means, yerr=stds, fmt='o-', capsize=5, label=f'{mb}', alpha=0.75, color=colors[i])\n",
    "\n",
    "    ax.legend(title=experiment_names[variable_of_interest], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "for ax in axis:\n",
    "    ax.axhline(max_steps, color='k', linestyle='--')\n",
    "    ax.text(1, max_steps-3500, f\"Maximal Number of Steps\", fontsize=8, color='k')\n",
    "    ax.set_xticks(second_variable_of_interest_values)\n",
    "    ax.set_xticklabels(second_variable_of_interest_values)\n",
    "    ax.set_xlabel('Information Units')\n",
    "    ax.set_ylabel('Number of Steps')\n",
    "    #ax.set_title(f'Number of Steps by {experiment_names[second_variable_of_interest]} and {experiment_names[variable_of_interest]}')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "\n",
    "plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_n_steps_conditions.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "32c6528bd481c055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3f523c848b1de341",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
