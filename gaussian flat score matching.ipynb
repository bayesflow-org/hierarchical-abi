{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flat Gaussian with compositional score matching\n",
    "\n",
    "\n",
    "In this notebook, we will use the compositional score matching to learn the posterior of a flat Gaussian model.\n",
    "The problem is defined as follows:\n",
    "- The prior is a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "- The simulator/likelihood is a Gaussian distribution with mean 0 and standard deviation 0.1.\n",
    "- We have an analytical solution for the posterior.\n",
    "- We set the dimension of the problem to $D=10$."
   ],
   "id": "146c29938a852c5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import CompositionalScoreModel, SDE, weighting_function, train_score_model, sde_sampling, \\\n",
    "    adaptive_sampling, probability_ode_solving, langevin_sampling, generate_diffusion_time, count_parameters\n",
    "from problems.gaussian_flat import GaussianProblem, Prior, Simulator, visualize_simulation_output, \\\n",
    "    generate_synthetic_data, \\\n",
    "    sample_posterior, posterior_contraction"
   ],
   "id": "a2cf1e9793c4966f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cuda\")",
   "id": "3c004b6c36927d87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior()\n",
    "simulator_test = Simulator()\n",
    "\n",
    "# test the simulator\n",
    "prior_test = prior.sample(2)\n",
    "sim_test = simulator_test(prior_test, n_obs=1000)\n",
    "visualize_simulation_output(sim_test['observable'])"
   ],
   "id": "ff842a23f12e95d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "max_number_of_obs = 1  # larger than one means we condition the score on multiple observations\n",
    "\n",
    "dataset = GaussianProblem(\n",
    "    n_data=10000,\n",
    "    prior=prior,\n",
    "    online_learning=True,\n",
    "    max_number_of_obs=max_number_of_obs\n",
    ")\n",
    "dataset_valid = GaussianProblem(\n",
    "    n_data=batch_size*2,\n",
    "    prior=prior,\n",
    "    max_number_of_obs=max_number_of_obs\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)"
   ],
   "id": "7db6e63f10c2b9e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "score_model = CompositionalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_x=prior.D,\n",
    "    hidden_dim=64,\n",
    "    n_blocks=3,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    time_embed_dim=16,\n",
    "    use_film=False,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior\n",
    ")\n",
    "#score_model.name = score_model.name + '_same_sim_budget'\n",
    "count_parameters(score_model)\n",
    "print(score_model.name)\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "b2c8d0ca5eb81f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 epochs=500, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Mean Train')\n",
    "plt.plot(loss_history[:, 1], label='Mean Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')\n",
    "plt.show()"
   ],
   "id": "20a0b52b7b69c6ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", weights_only=True, map_location=torch.device(torch_device)))\n",
    "score_model.eval();"
   ],
   "id": "f02e677587c85dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize Loss",
   "id": "435dfa4e1b5f9819"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_target = {}\n",
    "loss_list_score = {}\n",
    "loss_list_error_w_global = {}\n",
    "loss_list_error = {}\n",
    "\n",
    "score_model.to(torch_device)\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=100, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        loss_list_target[t.item()] = 0\n",
    "        loss_list_score[t.item()] = 0\n",
    "        loss_list_error_w_global[t.item()] = 0\n",
    "        loss_list_error[t.item()] = 0\n",
    "\n",
    "        for theta_global_batch, _, x_batch in dataloader_valid:\n",
    "            theta_global_batch = theta_global_batch.to(torch_device)\n",
    "            x_batch = x_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon_global = torch.randn_like(theta_global_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_global_batch.shape[0], 1), t,\n",
    "                                  dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            snr = score_model.sde.get_snr(t=t_tensor)\n",
    "            alpha, sigma = score_model.sde.kernel(log_snr=snr)\n",
    "            z_global = alpha * theta_global_batch + sigma * epsilon_global\n",
    "\n",
    "            # predict from perturbed theta\n",
    "            pred_epsilon_global = score_model(theta_global=z_global, time=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score_global = score_model(theta_global=z_global, time=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score_global = score_model.sde.grad_log_kernel(x=z_global,\n",
    "                                                                x0=theta_global_batch, t=t_tensor)\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target_global = -epsilon_global / sigma\n",
    "                pred_target_global = -pred_epsilon_global / sigma\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                target_global = epsilon_global\n",
    "                pred_target_global = pred_epsilon_global\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target_global = alpha*epsilon_global - sigma * theta_global_batch\n",
    "                pred_target_global = alpha*pred_epsilon_global - sigma * theta_global_batch\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target_global = theta_global_batch\n",
    "                pred_target_global = (z_global - pred_epsilon_global * sigma) / alpha\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_target_global - target_global), dim=-1))\n",
    "            loss = loss_global\n",
    "            loss_list_target[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_score_global - true_score_global), dim=-1))\n",
    "            loss = loss_global\n",
    "            loss_list_score[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            w = weighting_function(t_tensor, sde=score_model.sde,\n",
    "                                   weighting_type=score_model.weighting_type, prediction_type=score_model.prediction_type)\n",
    "            loss_global = torch.mean(w * torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_list_error_w_global[t.item()] += loss_global.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss = loss_global\n",
    "            loss_list_error[t.item()] += loss.item()"
   ],
   "id": "af4f7e0170f61435",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_target = pd.DataFrame(loss_list_target.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_global = pd.DataFrame(loss_list_error_w_global.items(), columns=['Time', 'Loss'])\n",
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "# compute snr\n",
    "snr = score_model.sde.get_snr(diffusion_time).cpu()\n",
    "#upper_bound_loss = (np.sqrt(2) + 1) / (std.numpy()**2)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, sharex=True, figsize=(16, 3), tight_layout=True)\n",
    "ax[0].plot(df_target['Time'], np.log(df_target['Loss']), label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], np.log(df_score['Loss']), label='Score Loss')\n",
    "#ax[1].plot(df_score['Time'], df_score['Loss'] / upper_bound_loss, label='Score Loss')\n",
    "ax[1].plot(diffusion_time.cpu(), snr, label='log snr', alpha=0.5)\n",
    "ax[2].plot(df_error_w_global['Time'], np.log(df_error_w_global['Loss']), label='Weighted Loss (as in Optimization)')\n",
    "ax[3].plot(df_error['Time'], np.log(df_error['Loss']), label='Loss on Error')\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Log Loss')\n",
    "    a.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/losses_diffusion_time.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time.cpu(),\n",
    "         weighting_function(diffusion_time, sde=score_model.sde, weighting_type=score_model.weighting_type,\n",
    "                            prediction_type=score_model.prediction_type).cpu(),\n",
    "         label='weighting')\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "8bd29b88d0b34551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "e5b8ae29435a88fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_size = 100  # number of observations\n",
    "valid_prior_global, valid_data = generate_synthetic_data(prior, n_samples=100, data_size=data_size,\n",
    "                                                         normalize=False, random_seed=0)\n",
    "param_names = ['$D_{' + str(i+1) + '}$' for i in range(prior.D)]\n",
    "n_post_samples = 100\n",
    "score_model.current_number_of_obs = 1\n",
    "score_model.sde.s_shift_cosine = 0"
   ],
   "id": "3164a89ee6f41ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(valid_data)",
   "id": "38e32ca9089d57d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_posterior_single = lambda vd: sample_posterior(\n",
    "    vd,\n",
    "    prior_sigma=prior.scale,\n",
    "    sigma=prior.simulator.scale,\n",
    "    n_samples=n_post_samples\n",
    ")\n",
    "posterior_global_samples_true = np.array([sample_posterior_single(vd) for vd in valid_data])"
   ],
   "id": "ccd5e6a7ec29ebe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_global_samples_true, np.array(valid_prior_global), variable_names=param_names)\n",
    "diagnostics.calibration_ecdf(posterior_global_samples_true, np.array(valid_prior_global),\n",
    "                             difference=True, variable_names=param_names);"
   ],
   "id": "a71a1cccc3c912f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 10\n",
    "t1_value = mini_batch_size /( data_size //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    'size': mini_batch_size,\n",
    "    #'damping_factor': lambda t: t1_value + (t0_value - t1_value) * 0.5 * (1 + torch.cos(torch.pi * t)),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * score_model.sde.kernel(log_snr=score_model.sde.get_snr(t))[1],\n",
    "    'damping_factor': lambda t: 0.1, #t1_value,\n",
    "    #'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * torch.sigmoid(20*(t-0.3))\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "5f609cc5bc7b5b10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = langevin_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   diffusion_steps=300, langevin_steps=5, step_size_factor=0.05,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "3b2d70d3e70d8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_langevin_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_langevin_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "9e20879d9175d9a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = sde_sampling(score_model, valid_data, n_post_samples=n_post_samples, diffusion_steps=300,\n",
    "                                              method=['euler', 'milstein_grad_free', 'srk1w1'][1],\n",
    "                                           device=torch_device, verbose=True)"
   ],
   "id": "b65d314296ebcd21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "d4a0ba4367c015ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = sde_sampling(score_model, valid_data, n_post_samples=n_post_samples, diffusion_steps=100,\n",
    "                                              method=['euler', 'milstein_grad_free', 'srk1w1'][1],\n",
    "                                              mini_batch_arg=mini_batch_arg,\n",
    "                                              device=torch_device, verbose=True)"
   ],
   "id": "7e5f5807547590de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "b22b951fa9c21fe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                   #e_rel=0.6, #e_abs = 0.01 * np.sqrt(prior.D),\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=False)"
   ],
   "id": "da447b7f3b653a0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.z_score_contraction(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                            variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/z_score_global_adaptive_sampler{score_model.current_number_of_obs}.png')"
   ],
   "id": "a00aee3bc046ff26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = probability_ode_solving(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                         #run_sampling_in_parallel=False,\n",
    "                                                         method=['RK45', 'RK23', 'Radau', 'LSODA'][0],\n",
    "                                                         device=torch_device, verbose=True)"
   ],
   "id": "d2b84cc91604a04d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_ode{score_model.current_number_of_obs}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_ode{score_model.current_number_of_obs}.png')"
   ],
   "id": "8dffd9351e527a73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step Size for different Grid Sizes\n",
    "\n",
    "- we compare score model with only one condition, and with $k$-conditions\n",
    "- we show that the scaling in the number of needed sampling steps only depends on the Bayesian Units used\n",
    "- error reduces when using more conditions, but since network size stays the same, increases at some point again\n",
    "- we show how mini batching effects the posterior\n",
    "\n",
    "Metrics:\n",
    "- MMD between true and estimated posterior samples\n",
    "- RMSE between the medians of true and estimated posterior samples\n",
    "- Posterior contraction: (1 - var_empirical_posterior / var_prior) / (1 - var_true_posterior / var_prior), and using the mean variances over all parameters"
   ],
   "id": "a75e6f2f29fa0cc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:10:49.757214Z",
     "start_time": "2025-03-13T01:10:49.754477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gaussian_kernel(x, y, sigma):\n",
    "    \"\"\"Compute Gaussian kernel between two sets of samples.\"\"\"\n",
    "    x = np.atleast_2d(x)\n",
    "    y = np.atleast_2d(y)\n",
    "    sq_dists = np.sum((x[:, None, :] - y[None, :, :]) ** 2, axis=2)\n",
    "    return np.exp(-sq_dists / (2 * sigma ** 2))\n",
    "\n",
    "def compute_mmd(x, y, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Maximum Mean Discrepancy (MMD) between two sets of samples.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Samples from distribution P, shape (n, d).\n",
    "        y (np.ndarray): Samples from distribution Q, shape (m, d).\n",
    "        sigma (float): Bandwidth for the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated MMD^2 value.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Compute kernel matrices\n",
    "    K_xx = gaussian_kernel(x, x, sigma)\n",
    "    K_yy = gaussian_kernel(y, y, sigma)\n",
    "    K_xy = gaussian_kernel(x, y, sigma)\n",
    "\n",
    "    # Compute MMD^2\n",
    "    mmd_squared = (np.mean(K_xx) + np.mean(K_yy) - 2 * np.mean(K_xy))\n",
    "    return mmd_squared"
   ],
   "id": "df843082dfd65c9b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:10:50.361263Z",
     "start_time": "2025-03-13T01:10:50.359976Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "196cbbd6b2f9abaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T01:11:09.316271Z",
     "start_time": "2025-03-13T01:10:50.817349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure we generate enough synthetic data samples.\n",
    "n_samples_data = 10\n",
    "n_post_samples = 100\n",
    "score_model.current_number_of_obs = 1\n",
    "max_steps = 5000\n",
    "#variable_of_interest = ['mini_batch', 'n_conditions', 'cosine_shift', 'damping_factor'][-1]\n",
    "\n",
    "for variable_of_interest in ['mini_batch', 'n_conditions', 'cosine_shift', 'damping_factor'][::-1]:\n",
    "    mini_batch = [None]\n",
    "    n_conditions = [1]\n",
    "    cosine_shifts = [0]\n",
    "    d_factors = [0.01]  # using the d factor depending on the mini batch size\n",
    "\n",
    "    if variable_of_interest == 'mini_batch':\n",
    "        # Set up your data sizes and mini-batch parameters.\n",
    "        data_sizes = np.array([1, 10, 100, 10000, 100000, 200000, 250000])  # todo: up to 250.000\n",
    "        mini_batch = [1, 10, 100, 1000, 10000, None]\n",
    "        second_variable_of_interest = 'data_size'\n",
    "\n",
    "    elif variable_of_interest == 'n_conditions':\n",
    "        # Set up your data sizes and mini-batch parameters.\n",
    "        data_sizes = np.array([1, 10, 100, 1000])\n",
    "        n_conditions = [1, 5, 10, 20, 50, 100]\n",
    "        second_variable_of_interest = 'data_size'\n",
    "\n",
    "    elif variable_of_interest == 'cosine_shift':\n",
    "        # Set up your data sizes and mini-batch parameters.\n",
    "        data_sizes = np.array([10000])\n",
    "        mini_batch = [10]\n",
    "        cosine_shifts = [0, -1, 1, 2, 5, 10]\n",
    "        second_variable_of_interest = 'data_size'\n",
    "\n",
    "    elif variable_of_interest == 'damping_factor':\n",
    "        # Set up your data sizes and mini-batch parameters.\n",
    "        data_sizes = np.array([1000])\n",
    "        mini_batch = [1, 10, 100, None]\n",
    "        d_factors = [0.01, 0.1, 0.5, 0.75, 0.9, 1]\n",
    "        second_variable_of_interest = 'mini_batch'\n",
    "    else:\n",
    "        raise ValueError('Unknown variable_of_interest')\n",
    "\n",
    "    df_path = f'plots/{score_model.name}/df_results_{variable_of_interest}.csv'\n",
    "# if os.path.exists(df_path):\n",
    "#     # Load CSV\n",
    "#     df_results = pd.read_csv(df_path)\n",
    "#     # Convert string representations back to lists\n",
    "#     df_results['list_steps'] = df_results['list_steps'].apply(lambda x: ast.literal_eval(x))\n",
    "# else:\n",
    "    df_results = None\n",
    "\n",
    "    # List to store results.\n",
    "    results = []\n",
    "    reached_max_evals = []\n",
    "\n",
    "    # Iterate over data sizes.\n",
    "    for n in data_sizes:\n",
    "        # Generate synthetic data with enough samples\n",
    "        true_params, test_data = generate_synthetic_data(prior, n_samples=n_samples_data, data_size=n,\n",
    "                                                          normalize=False, random_seed=0)\n",
    "        true_params = true_params.numpy()\n",
    "        # Iterate over experimental setting\n",
    "        for mb, nc, cs, d_factor in itertools.product(mini_batch, n_conditions, cosine_shifts, d_factors):\n",
    "            # Skip mini-batch settings that are larger than or equal to the data size.\n",
    "            if mb is not None and mb >= n:\n",
    "                continue\n",
    "\n",
    "            for max_reached in reached_max_evals:\n",
    "                if max_reached[1] == nc and max_reached[2] == cs and max_reached[3] == d_factor:\n",
    "                    # for this condition, if a lower mini batch size already failed we can skip that as well\n",
    "                    if max_reached[0] <= mb:\n",
    "                        print(f'smaller mini batch size already failed, skipping {nc}, {cs}')\n",
    "                        continue\n",
    "                if max_reached[0] == mb and max_reached[1] == nc and max_reached[2] == cs:\n",
    "                    # for this condition, if a lower mini batch size already failed we can skip that as well\n",
    "                    if max_reached[3] <= d_factor:\n",
    "                        print(f'smaller damping factor Â´already failed, skipping {nc}, {cs}')\n",
    "                        continue\n",
    "\n",
    "            print(f\"Data Size: {n}, Mini Batch: {mb}, Conditions: {nc}, Cosine shift: {cs}, Damping Factor: {d_factor}\")\n",
    "            # Set current number of conditions\n",
    "            score_model.current_number_of_obs = nc\n",
    "\n",
    "            # Set cosine shit\n",
    "            score_model.sde.s_shift_cosine = cs\n",
    "            # Damping factor\n",
    "            damping_factor = lambda t: d_factor # mb/ (n // score_model.current_number_of_obs)\n",
    "\n",
    "            # Run adaptive sampling.\n",
    "            test_samples, list_steps = adaptive_sampling(score_model, test_data, conditions=None,\n",
    "                                                         n_post_samples=n_post_samples,\n",
    "                                                         mini_batch_arg={'size': mb, 'damping_factor': damping_factor} if mb is not None else {'damping_factor': damping_factor},\n",
    "                                                         max_evals=max_steps*2,\n",
    "                                                         t_end=0, random_seed=0, device=torch_device,\n",
    "                                                         run_sampling_in_parallel=False,  # can actually be faster\n",
    "                                                         return_steps=True)\n",
    "            # Sample the true posterior.\n",
    "            true_samples = np.stack([sample_posterior(x, prior_sigma=prior.scale,\n",
    "                                            sigma=prior.simulator.scale, n_samples=n_post_samples) for x in test_data], axis=0)\n",
    "            #true_median = np.array([analytical_posterior_median(x, prior_std=prior.scale, likelihood_std=prior.simulator.scale)\n",
    "            #                        for x in test_data])\n",
    "\n",
    "            # Compute metrics.\n",
    "            mmd = [compute_mmd(test_samples[i], true_samples[i]) for i in range(n_samples_data)]\n",
    "            rmse = diagnostics.root_mean_squared_error(test_samples, true_params)['values']\n",
    "            c_error = diagnostics.calibration_error(test_samples, true_params)['values']\n",
    "\n",
    "            contractions = diagnostics.posterior_contraction(test_samples, true_params)['values']\n",
    "            true_contraction = posterior_contraction(prior_std=prior.scale, likelihood_std=prior.simulator.scale, n_obs=n)\n",
    "            rel_contraction = (contractions / true_contraction.mean())\n",
    "\n",
    "            # Number of steps\n",
    "            if np.isnan(test_samples).any():\n",
    "                n_steps = np.inf\n",
    "                reached_max_evals.append((mb, nc, cs, d_factor))\n",
    "            else:\n",
    "                n_steps = np.mean([len(ls) for ls in list_steps])\n",
    "                if n_steps >= max_steps:\n",
    "                    # no need to check larger mini batches, will also fail to converge\n",
    "                    reached_max_evals.append((mb, nc, cs, d_factor))\n",
    "\n",
    "            # Print current metrics.\n",
    "            print(f\"MMD: {np.mean(mmd)}, #Steps: {n_steps}\")\n",
    "\n",
    "            # Save results into a dictionary.\n",
    "            for i in range(n_samples_data):\n",
    "                results.append({\n",
    "                    \"data_size\": n,\n",
    "                    \"data_id\": i,\n",
    "                    \"mini_batch\": mb if mb is not None else n,\n",
    "                    \"damping_factor\": d_factor,\n",
    "                    'n_conditions': nc,\n",
    "                    'cosine_shift': cs,\n",
    "                    \"n_steps\": n_steps,\n",
    "                    \"list_steps\": np.where(np.isnan(list_steps[0]), None, list_steps[0]).tolist(),  # only for the first sample\n",
    "                    \"mmd\": mmd[i],\n",
    "                    \"median\": np.median(test_samples, axis=1)[i],\n",
    "                    \"median_rmse\": rmse[i],\n",
    "                    \"c_error\": c_error[i],\n",
    "                    \"contractions\": contractions[i],\n",
    "                    \"rel_contraction\": rel_contraction[i]\n",
    "                })\n",
    "\n",
    "            # Create a DataFrame from the results list. Save intermediate results\n",
    "            df_results = pd.DataFrame(results)\n",
    "            # Convert lists to strings for CSV storage\n",
    "            df_results['list_steps'] = df_results['list_steps'].apply(lambda x: str(x))\n",
    "            df_results.to_csv(df_path)\n",
    "\n",
    "        # Convert string representations back to lists\n",
    "        df_results['list_steps'] = df_results['list_steps'].apply(lambda x: ast.literal_eval(x))"
   ],
   "id": "ca4604e9d369434a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 1000, Mini Batch: 1, Conditions: 1, Cosine shift: 0, Damping Factor: 0.01\n",
      "MMD: 0.2008611846596243, #Steps: 34.0\n",
      "Data Size: 1000, Mini Batch: 1, Conditions: 1, Cosine shift: 0, Damping Factor: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 89\u001B[39m\n\u001B[32m     86\u001B[39m damping_factor = \u001B[38;5;28;01mlambda\u001B[39;00m t: d_factor \u001B[38;5;66;03m# mb/ (n // score_model.current_number_of_obs)\u001B[39;00m\n\u001B[32m     88\u001B[39m \u001B[38;5;66;03m# Run adaptive sampling.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m test_samples, list_steps = \u001B[43madaptive_sampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscore_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     90\u001B[39m \u001B[43m                                             \u001B[49m\u001B[43mn_post_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_post_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     91\u001B[39m \u001B[43m                                             \u001B[49m\u001B[43mmini_batch_arg\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msize\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdamping_factor\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdamping_factor\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmb\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdamping_factor\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdamping_factor\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     92\u001B[39m \u001B[43m                                             \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m*\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[43m                                             \u001B[49m\u001B[43mt_end\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     94\u001B[39m \u001B[43m                                             \u001B[49m\u001B[43mrun_sampling_in_parallel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# can actually be faster\u001B[39;49;00m\n\u001B[32m     95\u001B[39m \u001B[43m                                             \u001B[49m\u001B[43mreturn_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m \u001B[38;5;66;03m# Sample the true posterior.\u001B[39;00m\n\u001B[32m     97\u001B[39m true_samples = np.stack([sample_posterior(x, prior_sigma=prior.scale,\n\u001B[32m     98\u001B[39m                                 sigma=prior.simulator.scale, n_samples=n_post_samples) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m test_data], axis=\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/diffusion_model/sampling_algorithms.py:610\u001B[39m, in \u001B[36madaptive_sampling\u001B[39m\u001B[34m(model, x_obs, n_post_samples, conditions, e_abs, e_rel, h_init, r, adapt_safety, max_evals, t_start, t_end, mini_batch_arg, run_sampling_in_parallel, random_seed, device, return_steps, verbose)\u001B[39m\n\u001B[32m    608\u001B[39m list_accepted_steps = []\n\u001B[32m    609\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_post_samples):\n\u001B[32m--> \u001B[39m\u001B[32m610\u001B[39m     ps, ls = \u001B[43madaptive_sampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_obs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx_obs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_post_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    611\u001B[39m \u001B[43m                          \u001B[49m\u001B[43me_abs\u001B[49m\u001B[43m=\u001B[49m\u001B[43me_abs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me_rel\u001B[49m\u001B[43m=\u001B[49m\u001B[43me_rel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh_init\u001B[49m\u001B[43m=\u001B[49m\u001B[43mh_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapt_safety\u001B[49m\u001B[43m=\u001B[49m\u001B[43madapt_safety\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    612\u001B[39m \u001B[43m                          \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_start\u001B[49m\u001B[43m=\u001B[49m\u001B[43mt_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_end\u001B[49m\u001B[43m=\u001B[49m\u001B[43mt_end\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmini_batch_arg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmini_batch_arg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    613\u001B[39m \u001B[43m                          \u001B[49m\u001B[43mrun_sampling_in_parallel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    614\u001B[39m \u001B[43m                          \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    615\u001B[39m \u001B[43m                          \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    616\u001B[39m     post_samples.append(ps)\n\u001B[32m    617\u001B[39m     list_accepted_steps.append(ls)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/diffusion_model/sampling_algorithms.py:675\u001B[39m, in \u001B[36madaptive_sampling\u001B[39m\u001B[34m(model, x_obs, n_post_samples, conditions, e_abs, e_rel, h_init, r, adapt_safety, max_evals, t_start, t_end, mini_batch_arg, run_sampling_in_parallel, random_seed, device, return_steps, verbose)\u001B[39m\n\u001B[32m    672\u001B[39m     sub_x_expanded = x_exp\n\u001B[32m    674\u001B[39m \u001B[38;5;66;03m# Euler-Maruyama step.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m675\u001B[39m scores = \u001B[43meval_compositional_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiffusion_time\u001B[49m\u001B[43m=\u001B[49m\u001B[43mt_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    676\u001B[39m \u001B[43m                                  \u001B[49m\u001B[43mx_exp\u001B[49m\u001B[43m=\u001B[49m\u001B[43msub_x_expanded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconditions_exp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconditions_exp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    677\u001B[39m \u001B[43m                                  \u001B[49m\u001B[43mbatch_size_full\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size_full\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_scores_update_full\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_scores_update\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    678\u001B[39m \u001B[43m                                  \u001B[49m\u001B[43mmini_batch_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmini_batch_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    679\u001B[39m theta_eul = euler_maruyama_step(model, theta, score=scores, t=t_tensor, dt=h, noise=z)\n\u001B[32m    681\u001B[39m \u001B[38;5;66;03m# Heun-style improved step.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/diffusion_model/sampling_algorithms.py:311\u001B[39m, in \u001B[36meval_compositional_score\u001B[39m\u001B[34m(model, theta, diffusion_time, x_exp, conditions_exp, batch_size_full, n_scores_update_full, mini_batch_dict)\u001B[39m\n\u001B[32m    309\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m conditions_exp \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    310\u001B[39m     theta_exp = theta.unsqueeze(\u001B[32m1\u001B[39m).expand(-\u001B[32m1\u001B[39m, mini_batch_dict[\u001B[33m'\u001B[39m\u001B[33msize\u001B[39m\u001B[33m'\u001B[39m], -\u001B[32m1\u001B[39m).contiguous().view(-\u001B[32m1\u001B[39m, model.prior.n_params_global)\n\u001B[32m--> \u001B[39m\u001B[32m311\u001B[39m     model_indv_scores = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward_global\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtheta_global\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtheta_exp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtime\u001B[49m\u001B[43m=\u001B[49m\u001B[43mt_exp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx_exp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpred_score\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclip_x\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m    317\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    318\u001B[39m     \u001B[38;5;66;03m# Reshape to (batch_size_full, n_obs, -1) and sum over observations\u001B[39;00m\n\u001B[32m    319\u001B[39m     model_sum_scores_indv = model_indv_scores.contiguous().view(batch_size_full, mini_batch_dict[\u001B[33m'\u001B[39m\u001B[33msize\u001B[39m\u001B[33m'\u001B[39m], -\u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/diffusion_model/diffusion_sde_model.py:604\u001B[39m, in \u001B[36mCompositionalScoreModel.forward_global\u001B[39m\u001B[34m(self, theta_global, time, x, pred_score, clip_x)\u001B[39m\n\u001B[32m    601\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m x.ndim == \u001B[32m3\u001B[39m:\n\u001B[32m    602\u001B[39m     \u001B[38;5;66;03m# there is time dimension, which we do not need\u001B[39;00m\n\u001B[32m    603\u001B[39m     x = x.squeeze(\u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtheta_global\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtheta_global\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred_score\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpred_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_x\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclip_x\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/diffusion_model/diffusion_sde_model.py:577\u001B[39m, in \u001B[36mCompositionalScoreModel.forward\u001B[39m\u001B[34m(self, theta_global, time, x, pred_score, clip_x)\u001B[39m\n\u001B[32m    575\u001B[39m x_emb = \u001B[38;5;28mself\u001B[39m.summary_net(x)\n\u001B[32m    576\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.max_number_of_obs == \u001B[32m1\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m577\u001B[39m     global_out = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mglobal_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtheta\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtheta_global\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtime\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mx_emb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    578\u001B[39m \u001B[43m                                           \u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpred_score\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpred_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_x\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclip_x\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    579\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    580\u001B[39m     \u001B[38;5;66;03m# for global, concat observations\u001B[39;00m\n\u001B[32m    581\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m x_emb.shape[\u001B[32m1\u001B[39m] > \u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/diffusion_model/diffusion_sde_model.py:310\u001B[39m, in \u001B[36mScoreModel.forward\u001B[39m\u001B[34m(self, theta, time, x, conditions, pred_score, clip_x)\u001B[39m\n\u001B[32m    308\u001B[39m \u001B[38;5;66;03m# Pass through each block, injecting the same cond at each layer\u001B[39;00m\n\u001B[32m    309\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.blocks:\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     h = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcond\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[38;5;66;03m# Add the skip connection from theta (or from the input projection)\u001B[39;00m\n\u001B[32m    313\u001B[39m h = h + skip\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/diffusion_model/diffusion_sde_model.py:119\u001B[39m, in \u001B[36mConditionalResidualBlock.forward\u001B[39m\u001B[34m(self, h, cond)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, h, cond):\n\u001B[32m    114\u001B[39m     \u001B[38;5;66;03m# h: [batch_size, in_dim]\u001B[39;00m\n\u001B[32m    115\u001B[39m     \u001B[38;5;66;03m# cond: [batch_size, cond_dim]\u001B[39;00m\n\u001B[32m    116\u001B[39m \n\u001B[32m    117\u001B[39m     \u001B[38;5;66;03m# First transformation with conditioning\u001B[39;00m\n\u001B[32m    118\u001B[39m     x = torch.cat([h, cond], dim=-\u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# [batch_size, in_dim + cond_dim]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     out = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m     out = \u001B[38;5;28mself\u001B[39m.norm1(out)\n\u001B[32m    121\u001B[39m     out = \u001B[38;5;28mself\u001B[39m.activation(out)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmProjects/hierarchical-abi/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List to store results.\n",
    "results = []\n",
    "reached_max_evals = []\n",
    "\n",
    "# Iterate over data sizes.\n",
    "for n in data_sizes:\n",
    "    # Generate synthetic data with enough samples\n",
    "    true_params, test_data = generate_synthetic_data(prior, n_samples=n_samples_data, data_size=n,\n",
    "                                                      normalize=False, random_seed=0)\n",
    "    true_params = true_params.numpy()\n",
    "    # Iterate over experimental setting\n",
    "    for mb, nc, cs, d_factor in itertools.product(mini_batch, n_conditions, cosine_shifts, d_factors):\n",
    "        # Skip mini-batch settings that are larger than or equal to the data size.\n",
    "        if mb is not None and mb >= n:\n",
    "            continue\n",
    "\n",
    "        for max_reached in reached_max_evals:\n",
    "            if max_reached[1] == nc and max_reached[2] == cs and max_reached[3] == d_factor:\n",
    "                # for this condition, if a lower mini batch size already failed we can skip that as well\n",
    "                if max_reached[0] <= mb:\n",
    "                    print(f'smaller mini batch size already failed, skipping {nc}, {cs}')\n",
    "                    continue\n",
    "            if max_reached[0] == mb and max_reached[1] == nc and max_reached[2] == cs:\n",
    "                # for this condition, if a lower mini batch size already failed we can skip that as well\n",
    "                if max_reached[3] <= d_factor:\n",
    "                    print(f'smaller damping factor Â´already failed, skipping {nc}, {cs}')\n",
    "                    continue\n",
    "\n",
    "        print(f\"Data Size: {n}, Mini Batch: {mb}, Conditions: {nc}, Cosine shift: {cs}, Damping Factor: {d_factor}\")\n",
    "        # Set current number of conditions\n",
    "        score_model.current_number_of_obs = nc\n",
    "\n",
    "        # Set cosine shit\n",
    "        score_model.sde.s_shift_cosine = cs\n",
    "        # Damping factor\n",
    "        damping_factor = lambda t: d_factor # mb/ (n // score_model.current_number_of_obs)\n",
    "\n",
    "        # Run adaptive sampling.\n",
    "        test_samples, list_steps = adaptive_sampling(score_model, test_data, conditions=None,\n",
    "                                                     n_post_samples=n_post_samples,\n",
    "                                                     mini_batch_arg={'size': mb, 'damping_factor': damping_factor} if mb is not None else {'damping_factor': damping_factor},\n",
    "                                                     max_evals=max_steps*2,\n",
    "                                                     t_end=0, random_seed=0, device=torch_device,\n",
    "                                                     run_sampling_in_parallel=False,  # can actually be faster\n",
    "                                                     return_steps=True)\n",
    "        # Sample the true posterior.\n",
    "        true_samples = np.stack([sample_posterior(x, prior_sigma=prior.scale,\n",
    "                                        sigma=prior.simulator.scale, n_samples=n_post_samples) for x in test_data], axis=0)\n",
    "        #true_median = np.array([analytical_posterior_median(x, prior_std=prior.scale, likelihood_std=prior.simulator.scale)\n",
    "        #                        for x in test_data])\n",
    "\n",
    "        # Compute metrics.\n",
    "        mmd = [compute_mmd(test_samples[i], true_samples[i]) for i in range(n_samples_data)]\n",
    "        rmse = diagnostics.root_mean_squared_error(test_samples, true_params)['values']\n",
    "        c_error = diagnostics.calibration_error(test_samples, true_params)['values']\n",
    "\n",
    "        contractions = diagnostics.posterior_contraction(test_samples, true_params)['values']\n",
    "        true_contraction = posterior_contraction(prior_std=prior.scale, likelihood_std=prior.simulator.scale, n_obs=n)\n",
    "        rel_contraction = (contractions / true_contraction.mean())\n",
    "\n",
    "        # Number of steps\n",
    "        if np.isnan(test_samples).any():\n",
    "            n_steps = np.inf\n",
    "            reached_max_evals.append((mb, nc, cs, d_factor))\n",
    "        else:\n",
    "            n_steps = np.mean([len(ls) for ls in list_steps])\n",
    "            if n_steps >= max_steps:\n",
    "                # no need to check larger mini batches, will also fail to converge\n",
    "                reached_max_evals.append((mb, nc, cs, d_factor))\n",
    "\n",
    "        # Print current metrics.\n",
    "        print(f\"MMD: {np.mean(mmd)}, #Steps: {n_steps}\")\n",
    "\n",
    "        # Save results into a dictionary.\n",
    "        for i in range(n_samples_data):\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": i,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"damping_factor\": d_factor,\n",
    "                'n_conditions': nc,\n",
    "                'cosine_shift': cs,\n",
    "                \"n_steps\": n_steps,\n",
    "                \"list_steps\": np.where(np.isnan(list_steps[0]), None, list_steps[0]).tolist(),  # only for the first sample\n",
    "                \"mmd\": mmd[i],\n",
    "                \"median\": np.median(test_samples, axis=1)[i],\n",
    "                \"median_rmse\": rmse[i],\n",
    "                \"c_error\": c_error[i],\n",
    "                \"contractions\": contractions[i],\n",
    "                \"rel_contraction\": rel_contraction[i]\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the results list. Save intermediate results\n",
    "        df_results = pd.DataFrame(results)\n",
    "        # Convert lists to strings for CSV storage\n",
    "        df_results['list_steps'] = df_results['list_steps'].apply(lambda x: str(x))\n",
    "        df_results.to_csv(df_path)\n",
    "\n",
    "    # Convert string representations back to lists\n",
    "    df_results['list_steps'] = df_results['list_steps'].apply(lambda x: ast.literal_eval(x))"
   ],
   "id": "fba169dd9954eb22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Group by both second_variable_of_interest and variable_of_interest to compute mean and standard deviation of n_steps.\n",
    "grouped_bar = df_results.groupby([second_variable_of_interest, variable_of_interest])['n_steps'].agg(['mean','std']).reset_index()\n",
    "\n",
    "# Determine unique second_variable_of_interest and variable_of_interest values.\n",
    "second_variable_of_interest_values = sorted(grouped_bar[second_variable_of_interest].unique())\n",
    "# Order variable_of_interest values\n",
    "variable_batch_values = sorted(grouped_bar[variable_of_interest].unique())\n",
    "\n",
    "# Set up bar plot parameters.\n",
    "n_groups = len(second_variable_of_interest_values)\n",
    "n_bars = len(variable_batch_values)\n",
    "bar_width = 0.8 / n_bars  # total width is 0.8 for each group\n",
    "x = np.arange(n_groups)  # x locations for groups\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "# Plot a bar for each variable_of_interest value within each data_size group.\n",
    "for i, mb in enumerate(variable_batch_values):\n",
    "    # Filter the grouped DataFrame for the current variable_of_interest value.\n",
    "    subset = grouped_bar[grouped_bar[variable_of_interest] == mb]\n",
    "    means = []\n",
    "    stds = []\n",
    "    for ds in second_variable_of_interest_values:\n",
    "        # Find the row corresponding to this data size and variable_of_interest.\n",
    "        row = subset[subset[second_variable_of_interest] == ds]\n",
    "        if not row.empty:\n",
    "            means.append(row['mean'].values[0])\n",
    "            stds.append(row['std'].values[0])\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            stds.append(0)\n",
    "\n",
    "    # Define label\n",
    "    label = f'{int(mb)}'\n",
    "    # Compute positions for the bars.\n",
    "    positions = x + i * bar_width\n",
    "    ax.bar(positions, means, width=bar_width, yerr=stds, capsize=5, label=label)\n",
    "\n",
    "# Set the x-axis ticks and labels so that groups are centered.\n",
    "ax.set_xticks(x + bar_width*(n_bars-1)/2)\n",
    "ax.set_xticklabels(data_sizes)\n",
    "ax.set_xlabel(second_variable_of_interest)\n",
    "ax.set_ylabel('Number of Steps')\n",
    "ax.set_title(f'Number of Steps by {second_variable_of_interest} and {variable_of_interest}')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(title=f'{variable_of_interest}')\n",
    "plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_n_steps.png')\n",
    "plt.show()"
   ],
   "id": "96e8f9f35d2d3bef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if variable_batch_values == 'mini_batch_size':\n",
    "    # ------------------------------\n",
    "    # Plot 1: Bar plot of n_steps for the full-batch  case.\n",
    "    # ------------------------------\n",
    "\n",
    "    # Filter the full-batch rows\n",
    "    df_full = df_results[df_results['data_size'] == df_results['mini_batch']]\n",
    "\n",
    "    # Group by data_size and compute mean and standard deviation of n_steps.\n",
    "    grouped_full = df_full.groupby('data_size')['n_steps'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    plt.bar(grouped_full['data_size'], grouped_full['mean'],\n",
    "            yerr=grouped_full['std'], capsize=5, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Number of Steps')\n",
    "    plt.title('Number of Steps (Full Batch) per Data Size')\n",
    "    plt.xticks(grouped_full['data_size'])\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_steps_full_batch.png')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 2: Errorbar plots for MMD, RMSE, and Contraction vs. variable_of_interest.\n",
    "# ------------------------------\n",
    "\n",
    "# Filter rows with a variable_of_interest value (skip full-batch rows).\n",
    "df_mb = df_results[df_results[variable_of_interest].notnull()].copy()\n",
    "# Convert mini_batch to float (if not already) to allow proper plotting on the x-axis.\n",
    "df_mb[variable_of_interest] = df_mb[variable_of_interest].astype(float)\n",
    "\n",
    "# Define the metrics to plot: key is dataframe column, value is label for y-axis.\n",
    "metrics = {\n",
    "    'mmd': 'MMD',\n",
    "    'median_rmse': 'RMSE (Posterior Median)',\n",
    "    'rel_contraction': 'Relative Posterior Contraction'\n",
    "}\n",
    "\n",
    "# Identify the unique data sizes (to plot different lines per data size).\n",
    "unique_second_variable_of_interest = sorted(df_mb[second_variable_of_interest].unique())\n",
    "\n",
    "# Create one figure per metric.\n",
    "for metric, metric_label in metrics.items():\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    for ds in unique_second_variable_of_interest:\n",
    "        # Select the rows for this particular data size.\n",
    "        df_sub = df_mb[df_mb[second_variable_of_interest] == ds]\n",
    "        # Group by variable_of_interest size to get mean and std of the metric.\n",
    "        grouped = df_sub.groupby(variable_of_interest)[metric].agg(['mean', 'std']).reset_index()\n",
    "        plt.errorbar(grouped[variable_of_interest], grouped['mean'], yerr=grouped['std'],\n",
    "                     marker='o', capsize=5, label=f'{ds}')\n",
    "    plt.xlabel(variable_of_interest)\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.title(f'{metric_label} vs {variable_of_interest}')\n",
    "    plt.legend(title=second_variable_of_interest)\n",
    "    # Using a logarithmic scale for the x-axis since mini-batch sizes vary widely.\n",
    "    if variable_of_interest == 'mini_batch':\n",
    "        plt.xscale('log')\n",
    "    if metric == 'rel_contraction':\n",
    "        plt.ylim(0, 1.2)\n",
    "    plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_{metrics}.png')\n",
    "    plt.show()"
   ],
   "id": "4b74a63bf6db1eee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if variable_of_interest == 'mini_batch':\n",
    "    # Figure 1: Full Batch (mini_batch is None) for data_id 0\n",
    "    # ------------------------------\n",
    "    # Filter for valid_id 0 and full-batch runs\n",
    "    df_full = df_results[(df_results['data_id'] == 0) & (df_results['data_size'] == df_results['mini_batch'])]\n",
    "\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    for ds in sorted(df_full['data_size'].unique()):\n",
    "        # Extract the row for this data_size (should be a single row per combination)\n",
    "        row = df_full[df_full['data_size'] == ds]\n",
    "        if not row.empty:\n",
    "            # Extract the list of step sizes (assumed to be a list or array)\n",
    "            steps_list = row.iloc[0]['list_steps']\n",
    "            # Plot step size vs iteration\n",
    "            plt.plot(range(len(steps_list)), steps_list, label=f\"{ds}\", alpha=.75)\n",
    "\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Step Size\")\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Step Size Over Time for Full Batch\")\n",
    "    plt.legend(title=\"Data Size\")\n",
    "    plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_full_batch_step_size.png')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Figure 2\n",
    "# ------------------------------\n",
    "# Filter for data_id 0 and only mini-batch runs.\n",
    "df_mb = df_results[(df_results['data_id'] == 0) & (df_results[variable_of_interest].notnull())]\n",
    "\n",
    "# Get the sorted unique mini_batch values\n",
    "variable_batch_values = sorted(df_mb[variable_of_interest].unique())\n",
    "n_subplots = len(variable_batch_values)\n",
    "\n",
    "# Create subplots (one per mini_batch value)\n",
    "fig, axes = plt.subplots(1, n_subplots, figsize=(4 * n_subplots, 3), sharey=True, tight_layout=True)\n",
    "\n",
    "# In case there's only one subplot, wrap axes in a list for uniformity.\n",
    "if n_subplots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, mb in zip(axes, variable_batch_values):\n",
    "    # Filter for the current mini_batch value\n",
    "    df_mb_subset = df_mb[df_mb[variable_of_interest] == mb]\n",
    "    for ds in sorted(df_mb_subset[second_variable_of_interest].unique()):\n",
    "        row = df_mb_subset[df_mb_subset[second_variable_of_interest] == ds]\n",
    "        if not row.empty:\n",
    "            steps_list = row.iloc[0]['list_steps']\n",
    "            ax.plot(range(len(steps_list)), steps_list, label=f\"{ds}\", alpha=.75)\n",
    "\n",
    "    ax.set_title(f\"Step Size Over Time for {variable_of_interest} = {mb}\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Step Size\")\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend(title=second_variable_of_interest)\n",
    "plt.savefig(f'plots/{score_model.name}/{variable_of_interest}_step_size.png')\n",
    "plt.show()"
   ],
   "id": "478a1f91225cdbf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9872d745da89c83d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
