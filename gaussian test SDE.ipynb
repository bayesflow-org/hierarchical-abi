{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gaussian on a Grid Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gaussian_test_simulator import Prior, Simulator, visualize_simulation_output, generate_synthetic_data"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cpu\")",
   "id": "70c931844e56c698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior(torch_device)\n",
    "simulator_test = Simulator()\n",
    "\n",
    "# test the simulator\n",
    "sim_test = simulator_test(prior.sample_full(1))['observable']\n",
    "visualize_simulation_output(sim_test)"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights during initialization. These weights are fixed\n",
    "    # during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_proj = x * self.W * 2 * np.pi\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "# Define the Residual Block\n",
    "class ConditionalResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, cond_dim, dropout_rate, use_spectral_norm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_dim (int): Dimensionality of the input hidden state.\n",
    "            out_dim (int): Desired dimensionality of the output.\n",
    "            cond_dim (int): Dimensionality of the conditioning vector.\n",
    "            dropout_rate (float): Dropout rate.\n",
    "            use_spectral_norm (bool): Whether to apply spectral normalization.\n",
    "        \"\"\"\n",
    "        super(ConditionalResidualBlock, self).__init__()\n",
    "\n",
    "        # First linear layer: from [in_dim; cond_dim] -> out_dim\n",
    "        self.fc1 = nn.Linear(in_dim + cond_dim, out_dim)\n",
    "        self.norm1 = nn.LayerNorm(out_dim)\n",
    "\n",
    "        # Second linear layer: from [out_dim; cond_dim] -> out_dim\n",
    "        self.fc2 = nn.Linear(out_dim + cond_dim, out_dim)\n",
    "        self.norm2 = nn.LayerNorm(out_dim)\n",
    "\n",
    "        self.activation = nn.SiLU()  # SiLU is equivalent to swish\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # If input and output dims differ, project h for the skip connection.\n",
    "        if in_dim != out_dim:\n",
    "            self.skip = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "        # Apply spectral normalization if specified.\n",
    "        if use_spectral_norm:\n",
    "            self.fc1 = nn.utils.parametrizations.spectral_norm(self.fc1)\n",
    "            self.fc2 = nn.utils.parametrizations.spectral_norm(self.fc2)\n",
    "            if self.skip is not None:\n",
    "                self.skip = nn.utils.parametrizations.spectral_norm(self.skip)\n",
    "\n",
    "    def forward(self, h, cond):\n",
    "        # h: [batch_size, in_dim]\n",
    "        # cond: [batch_size, cond_dim]\n",
    "\n",
    "        # First transformation with conditioning\n",
    "        x = torch.cat([h, cond], dim=-1)  # [batch_size, in_dim + cond_dim]\n",
    "        out = self.fc1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Second transformation with conditioning injected again\n",
    "        out = self.fc2(torch.cat([out, cond], dim=-1))\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        # Apply skip connection: if dims differ, project h first.\n",
    "        skip = self.skip(h) if self.skip is not None else h\n",
    "\n",
    "        return self.activation(out + skip)\n",
    "\n",
    "class FiLMResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, cond_dim, dropout_rate, use_spectral_norm):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.film_gamma = nn.Linear(cond_dim, out_dim)\n",
    "        self.film_beta = nn.Linear(cond_dim, out_dim)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        if in_dim != out_dim:\n",
    "            self.skip = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "        self.norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "        # Apply spectral normalization if specified.\n",
    "        if use_spectral_norm:\n",
    "            self.fc = nn.utils.parametrizations.spectral_norm(self.fc)\n",
    "            if in_dim != out_dim:\n",
    "                self.skip = nn.utils.parametrizations.spectral_norm(self.skip)\n",
    "\n",
    "    def forward(self, h, cond):\n",
    "        # h: [batch, in_dim], cond: [batch, cond_dim]\n",
    "        x = self.fc(h)\n",
    "        # Compute modulation parameters\n",
    "        gamma = self.film_gamma(cond)\n",
    "        beta = self.film_beta(cond)\n",
    "        # Apply FiLM modulation\n",
    "        x = gamma * x + beta\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(x)\n",
    "        return self.activation(x + self.skip(h))\n",
    "\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network model that computes score estimates.\n",
    "\n",
    "        Args:\n",
    "            input_dim_theta (int): Input dimension for theta.\n",
    "            input_dim_x (int): Input dimension for x.\n",
    "            input_dim_condition (int): Input dimension for the condition. Can be 0 for global score.\n",
    "            hidden_dim (int): Hidden dimension for theta network.\n",
    "            n_blocks (int): Number of residual blocks.\n",
    "            marginal_prob_std_fn (callable): Function to compute the inverse marginal probability standard deviation.\n",
    "            prediction_type (str): Type of prediction to perform. Can be 'score', 'e', 'x', or 'v'.\n",
    "            time_embed_dim (int): Dimension of time embedding.\n",
    "            dropout_rate (float): Dropout rate.\n",
    "            use_film (bool): Whether to use FiLM-residual blocks.\n",
    "            use_spectral_norm (bool): Whether to use spectral normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim_theta, input_dim_x, input_dim_condition,\n",
    "                 hidden_dim, n_blocks, marginal_prob_std_fn, prediction_type,\n",
    "                 time_embed_dim, use_film, dropout_rate, use_spectral_norm):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.marginal_prob_std_fn = marginal_prob_std_fn\n",
    "        if prediction_type not in ['score', 'e', 'x', 'v']:\n",
    "            raise ValueError(\"Invalid prediction type. Must be one of 'score', 'e', 'x', or 'v'.\")\n",
    "        self.prediction_type = prediction_type\n",
    "\n",
    "        # Gaussian random feature embedding layer for time\n",
    "        self.embed = nn.Sequential(\n",
    "            GaussianFourierProjection(embed_dim=time_embed_dim),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # Define the dimension of the conditioning vector\n",
    "        cond_dim = input_dim_x + input_dim_condition + time_embed_dim\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        # Project the concatenation of theta and the condition into hidden_dim\n",
    "        self.input_layer = nn.Linear(input_dim_theta, hidden_dim)\n",
    "\n",
    "        # Create a sequence of conditional residual blocks\n",
    "        if not use_film:\n",
    "            self.blocks = nn.ModuleList([\n",
    "                ConditionalResidualBlock(in_dim=hidden_dim, out_dim=hidden_dim if b < n_blocks - 1 else input_dim_theta,\n",
    "                                         cond_dim=cond_dim, dropout_rate=dropout_rate, use_spectral_norm=use_spectral_norm)\n",
    "                for b in range(n_blocks)\n",
    "            ])\n",
    "        else:\n",
    "            # Create a series of FiLM-residual blocks\n",
    "            self.blocks = nn.ModuleList([\n",
    "                FiLMResidualBlock(in_dim=hidden_dim, out_dim=hidden_dim if b < n_blocks - 1 else input_dim_theta,\n",
    "                                  cond_dim=cond_dim, dropout_rate=dropout_rate, use_spectral_norm=use_spectral_norm)\n",
    "                for b in range(n_blocks)\n",
    "            ])\n",
    "\n",
    "        # Final layer to get back to the theta dimension\n",
    "        self.final_linear = nn.Linear(input_dim_theta, input_dim_theta)\n",
    "        if use_spectral_norm:\n",
    "            # initialize weights close zero since we want to predict the noise (otherwise in conflict with the spectral norm)\n",
    "            nn.init.xavier_uniform_(self.final_linear.weight, gain=1e-3)\n",
    "        else:\n",
    "             # initialize weights to zero since we want to predict the noise\n",
    "            nn.init.zeros_(self.final_linear.weight)\n",
    "        nn.init.zeros_(self.final_linear.bias)\n",
    "\n",
    "        # Apply spectral normalization\n",
    "        if use_spectral_norm:\n",
    "            self.input_layer = nn.utils.parametrizations.spectral_norm(self.input_layer)\n",
    "            self.final_linear = nn.utils.parametrizations.spectral_norm(self.final_linear)\n",
    "\n",
    "    def forward(self, theta, t, x, conditions, pred_score):\n",
    "        \"\"\"\n",
    "        Forward pass of the ScoreModel.\n",
    "\n",
    "        Args:\n",
    "            theta (torch.Tensor): Input theta tensor of shape (batch_size, input_dim_theta).\n",
    "            t (torch.Tensor): Input time tensor of shape (batch_size, 1).\n",
    "            x (torch.Tensor): Input x tensor of shape (batch_size, input_dim_x).\n",
    "            conditions (torch.Tensor or None): Input condition tensor of shape (batch_size, input_dim_condition). Can be None.\n",
    "            pred_score (bool): Whether to predict the score (True) or the whatever is specified in prediction_type (False).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the network (dependent on prediction_type) or the score of shape (batch_size, input_dim_theta).\n",
    "        \"\"\"\n",
    "        # Compute a time embedding (shape: [batch, time_embed_dim])\n",
    "        t_emb = self.embed(t)\n",
    "\n",
    "        # Form the conditioning vector. If conditions is None, only x and time are used.\n",
    "        if conditions is not None:\n",
    "            cond = torch.cat([x, conditions, t_emb], dim=-1)\n",
    "        else:\n",
    "            cond = torch.cat([x, t_emb], dim=-1)\n",
    "\n",
    "        # Save the skip connection (this bypasses the blocks)\n",
    "        skip = theta.clone()\n",
    "\n",
    "        # initial input\n",
    "        h = self.input_layer(theta)\n",
    "\n",
    "        # Pass through each block, injecting the same cond at each layer\n",
    "        for block in self.blocks:\n",
    "            h = block(h, cond)\n",
    "\n",
    "        # Add the skip connection from theta (or from the input projection)\n",
    "        h = h + skip\n",
    "\n",
    "        # Final linear layer to get back to the theta dimension\n",
    "        theta_emb = self.final_linear(h)\n",
    "\n",
    "        if not pred_score:\n",
    "            # just return the prediction\n",
    "            return theta_emb\n",
    "        # return the score, depends on the prediction type\n",
    "        if self.prediction_type == 'score':\n",
    "            # prediction is the score\n",
    "            return theta_emb\n",
    "\n",
    "        # todo: check if this is correct\n",
    "        alpha, sigma = self.marginal_prob_std_fn(t)\n",
    "        if self.prediction_type == 'x':\n",
    "            # convert prediction into error\n",
    "            error = -(theta_emb * alpha - theta) / sigma\n",
    "        elif self.prediction_type == 'v':\n",
    "            # convert prediction into error\n",
    "            x = alpha * theta - sigma * theta_emb\n",
    "            error = -(x * alpha - theta) / sigma\n",
    "        else:\n",
    "            # prediction is the error\n",
    "            error = theta_emb\n",
    "\n",
    "        # divide by the std to predict the score\n",
    "        return -error / sigma\n",
    "\n",
    "\n",
    "class HierarchicalScoreModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network model that computes score estimates for a hierarchical model.\n",
    "\n",
    "        Args:\n",
    "            input_dim_theta_global (int): Input dimension for global theta.\n",
    "            input_dim_theta_local (int): Input dimension for local theta.\n",
    "            input_dim_x (int): Input dimension for x.\n",
    "            hidden_dim (int): Hidden dimension for theta network.\n",
    "            n_blocks (int): Number of residual blocks.\n",
    "            marginal_prob_std_fn (callable): Function to compute the inverse marginal probability standard deviation.\n",
    "            prediction_type (str): Type of prediction to perform. Can be 'score', 'e', 'x', or 'v'.\n",
    "            time_embed_dim (int, optional): Dimension of time embedding. Default is 16.\n",
    "            use_film (bool, optional): Whether to use FiLM-residual blocks. Default is False.\n",
    "            dropout_rate (float, optional): Dropout rate. Default is 0.1.\n",
    "            use_spectral_norm (bool, optional): Whether to use spectral normalization. Default is False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim_theta_global, input_dim_theta_local, input_dim_x,\n",
    "                 hidden_dim, n_blocks, marginal_prob_std_fn, prediction_type,\n",
    "                 time_embed_dim=16, use_film=False, dropout_rate=0.1, use_spectral_norm=False):\n",
    "        super(HierarchicalScoreModel, self).__init__()\n",
    "        self.prediction_type = prediction_type\n",
    "        self.summary_net = nn.GRU(\n",
    "            input_size=input_dim_x,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.n_params_global = input_dim_theta_global\n",
    "        self.global_model = ScoreModel(\n",
    "            input_dim_theta=input_dim_theta_global,\n",
    "            input_dim_x=hidden_dim,\n",
    "            input_dim_condition=0,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_blocks=n_blocks,\n",
    "            marginal_prob_std_fn=marginal_prob_std_fn,\n",
    "            prediction_type=prediction_type,\n",
    "            time_embed_dim=time_embed_dim,\n",
    "            use_film=use_film,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_spectral_norm=use_spectral_norm\n",
    "        )\n",
    "        self.n_params_local = input_dim_theta_local\n",
    "        self.local_model = ScoreModel(\n",
    "            input_dim_theta=input_dim_theta_local,\n",
    "            input_dim_x=hidden_dim,\n",
    "            input_dim_condition=input_dim_theta_global,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_blocks=n_blocks,\n",
    "            marginal_prob_std_fn=marginal_prob_std_fn,\n",
    "            prediction_type=prediction_type,\n",
    "            time_embed_dim=time_embed_dim,\n",
    "            use_film=use_film,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_spectral_norm=use_spectral_norm\n",
    "        )\n",
    "\n",
    "    def forward(self, theta, t, x, pred_score=False):  # __call__ method for the model\n",
    "        \"\"\"Forward pass through the global and local model. This usually only used, during training.\"\"\"\n",
    "        theta_global, theta_local = torch.split(theta, [self.n_params_global, self.n_params_local], dim=-1)\n",
    "        _, x_emb = self.summary_net(x)\n",
    "        x_emb = x_emb[0]  # only one layer, not bidirectional\n",
    "        global_out = self.global_model.forward(theta=theta_global, t=t, x=x_emb, conditions=None, pred_score=pred_score)\n",
    "        local_out = self.local_model.forward(theta=theta_local, t=t, x=x_emb, conditions=theta_global, pred_score=pred_score)\n",
    "        return torch.cat([global_out, local_out], dim=-1)\n",
    "\n",
    "    def forward_local(self, theta_local, theta_global, t, x, pred_score=True):\n",
    "        \"\"\"Forward pass through the local model. Usually we want the score, not the predicting task from training.\"\"\"\n",
    "        _, x_emb = self.summary_net(x)\n",
    "        x_emb = x_emb[0]  # only one layer, not bidirectional\n",
    "        local_out = self.local_model.forward(theta=theta_local, t=t, x=x_emb, conditions=theta_global, pred_score=pred_score)\n",
    "        return local_out\n",
    "\n",
    "    def forward_global(self, theta_global, t, x, pred_score=True):\n",
    "        \"\"\"Forward pass through the global model. Usually we want the score, not the predicting task from training.\"\"\"\n",
    "        _, x_emb = self.summary_net(x)\n",
    "        x_emb = x_emb[0]  # only one layer, not bidirectional\n",
    "        global_out = self.global_model.forward(theta=theta_global, t=t, x=x_emb, conditions=None, pred_score=pred_score)\n",
    "        return global_out"
   ],
   "id": "6d13bcb1279b6f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BETA_MIN = 0.1\n",
    "BETA_MAX = 20.0  # todo: check if this is a good value, 20 before\n",
    "\n",
    "def beta(t):\n",
    "    \"\"\"beta(t) = beta_\\text{min} + t*(beta_\\text{max} - beta_\\text{min})\"\"\"\n",
    "    return BETA_MIN + t * (BETA_MAX - BETA_MIN)\n",
    "\n",
    "def variance_preserving_kernel(t):\n",
    "    \"\"\"\n",
    "    Computes the variance-preserving kernel p(x_t | x_0) for the diffusion process.\n",
    "    Assuming beta_t is an arithmetic sequence as in DDPM models, we get the following:\n",
    "        beta(t) = beta_\\text{min} + t*(beta_\\text{max} - beta_\\text{min})\n",
    "\n",
    "    Args:\n",
    "        t (torch.Tensor): The time at which to evaluate the kernel in [0,1]. Should be not too close to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The mean and standard deviation of the kernel at time t.\n",
    "    \"\"\"\n",
    "    # mean is x(0) exp(-1/2 \\int_0^t beta(s) ds)\n",
    "    log_integral = -0.5 * (t**2) * (BETA_MAX - BETA_MIN) - 0.5 * t * BETA_MIN\n",
    "    # var is 1 - exp(- \\int_0^t beta(s) ds)\n",
    "    std = torch.sqrt(1 - torch.exp(log_integral))\n",
    "    mean = torch.exp(0.5 * log_integral)  # * x_0 is the mean\n",
    "    return mean, std\n",
    "\n",
    "def weighting_function(t):\n",
    "    \"\"\"for clarity, we define the weighting function as the inverse of the expectation of the log score\"\"\"\n",
    "    #return variance_preserving_kernel(1+1e-3-t)**2\n",
    "    #return torch.ones_like(t)\n",
    "    return beta(t)  # likelihood weighting, since beta(t) = g(t)^2\n",
    "    #return variance_preserving_kernel(t)**2\n",
    "    #return variance_preserving_kernel(t)**4\n",
    "\n",
    "def log_grad_kernel(x, x0, t):\n",
    "    # Compute log_mean, m and std as in your kernel\n",
    "    log_mean = -0.25 * (t**2) * (BETA_MAX - BETA_MIN) - 0.5 * t * BETA_MIN\n",
    "    m = x0 * torch.exp(log_mean)\n",
    "    std = torch.sqrt(1 - torch.exp(2. * log_mean))\n",
    "    var = std ** 2  # var = 1 - exp(2 * log_mean)\n",
    "\n",
    "    # Gradient of log p(x|x0) wrt x\n",
    "    grad_log_p = -(x - m) / var\n",
    "    return grad_log_p"
   ],
   "id": "3fa8ea51af71f5e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_weighted_time_points(num_points, num_grid=10000, device=None):\n",
    "    \"\"\"\n",
    "    Generate time points between 0 and 1 with spacing proportional to weighting_function(t).\n",
    "\n",
    "    The idea is to compute the cumulative density function (CDF)\n",
    "    from the weighting function via numerical integration and then\n",
    "    invert the CDF at equally spaced values.\n",
    "\n",
    "    Args:\n",
    "        num_points (int): Number of time points to generate.\n",
    "        num_grid (int, optional): Number of points in the fine grid for numerical integration.\n",
    "        device (torch.device, optional): Device to perform computation on. Defaults to CPU.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (num_points,) containing the generated time points.\n",
    "    \"\"\"\n",
    "    # Create a fine grid over [0, 1]\n",
    "    t_grid = torch.linspace(0, 1, steps=num_grid, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Evaluate the weighting function on this grid\n",
    "    weights = weighting_function(t_grid)\n",
    "\n",
    "    # Ensure weights are non-negative\n",
    "    if (weights < 0).any():\n",
    "        raise ValueError(\"weighting_function returned negative values.\")\n",
    "\n",
    "    # Use the trapezoidal rule to compute the cumulative integral (CDF)\n",
    "    dt = t_grid[1] - t_grid[0]\n",
    "    cumulative = torch.cumsum(weights, dim=0) * dt\n",
    "\n",
    "    # Normalize so that the final value is 1 (i.e. form a proper CDF)\n",
    "    cumulative = cumulative / cumulative[-1]\n",
    "\n",
    "    # Create equally spaced values in [0,1] which serve as target CDF values\n",
    "    target_cdf = torch.linspace(0, 1, steps=num_points, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Invert the CDF: for each target value, find the corresponding t\n",
    "    t_points = torch.tensor(np.interp(target_cdf.cpu().numpy(),\n",
    "                                          cumulative.cpu().numpy(),\n",
    "                                          t_grid.cpu().numpy()), dtype=torch.float32, device=device)\n",
    "    return t_points"
   ],
   "id": "8d25006405ad24bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_diffusion_time(size, epsilon=5e-3, return_batch=False, weighted_time=False, device=None):\n",
    "    \"\"\"\n",
    "    Generates equally spaced diffusion time values in [epsilon,1].\n",
    "    The time is generated uniformly in [epsilon, 1] if return_batch is True.\n",
    "    \"\"\"\n",
    "    if not return_batch and not weighted_time:\n",
    "        time = torch.linspace(epsilon, 1, steps=size, dtype=torch.float32, device=device)\n",
    "        return time\n",
    "    if weighted_time:\n",
    "        # make time step size proportional to the weighting function\n",
    "        time = generate_weighted_time_points(num_points=size, device=device)\n",
    "        if return_batch:\n",
    "            return time.unsqueeze(1)\n",
    "        return time\n",
    "\n",
    "    #time = torch.rand(size, dtype=torch.float32, device=device) * (1 - epsilon) + epsilon\n",
    "    beta_dist = torch.distributions.Beta(1, 3)\n",
    "    samples = beta_dist.sample((size,))\n",
    "    time = epsilon + (1 - epsilon) * samples\n",
    "    # low discrepancy sequence\n",
    "    # t_i = \\mod (u_0 + i/k, 1)\n",
    "    #u0 = torch.rand(1, dtype=torch.float32, device=device)\n",
    "    #i = torch.arange(0, size, dtype=torch.float32, device=device)  # i as a tensor of indices\n",
    "    #time = ((u0 + i / size) % 1) * (1 - epsilon) + epsilon\n",
    "    #time, _ = time.sort()\n",
    "\n",
    "    # Add a new dimension so that each tensor has shape (size, 1)\n",
    "    return time.unsqueeze(1)"
   ],
   "id": "8c190dd2306e03e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.hist(generate_diffusion_time(10000, return_batch=True), bins=50)",
   "id": "bdaad23a7a094a0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the kernel\n",
    "t = generate_diffusion_time(100)\n",
    "beta_t = beta(t)\n",
    "plt.plot(t, torch.sqrt(beta_t), label='beta_t')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Noise Level')\n",
    "plt.show()\n",
    "\n",
    "# plot the kernel\n",
    "m, std = variance_preserving_kernel(t)\n",
    "plt.plot(t, m, label='mean')\n",
    "plt.plot(t, std**2, label='variance')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Variance Preserving Kernel')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, weighting_function(t), label='weighting')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Weighting Function')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, 1/std, label='score scaling')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('score scaling')\n",
    "plt.show()"
   ],
   "id": "3985a1dbd9349209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_score_loss(theta_batch, x_batch, model):\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=theta_batch.shape[0], return_batch=True, device=theta_batch.device)\n",
    "\n",
    "    # sample from the Gaussian kernel, just learn the noise\n",
    "    epsilon = torch.randn_like(theta_batch, dtype=theta_batch.dtype, device=theta_batch.device)\n",
    "\n",
    "    # perturb the theta batch\n",
    "    alpha, sigma = variance_preserving_kernel(t=diffusion_time)\n",
    "    z = alpha * theta_batch + sigma * epsilon\n",
    "    # predict from perturbed theta\n",
    "    pred = model(theta=z, t=diffusion_time, x=x_batch)\n",
    "    snr = torch.log(torch.square(alpha)) - torch.log(torch.square(sigma))\n",
    "\n",
    "    if model.prediction_type == 'score':\n",
    "        target = log_grad_kernel(x=z, x0=theta_batch, t=diffusion_time)\n",
    "        pred_type_weight = 1\n",
    "    elif model.prediction_type == 'e':\n",
    "        # divide by the std to learn the score\n",
    "        pred_type_weight = 1 / torch.square(sigma)\n",
    "        target = epsilon\n",
    "    elif model.prediction_type == 'x':\n",
    "        target = theta_batch\n",
    "        # divide by the std to learn the score\n",
    "        pred_type_weight = torch.exp(-snr) / torch.square(sigma)\n",
    "    elif model.prediction_type == 'v':\n",
    "        target = alpha * epsilon - sigma * theta_batch\n",
    "        # divide by the std to learn the score\n",
    "        pred_type_weight = torch.square(alpha) * torch.square(torch.exp(-snr) - 1) / torch.square(sigma)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prediction type. Must be one of 'score', 'e', 'x', or 'v'.\")\n",
    "\n",
    "    effective_weight = pred_type_weight * weighting_function(diffusion_time)\n",
    "    # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "    loss = torch.mean(effective_weight * torch.sum(torch.square(pred - target), dim=-1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, dataloader_valid=None, epochs=100, lr=1e-3, device=None):\n",
    "    score_model.to(torch_device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Add Cosine Annealing Scheduler\n",
    "    #scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    loss_history = np.zeros((epochs, 2))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        # for each sample in the batch, calculate the loss for a random diffusion time\n",
    "        for theta_global_batch, theta_local_batch, x_batch in dataloader:\n",
    "            # initialize the gradients\n",
    "            optimizer.zero_grad()\n",
    "            theta_batch = torch.concat([theta_global_batch, theta_local_batch], dim=-1)\n",
    "            theta_batch = theta_batch.to(device)\n",
    "            # calculate the loss\n",
    "            loss = compute_score_loss(theta_batch=theta_batch, x_batch=x_batch, model=model)\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "        #scheduler.step()\n",
    "\n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        if dataloader_valid is not None:\n",
    "            for theta_global_batch, theta_local_batch, x_batch in dataloader_valid:\n",
    "                with torch.no_grad():\n",
    "                    theta_batch = torch.concat([theta_global_batch, theta_local_batch], dim=-1)\n",
    "                    theta_batch = theta_batch.to(device)\n",
    "                    loss = compute_score_loss(theta_batch=theta_batch, x_batch=x_batch, model=model)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "        loss_history[epoch] = [np.mean(total_loss), np.mean(valid_loss)]\n",
    "        print_str = f\"Epoch {epoch+1}/{epochs}, Loss: {np.mean(total_loss):.4f}, \"\\\n",
    "                    f\"Valid Loss: {np.mean(valid_loss):.4f}\"\n",
    "        print(print_str, end='\\r')\n",
    "        # Update the checkpoint after each epoch of training.\n",
    "        #torch.save(model.state_dict(), 'ckpt.pth')\n",
    "    return loss_history"
   ],
   "id": "3e4488af88ad4738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "n_data = 25000\n",
    "batch_size = 128\n",
    "\n",
    "# Create model and dataset\n",
    "thetas_global, thetas_local, xs = generate_synthetic_data(prior, n_data=n_data, normalize=True)\n",
    "\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(thetas_global, thetas_local, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create validation data\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_data=batch_size*2, normalize=True)\n",
    "dataset_valid = TensorDataset(valid_prior_global, valid_prior_local, valid_data)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x=1,\n",
    "    hidden_dim=64,\n",
    "    n_blocks=3,\n",
    "    time_embed_dim=16,\n",
    "    use_film=True,\n",
    "    marginal_prob_std_fn=partial(variance_preserving_kernel),\n",
    "    use_spectral_norm=False\n",
    ")"
   ],
   "id": "ded414838e34143f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 epochs=1000, lr=1e-4, device=torch_device)\n",
    "score_model.eval();"
   ],
   "id": "ab5eef7cdb5ec27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "torch.save(score_model.state_dict(), \"score_model.pt\")",
   "id": "70dd78d9bf8db4e3"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "score_model.load_state_dict(torch.load(\"score_model.pt\", weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "bec5a801baa5bda3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Mean Train')\n",
    "plt.plot(loss_history[:, 1], label='Mean Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "aca578e1fb265da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_error = {}\n",
    "loss_list_score = {}\n",
    "loss_list_w = {}\n",
    "loss_list_w2 = {}\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=50, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        for theta_global_batch, theta_local_batch, x_batch in dataloader_valid:\n",
    "            theta_batch = torch.cat([theta_global_batch, theta_local_batch], dim=-1)\n",
    "            theta_batch = theta_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon = torch.randn_like(theta_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_batch.shape[0], 1), t, dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            alpha, sigma = variance_preserving_kernel(t=t_tensor)\n",
    "            z = alpha * theta_batch + sigma * epsilon\n",
    "            snr = torch.log(torch.square(alpha)) - torch.log(torch.square(sigma))\n",
    "            # predict from perturbed theta\n",
    "            pred = score_model(theta=z, t=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score = score_model(theta=z, t=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score = log_grad_kernel(x=z, x0=theta_batch, t=t_tensor)\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target = log_grad_kernel(x=z, x0=theta_batch, t=t_tensor)\n",
    "                pred_type_weight = 1\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                # divide by the std to learn the score\n",
    "                pred_type_weight = 1 / torch.square(sigma)\n",
    "                target = epsilon\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target = theta_batch\n",
    "                # divide by the std to learn the score\n",
    "                pred_type_weight = torch.exp(-snr) / torch.square(sigma)\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target = alpha * epsilon - sigma * theta_batch\n",
    "                # divide by the std to learn the score\n",
    "                pred_type_weight = torch.square(alpha) * torch.square(torch.exp(-snr) - 1) / torch.square(sigma)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type. Must be one of 'score', 'e', 'x', or 'v'.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss = torch.mean(torch.sum(torch.square(pred - target), dim=-1))\n",
    "            loss_list_error[t.item()] = loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss = torch.mean(torch.sum(torch.square(pred_score - true_score), dim=-1))\n",
    "            loss_list_score[t.item()] = loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            loss = torch.mean(weighting_function(t) * pred_type_weight * torch.sum(torch.square(pred - target), dim=-1))\n",
    "            loss_list_w[t.item()] = loss.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss2 = torch.mean(weighting_function(t) * torch.sum(torch.square(pred_score - true_score), dim=-1))\n",
    "            loss_list_w2[t.item()] = loss2.item()"
   ],
   "id": "3e3644f3daaa5484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_score_w = pd.DataFrame(loss_list_w.items(), columns=['Time', 'Loss'])\n",
    "df_score_w2 = pd.DataFrame(loss_list_w2.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, sharex=True, figsize=(12, 3), tight_layout=True)\n",
    "ax[0].plot(df_error['Time'], df_error['Loss'], label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], df_score['Loss'], label='Score Loss')\n",
    "ax[2].plot(df_score_w['Time'], df_score_w['Loss'], label='Weighted Loss (as in Optimization)')\n",
    "ax[2].plot(df_score_w2['Time'], df_score_w2['Loss'], label='Weighted Loss on Scores')  # should be the same as the loss in optimization\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Loss')\n",
    "    a.legend()\n",
    "ax[-1].set_ylabel('Weighted Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time, weighting_function(diffusion_time), label='assumed weighting')\n",
    "#plt.plot(diffusion_time, weighting_function(diffusion_time) / (variance_preserving_kernel(diffusion_time)**2),\n",
    "#         label='effective weighting', alpha=0.5)\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "75adebcae950de41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sample from the Score Model",
   "id": "63a60228e18332cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def euler_maruyama_step(x, score, t, dt):\n",
    "    \"\"\"\n",
    "    Perform one Euler-Maruyama update step for the SDE\n",
    "\n",
    "    Parameters:\n",
    "        x: Current state (PyTorch tensor)\n",
    "        score: Score network output (PyTorch tensor)\n",
    "        t: Current time (scalar tensor or float)\n",
    "        dt: Time step size (float)\n",
    "\n",
    "    Returns:\n",
    "        z_next: Updated state after time dt.\n",
    "    \"\"\"\n",
    "    # Compute g(t)^2\n",
    "    g_t2 = beta(t)\n",
    "    # Compute f(x,t)\n",
    "    f_x_t = -0.5 * g_t2 * x\n",
    "    # Compute drift and diffusion\n",
    "    drift = f_x_t - g_t2 * score\n",
    "    diffusion = torch.sqrt(g_t2)\n",
    "\n",
    "    # Sample Gaussian noise (same shape as x), with variance dt\n",
    "    noise = torch.randn_like(x, dtype=x.dtype, device=x.device) * torch.sqrt(dt)\n",
    "\n",
    "    # Eulerâ€“Maruyama update step\n",
    "    x_mean = x + drift * dt\n",
    "    x_next = x_mean + diffusion * noise\n",
    "    return x_next, x_mean"
   ],
   "id": "61887926b5939d74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def eval_compositional_score(model, theta, diffusion_time, x_expanded, n_post_samples, n_obs, conditions_exp):\n",
    "    # Create tensors for current time step\n",
    "    t_exp = diffusion_time.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "    # Compute model scores\n",
    "    if conditions_exp is None: # compositional global scores\n",
    "        theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, prior.n_params_global)\n",
    "        model_indv_scores = model.forward_global(theta_global=theta_exp, t=t_exp, x=x_expanded)\n",
    "        # Sum over observations\n",
    "        model_sum_scores = model_indv_scores.reshape(n_post_samples, n_obs, -1).sum(dim=1)\n",
    "\n",
    "        # Compute prior score\n",
    "        prior_score = prior.score_global_batch(theta)\n",
    "        model_scores = (1 - n_obs) * (1 - diffusion_time) / 1 * prior_score + model_sum_scores\n",
    "    else:  # not compositional local scores\n",
    "        theta_exp = theta.reshape(-1, prior.n_params_local)\n",
    "        model_scores = model.forward_local(theta_local=theta_exp, t=t_exp, x=x_expanded, theta_global=conditions_exp)\n",
    "        model_scores = model_scores.reshape(n_post_samples, n_obs, -1)\n",
    "    return model_scores"
   ],
   "id": "18d14cc66f30fe35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Euler-Maruyama Sampling from Song et al. (2021)\n",
    "def euler_maruyama_sampling(model, x_obs, n_post_samples, conditions=None, diffusion_steps=100, device=None):\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Ensure x_obs is a PyTorch tensor\n",
    "        if not isinstance(x_obs, torch.Tensor):\n",
    "            x_obs = torch.tensor(x_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "        x_obs_norm = prior.normalize_data(x_obs)\n",
    "        x_obs_norm = x_obs_norm.reshape(x_obs_norm.shape[0], -1)\n",
    "        n_obs = x_obs_norm.shape[-1]\n",
    "        n_time_steps = x_obs_norm.shape[0]\n",
    "        x_obs_norm = x_obs_norm.T[:, :, None]\n",
    "\n",
    "        # Initialize parameters\n",
    "        if conditions is None:  # global\n",
    "            n_params = prior.n_params_global\n",
    "            theta = torch.randn(n_post_samples, prior.n_params_global, dtype=torch.float32, device=device) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32, device=device))\n",
    "            conditions_exp = None\n",
    "        else:\n",
    "            # Ensure conditions is a PyTorch tensor\n",
    "            if not isinstance(conditions, torch.Tensor):\n",
    "                conditions = torch.tensor(conditions, dtype=torch.float32, device=device)\n",
    "            conditions_norm = prior.normalize_theta(conditions, global_params=True)\n",
    "\n",
    "            n_params = prior.n_params_local*n_obs\n",
    "            theta = torch.randn(n_post_samples, n_obs, prior.n_params_local, dtype=torch.float32, device=device)\n",
    "            conditions_exp = conditions_norm.unsqueeze(0).expand(n_post_samples, n_obs, -1).reshape(-1, prior.n_params_global)\n",
    "\n",
    "        # Generate diffusion time parameters\n",
    "        diffusion_time = generate_diffusion_time(size=diffusion_steps+1, device=device, weighted_time=True) # todo: weighed times\n",
    "\n",
    "        # Expand x_obs_norm to match the number of posterior samples\n",
    "        x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, n_obs, n_time_steps, -1)  # Shape: (n_post_samples, n_obs, n_time_steps, d)\n",
    "        x_expanded = x_exp.reshape(n_post_samples*n_obs, n_time_steps, -1)\n",
    "\n",
    "        # Reverse iterate over diffusion times and step sizes\n",
    "        for t in tqdm(reversed(range(1, diffusion_steps+1)), total=diffusion_steps):\n",
    "            t_tensor = torch.full((n_post_samples, 1), diffusion_time[t], dtype=torch.float32, device=device)\n",
    "            scores = eval_compositional_score(model=model, theta=theta, diffusion_time=t_tensor, x_expanded=x_expanded,\n",
    "                                              n_obs=n_obs, n_post_samples=n_post_samples,\n",
    "                                              conditions_exp=conditions_exp)\n",
    "            # Make Euler-Maruyama step\n",
    "            if t == 1:\n",
    "                # do not include noise in the last step\n",
    "                _, theta = euler_maruyama_step(theta, score=scores, t=diffusion_time[t],\n",
    "                                              dt=diffusion_time[t] - diffusion_time[t-1])\n",
    "            else:\n",
    "                theta, _ = euler_maruyama_step(theta, score=scores, t=diffusion_time[t],\n",
    "                                               dt=diffusion_time[t] - diffusion_time[t-1])\n",
    "            # clip theta to avoid numerical issues\n",
    "            #theta = torch.clamp(theta, -5, 5)\n",
    "            if torch.isnan(theta).any():\n",
    "                print(\"NaNs in theta\")\n",
    "                break\n",
    "        # correct for normalization\n",
    "        if conditions is None:\n",
    "            theta = prior.denormalize_theta(theta, global_params=True)\n",
    "            # convert to numpy\n",
    "            theta = theta.detach().numpy().reshape(n_post_samples, prior.n_params_global)\n",
    "        else:\n",
    "            theta = prior.denormalize_theta(theta, global_params=False)\n",
    "            # convert to numpy\n",
    "            theta = theta.detach().numpy().reshape(n_post_samples, n_obs, prior.n_params_local)\n",
    "    return theta"
   ],
   "id": "ffbd8811e27ede9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Probability ODE from Song et al. (2021)\n",
    "from scipy.integrate import solve_ivp\n",
    "def probability_ode_solving(model, x_obs, n_post_samples, conditions=None, device=None):\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Ensure x_obs is a PyTorch tensor\n",
    "        if not isinstance(x_obs, torch.Tensor):\n",
    "            x_obs = torch.tensor(x_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "        x_obs_norm = prior.normalize_data(x_obs)\n",
    "        x_obs_norm = x_obs_norm.reshape(x_obs_norm.shape[0], -1)\n",
    "        n_obs = x_obs_norm.shape[-1]\n",
    "        n_time_steps = x_obs_norm.shape[0]\n",
    "        x_obs_norm = x_obs_norm.T[:, :, None]\n",
    "\n",
    "\n",
    "        # Initialize parameters\n",
    "        if conditions is None:  # global\n",
    "            theta = torch.randn(n_post_samples, prior.n_params_global, dtype=torch.float32, device=device) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32, device=device))\n",
    "            conditions_exp = None\n",
    "        else:\n",
    "            # Ensure conditions is a PyTorch tensor\n",
    "            if not isinstance(conditions, torch.Tensor):\n",
    "                conditions = torch.tensor(conditions, dtype=torch.float32, device=device)\n",
    "            conditions_norm = prior.normalize_theta(conditions, global_params=True)\n",
    "\n",
    "            theta = torch.randn(n_post_samples, n_obs, prior.n_params_local, dtype=torch.float32, device=device)\n",
    "            conditions_exp = conditions_norm.unsqueeze(0).expand(n_post_samples, n_obs, -1).reshape(-1, prior.n_params_global)\n",
    "\n",
    "        # Expand x_obs_norm to match the number of posterior samples\n",
    "        x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, n_obs, n_time_steps, -1)  # Shape: (n_post_samples, n_obs, n_time_steps, d)\n",
    "        x_expanded = x_exp.reshape(n_post_samples*n_obs, n_time_steps, -1)\n",
    "\n",
    "        # Reverse iterate over diffusion times and step sizes\n",
    "\n",
    "        def probability_ode(t, x):\n",
    "            t_tensor = torch.full((n_post_samples, 1), t, dtype=torch.float32, device=device)\n",
    "\n",
    "            if conditions is None:\n",
    "                x_torch = torch.tensor(x.reshape(n_post_samples, prior.n_params_global), dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                x_torch = torch.tensor(x.reshape(n_post_samples, n_obs, prior.n_params_local), dtype=torch.float32, device=device)\n",
    "            scores = eval_compositional_score(model=model, theta=x_torch, diffusion_time=t_tensor, x_expanded=x_expanded,\n",
    "                                              n_obs=n_obs, n_post_samples=n_post_samples,\n",
    "                                              conditions_exp=conditions_exp)\n",
    "            if conditions is None:\n",
    "                t_exp = t_tensor\n",
    "            else:\n",
    "                t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1)\n",
    "            # Compute g(t)^2\n",
    "            g_t2 = beta(t_exp)\n",
    "            # Compute f(x,t)\n",
    "            f_x_t = -0.5 * g_t2 * x_torch\n",
    "            # Compute drift and diffusion\n",
    "            drift = f_x_t - 0.5*g_t2 * scores\n",
    "            if conditions is None:\n",
    "                return drift.detach().numpy().reshape(n_post_samples * prior.n_params_global)\n",
    "            return drift.detach().numpy().reshape(n_post_samples * n_obs * prior.n_params_local)\n",
    "\n",
    "        if conditions is None:\n",
    "            x_0 = theta.detach().numpy().reshape(n_post_samples * prior.n_params_global)\n",
    "        else:\n",
    "            x_0 = theta.detach().numpy().reshape(n_post_samples * n_obs * prior.n_params_local)\n",
    "        sol = solve_ivp(probability_ode, t_span=[1, 5e-3], y0=x_0, method='RK45', t_eval=[5e-3])\n",
    "        print('ODE solved:', sol.success)\n",
    "        if not sol.success:\n",
    "            print(sol.message)\n",
    "        if conditions is None:\n",
    "            theta = torch.tensor(sol.y[:, -1].reshape(n_post_samples, prior.n_params_global), dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            theta = torch.tensor(sol.y[:, -1].reshape(n_post_samples, n_obs, prior.n_params_local), dtype=torch.float32, device=device)\n",
    "\n",
    "        # correct for normalization\n",
    "        if conditions is None:\n",
    "            theta = prior.denormalize_theta(theta, global_params=True)\n",
    "            # convert to numpy\n",
    "            theta = theta.detach().numpy().reshape(n_post_samples, prior.n_params_global)\n",
    "        else:\n",
    "            theta = prior.denormalize_theta(theta, global_params=False)\n",
    "            # convert to numpy\n",
    "            theta = theta.detach().numpy().reshape(n_post_samples, n_obs, prior.n_params_local)\n",
    "    return theta"
   ],
   "id": "7955075d18558aef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 8\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_data=10, grid_size=n_grid, full_grid=True,\n",
    "                                                                            normalize=False)\n",
    "n_post_samples = 2"
   ],
   "id": "28d63d6aa09d9245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   diffusion_steps=1000,\n",
    "                                                          device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "70cdf393b7169444",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([probability_ode_solving(score_model, vd, n_post_samples=n_post_samples, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$']);",
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$']);"
   ],
   "id": "e41154e220b31ef5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "conditions_global = np.median(posterior_global_samples_valid, axis=0)\n",
    "posterior_local_samples_valid = np.array([euler_maruyama_sampling(score_model, vd,\n",
    "                                                                  n_post_samples=n_post_samples, conditions=c,\n",
    "                                                                  diffusion_steps=1000, device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, conditions_global)])"
   ],
   "id": "cf4b764cd72fdba7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conditions_global = np.median(posterior_global_samples_valid, axis=1)\n",
    "posterior_local_samples_valid = np.array([probability_ode_solving(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                  conditions=c,\n",
    "                                                                  device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, conditions_global)])"
   ],
   "id": "3a45ca1277eefae1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          param_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "44320e5b15ea6349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1).shape, np.array(valid_prior_local).reshape(valid_data.shape[0], -1).shape",
   "id": "c3112a554555ed8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.array(valid_prior_local).reshape(valid_data.shape[0], -1)[0].shape",
   "id": "a04ae127939457cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "conditions_global",
   "id": "df4cccb79ef1633d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Number of parameters\n",
    "n_params = 64\n",
    "# Empirical estimates for each parameter\n",
    "empirical = np.array(valid_prior_local).reshape(valid_data.shape[0], -1)[0]\n",
    "# Global means for each parameter\n",
    "global_mean = np.ones_like(empirical) * conditions_global[0, 0]\n",
    "# Global standard deviations (optional, for reference)\n",
    "global_std = np.ones_like(empirical) * conditions_global[0, 1]\n",
    "\n",
    "# Compute posterior means and standard deviations\n",
    "posterior_samples = posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1)\n",
    "posterior_mean = np.mean(posterior_samples, axis=0)\n",
    "posterior_std = np.std(posterior_samples, axis=0)\n",
    "\n",
    "print(n_params, empirical.shape, global_mean.shape, global_std.shape, posterior_mean.shape, posterior_std.shape)\n",
    "\n",
    "# Compute shrinkage ratios for each parameter.\n",
    "# Avoid division by zero if any empirical equals the global mean.\n",
    "epsilon = 1e-8\n",
    "shrinkage = ( (empirical - global_mean) - (posterior_mean - global_mean) ) / (empirical - global_mean + epsilon)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "indices = np.arange(n_params)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Empirical vs Posterior vs Global\n",
    "plt.subplot(1, 2, 1)\n",
    "print(indices, empirical)\n",
    "plt.plot(indices, empirical, 'o', label='Empirical')\n",
    "plt.errorbar(indices, posterior_mean, yerr=posterior_std, fmt='s', label='Posterior Mean')\n",
    "plt.plot(indices, global_mean, 'x', label='Global Mean', markersize=10, color='red')\n",
    "# Draw vertical dashed lines to show the movement from empirical to posterior\n",
    "for i in indices:\n",
    "    plt.plot([i, i], [empirical[i], posterior_mean[i]], 'k--', lw=0.8)\n",
    "plt.xlabel('Parameter Index')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.title('Empirical, Posterior, and Global Estimates')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Shrinkage Ratios\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(indices, shrinkage, color='skyblue')\n",
    "plt.xlabel('Parameter Index')\n",
    "plt.ylabel('Shrinkage Ratio')\n",
    "plt.title('Shrinkage Ratio per Parameter\\n(0: No Shrinkage, 1: Full Shrinkage)')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "75d08ff06afc9160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 0\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 0]), np.std(posterior_global_samples_valid[valid_id, :, 0]))\n",
    "print('log tau:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][0].item())\n",
    "print('log tau:', valid_prior_global[valid_id][1].item())"
   ],
   "id": "c4bb5620888ab007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "med = np.median(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "std = np.std(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "error = (med-valid_prior_local[valid_id].numpy())**2\n",
    "visualize_simulation_output(np.stack((med, valid_prior_local[valid_id], )),\n",
    "                            title_prefix=['Posterior Median', 'True'])\n",
    "\n",
    "visualize_simulation_output(np.stack((std, error)), title_prefix=['Uncertainty', 'Error'], same_scale=False)"
   ],
   "id": "c9daf84cada1b0a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fb502aa27eeab6e6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
