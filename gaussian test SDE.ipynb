{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gaussian on a Grid Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from helper_functions import generate_diffusion_time\n",
    "from diffusion_sampling import euler_maruyama_sampling, adaptive_sampling, probability_ode_solving, langevin_sampling\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, weighting_function\n",
    "\n",
    "from gaussian_test_simulator import Prior, Simulator, visualize_simulation_output, generate_synthetic_data"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cpu\")",
   "id": "70c931844e56c698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior()\n",
    "simulator_test = Simulator()\n",
    "\n",
    "# test the simulator\n",
    "sim_test = simulator_test(prior.sample_full(1))['observable']\n",
    "visualize_simulation_output(sim_test)"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the different snrs\n",
    "t = torch.linspace(0, 1, 1000)\n",
    "bins = 100\n",
    "\n",
    "sub_vp = ['', 'sub_'][0]\n",
    "\n",
    "fig, ax = plt.subplots(2, 6, sharex='col', sharey='col', figsize=(12, 6), tight_layout=True)\n",
    "for a, sub_vp in zip(ax, ['', 'sub_']):\n",
    "    snr = SDE(sub_vp+'variance_preserving', 'linear').get_snr(t)\n",
    "    h1 = a[0].plot(t, snr, label='linear', alpha=0.75)\n",
    "    a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "    alpha, sigma = SDE(sub_vp+'variance_preserving', 'linear').kernel(t)\n",
    "    a[2].plot(t, alpha, label='linear', alpha=0.75)\n",
    "    a[3].plot(t, sigma, label='linear', alpha=0.75)\n",
    "    a[4].plot(t, alpha**2+sigma**2, label='linear', alpha=0.75)\n",
    "    a[5].plot(t, alpha+sigma, label='linear', alpha=0.75)\n",
    "\n",
    "    snr = SDE(sub_vp+'variance_preserving', 'cosine').get_snr(t)\n",
    "    h2 = a[0].plot(t, snr, label='cosine', alpha=0.75)\n",
    "    a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "    alpha, sigma = SDE(sub_vp+'variance_preserving', 'cosine').kernel(t)\n",
    "    a[2].plot(t, alpha, label='cosine', alpha=0.75)\n",
    "    a[3].plot(t, sigma, label='cosine', alpha=0.75)\n",
    "    a[4].plot(t, alpha**2+sigma**2, label='cosine', alpha=0.75)\n",
    "    a[5].plot(t, alpha+sigma, label='cosine', alpha=0.75)\n",
    "\n",
    "    snr = SDE(sub_vp+'variance_preserving', 'cosine', s_shift_cosine=0.8).get_snr(t)\n",
    "    h3 = a[0].plot(t, snr, label='shifted_cosine', alpha=0.75)\n",
    "    a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "    alpha, sigma = SDE(sub_vp+'variance_preserving', 'cosine', s_shift_cosine=0.8).kernel(t)\n",
    "    a[2].plot(t, alpha, label='shifted_cosine', alpha=0.75)\n",
    "    a[3].plot(t, sigma, label='shifted_cosine', alpha=0.75)\n",
    "    a[4].plot(t, alpha**2+sigma**2, label='shifted_cosine', alpha=0.75)\n",
    "    a[5].plot(t, alpha+sigma, label='shifted_cosine', alpha=0.75)\n",
    "\n",
    "    if sub_vp == 'sub_':\n",
    "        snr = SDE(sub_vp+'variance_preserving', 'flow_matching').get_snr(t)\n",
    "        h4 = a[0].plot(t, snr, label='flow_matching', alpha=0.75)\n",
    "        a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "        alpha, sigma = SDE(sub_vp+'variance_preserving', 'flow_matching').kernel(t)\n",
    "        a[2].plot(t, alpha, label='flow_matching', alpha=0.75)\n",
    "        a[3].plot(t, sigma, label='flow_matching', alpha=0.75)\n",
    "        a[4].plot(t, alpha**2+sigma**2, label='flow_matching', alpha=0.75)\n",
    "        a[5].plot(t, alpha+sigma, label='flow_matching', alpha=0.75)\n",
    "\n",
    "    # dotted line at 0\n",
    "    a[0].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for axis in a:\n",
    "        axis.set_xlabel('t')\n",
    "    a[1].set_xlabel('snr')\n",
    "    a[0].set_ylabel(f'snr\\n{sub_vp}variance_preserving')\n",
    "    a[0].set_title(f'Signal-to-noise ratio')\n",
    "    a[1].set_title(f'Signal-to-noise ratio')\n",
    "    a[2].set_title(f'alpha(t)')\n",
    "    a[3].set_title(f'sigma(t)')\n",
    "    a[4].set_title(f'alpha(t)^2 + sigma(t)^2')\n",
    "    a[5].set_title(f'alpha(t) + sigma(t)')\n",
    "fig.legend(handles=h1+h2+h3+h4, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig('noise schedules.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "4be4f20621659931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t = torch.linspace(0, 1, 1000)\n",
    "x_0 = torch.tensor([0.01])\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, sharex='col', sharey='col', figsize=(5, 4), tight_layout=True)\n",
    "for a, sub_vp in zip(ax, ['', 'sub_']):\n",
    "    f, g = SDE(sub_vp+'variance_preserving', 'linear').get_f_g(t, x=x_0)\n",
    "    h1 = a[0].plot(t, f, label='linear', alpha=0.75)\n",
    "    a[1].plot(t, g, label='linear', alpha=0.75)\n",
    "\n",
    "    f, g = SDE(sub_vp+'variance_preserving', 'cosine').get_f_g(t, x=x_0)\n",
    "    h2 = a[0].plot(t, f, label='cosine', alpha=0.75)\n",
    "    a[1].plot(t, g, label='cosine', alpha=0.75)\n",
    "\n",
    "    #f, g = SDE(sub_vp+'variance_preserving', 'cosine', s_shift_cosine=0.8).get_f_g(t, x=x_0)\n",
    "    #h3 = a[0].plot(t, f, label='shifted_cosine', alpha=0.75)\n",
    "    #a[1].plot(t, g, label='shifted_cosine', alpha=0.75)\n",
    "\n",
    "    if sub_vp == 'sub_':\n",
    "        f, g = SDE(sub_vp+'variance_preserving', 'flow_matching').get_f_g(t, x=x_0)\n",
    "        h4 = a[0].plot(t, f, label='flow_matching', alpha=0.75)\n",
    "        a[1].plot(t, g, label='flow_matching', alpha=0.75)\n",
    "\n",
    "    for axis in a:\n",
    "        axis.set_xlabel('t')\n",
    "    a[0].set_ylabel(f'snr\\n{sub_vp}variance_preserving')\n",
    "    a[0].set_title(f'f(z={round(x_0.item(), 2)},t)')\n",
    "    a[1].set_title(f'g(t)')\n",
    "fig.legend(handles=h1+h2+h4, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.05))\n",
    "#plt.savefig('noise schedules.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "9b1736d64b850509",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the kernel\n",
    "t = generate_diffusion_time(1000)\n",
    "sde_test = SDE('variance_preserving', 'cosine')\n",
    "snr = sde_test.get_snr(t)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, sharex='row', sharey='row', figsize=(12, 6), tight_layout=True)\n",
    "for a, wt in zip(ax.T, [None, 'likelihood_weighting', 'flow_matching', 'sigmoid', 'min-snr']):\n",
    "    w = weighting_function(t, sde_test, weighting_type=wt)\n",
    "    a[0].plot(t, w / max(w))\n",
    "    a[0].set_xlabel(r'$t$')\n",
    "    a[0].set_ylabel('Normalized weight')\n",
    "    a[0].set_title(wt)\n",
    "\n",
    "    a[1].plot(snr, w / max(w))\n",
    "    a[1].set_xlabel(r'$\\lambda$')\n",
    "    a[1].set_ylabel('Normalized weight')\n",
    "    a[0].set_title(wt)\n",
    "#plt.savefig(f'weighting_functions_{sde_test.noise_schedule}.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "3985a1dbd9349209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_score_loss(theta_batch, x_batch, model):\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=theta_batch.shape[0], return_batch=True, device=theta_batch.device)\n",
    "\n",
    "    # sample from the Gaussian kernel, just learn the noise\n",
    "    epsilon = torch.randn_like(theta_batch, dtype=theta_batch.dtype, device=theta_batch.device)\n",
    "\n",
    "    # perturb the theta batch\n",
    "    alpha, sigma = model.sde.kernel(t=diffusion_time)\n",
    "    z = alpha * theta_batch + sigma * epsilon\n",
    "    # predict from perturbed theta\n",
    "    pred = model(theta=z, time=diffusion_time, x=x_batch, pred_score=False)  # if prediction_type is 'score', this is still the score\n",
    "\n",
    "    if model.prediction_type == 'score':\n",
    "        target = model.sde.grad_log_kernel(x=z, x0=theta_batch, t=diffusion_time)\n",
    "    else:\n",
    "        target = epsilon\n",
    "\n",
    "    effective_weight = weighting_function(diffusion_time, sde=model.sde, weighting_type=model.weighting_type,\n",
    "                                          prediction_type=model.prediction_type)\n",
    "    # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "    loss = torch.mean(effective_weight * torch.sum(torch.square(pred - target), dim=-1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, dataloader_valid=None, epochs=100, lr=1e-4, device=None):\n",
    "    print(f\"Training {model.prediction_type}-model for {epochs} epochs with learning rate {lr} and {model.sde.kernel_type},\"\n",
    "          f\" {model.sde.noise_schedule} schedule and {model.weighting_type} weighting.\")\n",
    "    score_model.to(torch_device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Add Cosine Annealing Scheduler\n",
    "    #scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    loss_history = np.zeros((epochs, 2))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        # for each sample in the batch, calculate the loss for a random diffusion time\n",
    "        for theta_global_batch, theta_local_batch, x_batch in dataloader:\n",
    "            # initialize the gradients\n",
    "            optimizer.zero_grad()\n",
    "            theta_batch = torch.concat([theta_global_batch, theta_local_batch], dim=-1)\n",
    "            theta_batch = theta_batch.to(device)\n",
    "            x_batch = x_batch.to(device)\n",
    "            # calculate the loss\n",
    "            loss = compute_score_loss(theta_batch=theta_batch, x_batch=x_batch, model=model)\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "        #scheduler.step()\n",
    "\n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        if dataloader_valid is not None:\n",
    "            with torch.no_grad():\n",
    "                for theta_global_batch, theta_local_batch, x_batch in dataloader_valid:\n",
    "                    theta_batch = torch.concat([theta_global_batch, theta_local_batch], dim=-1)\n",
    "                    theta_batch = theta_batch.to(device)\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    loss = compute_score_loss(theta_batch=theta_batch, x_batch=x_batch, model=model)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "        loss_history[epoch] = [np.mean(total_loss), np.mean(valid_loss)]\n",
    "        print_str = f\"Epoch {epoch+1}/{epochs}, Loss: {np.mean(total_loss):.4f}, \"\\\n",
    "                    f\"Valid Loss: {np.mean(valid_loss):.4f}\"\n",
    "        print(print_str, end='\\r')\n",
    "        # Update the checkpoint after each epoch of training.\n",
    "        #torch.save(model.state_dict(), 'ckpt.pth')\n",
    "    return loss_history"
   ],
   "id": "3e4488af88ad4738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "n_data = 10000\n",
    "batch_size = 128\n",
    "\n",
    "# Create model and dataset\n",
    "thetas_global, thetas_local, xs = generate_synthetic_data(prior, n_data=n_data, normalize=True)\n",
    "\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(thetas_global, thetas_local, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create validation data\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_data=batch_size*2, normalize=True)\n",
    "dataset_valid = TensorDataset(valid_prior_global, valid_prior_local, valid_data)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][0]\n",
    ")\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x=1,\n",
    "    hidden_dim=64,\n",
    "    n_blocks=3,\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    time_embed_dim=16,\n",
    "    use_film=True,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior\n",
    ")"
   ],
   "id": "ded414838e34143f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 epochs=500, lr=1e-4, device=torch_device)\n",
    "score_model.eval();"
   ],
   "id": "ab5eef7cdb5ec27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(score_model.state_dict(), f\"score_model_{score_model.sde.noise_schedule}.pt\")",
   "id": "77ae74f8a7fa859c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "score_model.load_state_dict(torch.load(f\"score_model_{score_model.sde.noise_schedule}.pt\", weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "5a5959349d4a4731"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Mean Train')\n",
    "plt.plot(loss_history[:, 1], label='Mean Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'loss_plots/loss_training_{score_model.sde.noise_schedule}.png')\n",
    "plt.show()"
   ],
   "id": "aca578e1fb265da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_target = {}\n",
    "loss_list_score = {}\n",
    "loss_list_error_w = {}\n",
    "loss_list_error = {}\n",
    "\n",
    "pred_list = []\n",
    "score_list = []\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=100, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        for theta_global_batch, theta_local_batch, x_batch in dataloader_valid:\n",
    "            theta_batch = torch.cat([theta_global_batch, theta_local_batch], dim=-1)\n",
    "            theta_batch = theta_batch.to(torch_device)\n",
    "            x_batch = x_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon = torch.randn_like(theta_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_batch.shape[0], 1), t, dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            alpha, sigma = score_model.sde.kernel(t=t_tensor)\n",
    "            z = alpha * theta_batch + sigma * epsilon\n",
    "            snr = torch.log(torch.square(alpha)) - torch.log(torch.square(sigma))\n",
    "            # predict from perturbed theta\n",
    "            pred_epsilon = score_model(theta=z, time=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score = score_model(theta=z, time=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score = score_model.sde.grad_log_kernel(x=z, x0=theta_batch, t=t_tensor)\n",
    "            pred_list.append(torch.mean(pred_score))\n",
    "            score_list.append(torch.mean(true_score))\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target = score_model.sde.grad_log_kernel(x=z, x0=theta_batch, t=t_tensor)\n",
    "                pred_target = pred_epsilon  # is still the score\n",
    "                epsilon = target  # we do not need to calculate the error\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                target = epsilon\n",
    "                pred_target = pred_epsilon\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target = alpha*epsilon - sigma * theta_batch\n",
    "                pred_target = alpha*pred_epsilon - sigma * theta_batch\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target = theta_batch\n",
    "                pred_target = (z - pred_epsilon * sigma) / alpha\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss = torch.mean(torch.sum(torch.square(pred_target - target), dim=-1))\n",
    "            loss_list_target[t.item()] = loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss = torch.mean(torch.sum(torch.square(pred_score - true_score), dim=-1))\n",
    "            loss_list_score[t.item()] = loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            w = weighting_function(t_tensor, sde=score_model.sde,\n",
    "                                   weighting_type=score_model.weighting_type, prediction_type=score_model.prediction_type)\n",
    "            loss = torch.mean(w * torch.sum(torch.square(pred_epsilon - epsilon), dim=-1))\n",
    "            loss_list_error_w[t.item()] = loss.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss = torch.mean(torch.sum(torch.square(pred_epsilon - epsilon), dim=-1))\n",
    "            loss_list_error[t.item()] = loss.item()"
   ],
   "id": "3e3644f3daaa5484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_target = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_error_w = pd.DataFrame(loss_list_error_w.items(), columns=['Time', 'Loss'])\n",
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "# compute snr\n",
    "m, std = score_model.sde.kernel(diffusion_time)\n",
    "snr = torch.log(torch.square(m)) - torch.log(torch.square(std))\n",
    "upper_bound_loss = (np.sqrt(2) + 1) / (std.numpy()**2)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, sharex=True, figsize=(16, 3), tight_layout=True)\n",
    "ax[0].plot(df_target['Time'], np.log(df_target['Loss']), label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], np.log(df_score['Loss']), label='Score Loss')\n",
    "#ax[1].plot(df_score['Time'], df_score['Loss'] / upper_bound_loss, label='Score Loss')\n",
    "ax[1].plot(diffusion_time, snr, label='log snr', alpha=0.5)\n",
    "ax[2].plot(df_error_w['Time'], np.log(df_error_w['Loss']), label='Weighted Loss (as in Optimization)')\n",
    "ax[3].plot(df_error['Time'], np.log(df_error['Loss']), label='Loss on Error')\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Log Loss')\n",
    "    a.legend()\n",
    "plt.savefig(f'loss_plots/losses_diffusion_time_{score_model.sde.noise_schedule}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time.cpu(),\n",
    "         weighting_function(diffusion_time, sde=score_model.sde, weighting_type=score_model.weighting_type,\n",
    "                            prediction_type=score_model.prediction_type).cpu(),\n",
    "         label='weighting')\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "75adebcae950de41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 8\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_data=10, grid_size=n_grid, full_grid=True,\n",
    "                                                                            normalize=False)\n",
    "n_post_samples = 10"
   ],
   "id": "28d63d6aa09d9245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                            diffusion_steps=500, langevin_steps=5, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "fd3d0805fc460d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'loss_plots/recovery_global_{score_model.prediction_type}_langevin_sampler.png')"
   ],
   "id": "118c0de3d3ab0b1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   diffusion_steps=800, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "e906aed050d0714b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'loss_plots/recovery_global_{score_model.prediction_type}_euler_sampler.png')"
   ],
   "id": "874d1359427acaa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   pareto_smooth_sum_dict={'tail_fraction': 0.1, 'alpha': 0.6},\n",
    "                                                                   diffusion_steps=400, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "84418670f98f469c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'loss_plots/recovery_global_{score_model.prediction_type}_euler_pareto_sampler.png')"
   ],
   "id": "29cfcc0eb8d54a36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([adaptive_sampling(score_model, vd, n_post_samples,\n",
    "                                                             e_rel=0.1, max_steps=2000, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "ab55fadb21762acc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'loss_plots/recovery_global_{score_model.prediction_type}_adaptive_sampler.png')"
   ],
   "id": "509691ae9e76856a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.zeros((len(valid_data), n_post_samples, 2))\n",
    "for i, vd in enumerate(valid_data):\n",
    "    # solve for every sample individually, much slower, and still most of the samples were similar\n",
    "    #for j in range(n_post_samples):\n",
    "    #    posterior_global_samples_valid[i, j] = probability_ode_solving(score_model, vd, n_post_samples=1,\n",
    "    #                                                                    device=torch_device)\n",
    "    # solve for all samples at once\n",
    "    posterior_global_samples_valid[i] = probability_ode_solving(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                device=torch_device)"
   ],
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'loss_plots/recovery_global_{score_model.prediction_type}_ode.png')"
   ],
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$']);"
   ],
   "id": "e41154e220b31ef5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#conditions_global = np.median(posterior_global_samples_valid, axis=0)\n",
    "posterior_local_samples_valid = np.array([euler_maruyama_sampling(score_model, vd,\n",
    "                                                                  n_post_samples=n_post_samples, conditions=c,\n",
    "                                                                  diffusion_steps=300, device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, posterior_global_samples_valid)])"
   ],
   "id": "9de5da14663e13c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          param_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "5c954e8ad82c5e89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#conditions_global = np.median(posterior_global_samples_valid, axis=1)\n",
    "posterior_local_samples_valid = np.array([probability_ode_solving(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                  conditions=c, device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, posterior_global_samples_valid)])"
   ],
   "id": "3a45ca1277eefae1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          param_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "44320e5b15ea6349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Number of parameters\n",
    "n_params = 64\n",
    "# Empirical estimates for each parameter\n",
    "empirical = np.array(valid_prior_local).reshape(valid_data.shape[0], -1)[0]\n",
    "# Global means for each parameter\n",
    "global_mean = np.ones_like(empirical) * conditions_global[0, 0]\n",
    "# Global standard deviations (optional, for reference)\n",
    "global_std = np.ones_like(empirical) * conditions_global[0, 1]\n",
    "\n",
    "# Compute posterior means and standard deviations\n",
    "posterior_samples = posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1)\n",
    "posterior_mean = np.mean(posterior_samples, axis=0)\n",
    "posterior_std = np.std(posterior_samples, axis=0)\n",
    "\n",
    "print(n_params, empirical.shape, global_mean.shape, global_std.shape, posterior_mean.shape, posterior_std.shape)\n",
    "\n",
    "# Compute shrinkage ratios for each parameter.\n",
    "# Avoid division by zero if any empirical equals the global mean.\n",
    "epsilon = 1e-8\n",
    "shrinkage = ( (empirical - global_mean) - (posterior_mean - global_mean) ) / (empirical - global_mean + epsilon)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "indices = np.arange(n_params)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Empirical vs Posterior vs Global\n",
    "plt.subplot(1, 2, 1)\n",
    "print(indices, empirical)\n",
    "plt.plot(indices, empirical, 'o', label='Empirical')\n",
    "plt.errorbar(indices, posterior_mean, yerr=posterior_std, fmt='s', label='Posterior Mean')\n",
    "plt.plot(indices, global_mean, 'x', label='Global Mean', markersize=10, color='red')\n",
    "# Draw vertical dashed lines to show the movement from empirical to posterior\n",
    "for i in indices:\n",
    "    plt.plot([i, i], [empirical[i], posterior_mean[i]], 'k--', lw=0.8)\n",
    "plt.xlabel('Parameter Index')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.title('Empirical, Posterior, and Global Estimates')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Shrinkage Ratios\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(indices, shrinkage, color='skyblue')\n",
    "plt.xlabel('Parameter Index')\n",
    "plt.ylabel('Shrinkage Ratio')\n",
    "plt.title('Shrinkage Ratio per Parameter\\n(0: No Shrinkage, 1: Full Shrinkage)')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d30ad8be027b8be7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 0\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 0]), np.std(posterior_global_samples_valid[valid_id, :, 0]))\n",
    "print('log tau:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][0].item())\n",
    "print('log tau:', valid_prior_global[valid_id][1].item())"
   ],
   "id": "c4bb5620888ab007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "med = np.median(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "std = np.std(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "error = (med-valid_prior_local[valid_id].numpy())**2\n",
    "visualize_simulation_output(np.stack((med, valid_prior_local[valid_id], )),\n",
    "                            title_prefix=['Posterior Median', 'True'])\n",
    "\n",
    "visualize_simulation_output(np.stack((std, error)), title_prefix=['Uncertainty', 'Error'], same_scale=False)"
   ],
   "id": "c9daf84cada1b0a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize the Score",
   "id": "bc9c59ae04231887"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = np.random.randint(0, len(valid_data))\n",
    "\n",
    "diffusion_time = generate_diffusion_time(size=10, device=torch_device)\n",
    "x_valid = valid_data[valid_id].to(torch_device)\n",
    "theta_global = prior.normalize_theta(valid_prior_global[valid_id], global_params=True).cpu().numpy()  # we normalize as the score is normalized space\n",
    "print(valid_id, 'theta global', theta_global)"
   ],
   "id": "36b9381866daf22e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sample = adaptive_sampling(score_model, x_valid, conditions=None, n_post_samples=1,\n",
    "                                e_rel=0.1, max_steps=2500, t_end=diffusion_time[0], random_seed=0, device=torch_device)\n",
    "test_sample = prior.normalize_theta(torch.tensor(test_sample), global_params=True).cpu().numpy()\n",
    "print(test_sample)"
   ],
   "id": "e9dff25c3741f7d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_sample_path = np.array([adaptive_sampling(score_model, x_valid, conditions=None, n_post_samples=1,\n",
    "                                                                  e_rel=0.1, max_steps=2500, t_end=t, random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path = prior.normalize_theta(torch.tensor(posterior_sample_path), global_params=True).cpu().numpy()\n",
    "\n",
    "# posterior_sample_path2 = np.array([euler_maruyama_sampling(score_model, x_valid, diffusion_steps=40000,\n",
    "#                                                            conditions=None, n_post_samples=1, t_end=t, random_seed=0, device=torch_device)\n",
    "#                                   for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "#posterior_sample_path2 = prior.normalize_theta(torch.tensor(posterior_sample_path2), global_params=True).cpu().numpy()\n",
    "\n",
    "posterior_sample_path3 = np.array([probability_ode_solving(score_model, x_valid,\n",
    "                                                           conditions=None, n_post_samples=1, t_end=t, random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path3 = prior.normalize_theta(torch.tensor(posterior_sample_path3), global_params=True).cpu().numpy()\n",
    "\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "e9053cd01f24ecf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define grid boundaries and resolution for your 2D space.\n",
    "x_min, x_max, y_min, y_max = -1.5, 1.5, -1.5, 1.5\n",
    "grid_res = 10  # Number of points per dimension\n",
    "\n",
    "# Create a meshgrid of points\n",
    "x_vals = np.linspace(x_min, x_max, grid_res)\n",
    "y_vals = np.linspace(y_min, y_max, grid_res)\n",
    "xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "# Stack into (N,2) where N = grid_res*grid_res\n",
    "grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Convert grid to a torch tensor and move to device\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32, device=torch_device)\n",
    "x_valid_e = x_valid.reshape(10, -1)\n",
    "x_valid_ext = x_valid_e.unsqueeze(0).repeat(grid_tensor.shape[0], 1, 1).to(torch_device)\n",
    "\n",
    "# Dictionary to hold score outputs for each time\n",
    "scores = {}\n",
    "\n",
    "# Evaluate the score model for each time value\n",
    "for t in diffusion_time:\n",
    "    # Create a tensor of time values for each grid point\n",
    "    t_tensor = torch.full((grid_tensor.shape[0], 1), t.item(), dtype=torch.float32, device=torch_device)\n",
    "    epsilon = torch.randn_like(grid_tensor, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "    # perturb theta\n",
    "    alpha, sigma = score_model.sde.kernel(t=t_tensor)\n",
    "    z = grid_tensor #alpha * grid_tensor + sigma * epsilon\n",
    "\n",
    "    # Evaluate the score model\n",
    "    with torch.no_grad():\n",
    "        score = (1 - n_grid*n_grid) * (1 - t) / 1 * prior.score_global_batch(z)\n",
    "        for i in range(x_valid_ext.shape[2]):\n",
    "            score += score_model.forward_global(theta_global=z, time=t_tensor, x=x_valid_ext[:, :, i].unsqueeze(-1),\n",
    "                                                pred_score=True, clip_x=False)\n",
    "    scores[t.item()] = score.cpu().numpy()"
   ],
   "id": "474671e4362d31cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(scores.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)  # negative since we are plotting the reverse score\n",
    "\n",
    "    ax = axes[i]\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    j = len(diffusion_time)-1-i\n",
    "    if i != 0:\n",
    "        #h2, = ax.plot(posterior_sample_path[j, 0, 0], posterior_sample_path[j, 0, 1], 'o', label='Posterior Path Sampling')\n",
    "        h2, = ax.plot(posterior_sample_path[j:, 0, 0], posterior_sample_path[j:, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "        #h3, = ax.plot(posterior_sample_path2[j, 0, 0], posterior_sample_path2[j, 0, 1], 'o', label='Posterior Path Euler')\n",
    "        ##h3, = ax.plot(posterior_sample_path2[j:, 0, 0], posterior_sample_path2[j:, 0, 1], 'o-', label='Posterior Path Euler', alpha=0.5)\n",
    "        #h4, = ax.plot(posterior_sample_path3[j, 0, 0], posterior_sample_path3[j, 0, 1], 'o', label='Posterior Path ODE')\n",
    "        h4, = ax.plot(posterior_sample_path3[j:, 0, 0], posterior_sample_path3[j:, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    #ax.plot(posterior_sample_path3[:, 0, 0], posterior_sample_path3[:, 0, 1], 'o-', label='Posterior Path ODE')\n",
    "    ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=10*n_grid*n_grid, alpha=.75)\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h0, h1, h2, h4], loc='lower center', ncols=5, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'loss_plots/score_field_{score_model.sde.noise_schedule}.png')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "8458bd11effe5a4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "62494cc6c00a1582",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
