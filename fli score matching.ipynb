{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FLI with compositional score matching\n",
   "id": "80db02ba5075c403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:54:19.985017Z",
     "start_time": "2025-03-27T17:54:14.858306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.special import expit\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from bayesflow import diagnostics\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, weighting_function, euler_maruyama_sampling, adaptive_sampling, \\\n",
    "    generate_diffusion_time, count_parameters, train_score_model\n",
    "from diffusion_model.helper_networks import LSTM\n",
    "from problems.fli import FLIProblem, FLI_Prior, generate_synthetic_data\n",
    "from problems import plot_shrinkage, visualize_simulation_output"
   ],
   "id": "c1947461daedf5a7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:54:19.989861Z",
     "start_time": "2025-03-27T17:54:19.988195Z"
    }
   },
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cuda\")",
   "id": "e2cd8befc359fbb7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:54:20.234474Z",
     "start_time": "2025-03-27T17:54:20.031667Z"
    }
   },
   "cell_type": "code",
   "source": "prior = FLI_Prior()",
   "id": "a68f30166c7b6778",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:54:25.758147Z",
     "start_time": "2025-03-27T17:54:20.238917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "number_of_obs = [1, 2, 4, 5, 10]\n",
    "\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "dataset = FLIProblem(\n",
    "    n_data=10000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    online_learning=True,\n",
    "    number_of_obs=number_of_obs,\n",
    ")\n",
    "\n",
    "dataset_valid = FLIProblem(\n",
    "    n_data=1000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    number_of_obs=number_of_obs\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)"
   ],
   "id": "281bf48b6335aa28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel type: variance_preserving, noise schedule: cosine\n",
      "t_min: 0.00035210439818911254, t_max: 0.999647855758667\n",
      "alpha, sigma: (tensor(1.0000), tensor(0.0006)) (tensor(0.0006), tensor(1.0000))\n",
      "Moving prior to device: cpu\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T17:54:39.566497Z",
     "start_time": "2025-03-27T17:54:39.547826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "summary_net = LSTM(input_size=1, hidden_dim=256)\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x=256,\n",
    "    summary_net=summary_net,\n",
    "    hidden_dim=512,\n",
    "    n_blocks=5,\n",
    "    max_number_of_obs=number_of_obs if isinstance(number_of_obs, int) else max(number_of_obs),\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior,\n",
    "    name_prefix='FLI_'\n",
    ")\n",
    "print(score_model.name)\n",
    "count_parameters(score_model)\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "8264d867ba480846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------+\n",
      "|               Modules                | Parameters |\n",
      "+--------------------------------------+------------+\n",
      "|   blocks.res_blocks.0.dense.weight   |   266240   |\n",
      "|    blocks.res_blocks.0.dense.bias    |    512     |\n",
      "| blocks.res_blocks.0.projector.weight |   266240   |\n",
      "|   blocks.res_blocks.1.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.1.dense.bias    |    512     |\n",
      "|   blocks.res_blocks.2.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.2.dense.bias    |    512     |\n",
      "|   blocks.res_blocks.3.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.3.dense.bias    |    512     |\n",
      "|   blocks.res_blocks.4.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.4.dense.bias    |    512     |\n",
      "|    final_projection_linear.weight    |    3072    |\n",
      "|     final_projection_linear.bias     |     6      |\n",
      "+--------------------------------------+------------+\n",
      "Total Trainable Params: 1586694\n",
      "FLI_global_score_model_v_variance_preserving_cosine_likelihood_weighting\n",
      "+--------------------------------------+------------+\n",
      "|               Modules                | Parameters |\n",
      "+--------------------------------------+------------+\n",
      "|   blocks.res_blocks.0.dense.weight   |   136192   |\n",
      "|    blocks.res_blocks.0.dense.bias    |    512     |\n",
      "| blocks.res_blocks.0.projector.weight |   136192   |\n",
      "|   blocks.res_blocks.1.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.1.dense.bias    |    512     |\n",
      "|   blocks.res_blocks.2.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.2.dense.bias    |    512     |\n",
      "|   blocks.res_blocks.3.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.3.dense.bias    |    512     |\n",
      "|   blocks.res_blocks.4.dense.weight   |   262144   |\n",
      "|    blocks.res_blocks.4.dense.bias    |    512     |\n",
      "|    final_projection_linear.weight    |    1536    |\n",
      "|     final_projection_linear.bias     |     3      |\n",
      "+--------------------------------------+------------+\n",
      "Total Trainable Params: 1325059\n",
      "FLI_local_score_model_v_variance_preserving_cosine_likelihood_weighting\n",
      "+---------------------------------------------------+------------+\n",
      "|                      Modules                      | Parameters |\n",
      "+---------------------------------------------------+------------+\n",
      "|           summary_net.lstm.weight_ih_l0           |    1024    |\n",
      "|           summary_net.lstm.weight_hh_l0           |   262144   |\n",
      "|            summary_net.lstm.bias_ih_l0            |    1024    |\n",
      "|            summary_net.lstm.bias_hh_l0            |    1024    |\n",
      "|   global_model.blocks.res_blocks.0.dense.weight   |   266240   |\n",
      "|    global_model.blocks.res_blocks.0.dense.bias    |    512     |\n",
      "| global_model.blocks.res_blocks.0.projector.weight |   266240   |\n",
      "|   global_model.blocks.res_blocks.1.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.1.dense.bias    |    512     |\n",
      "|   global_model.blocks.res_blocks.2.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.2.dense.bias    |    512     |\n",
      "|   global_model.blocks.res_blocks.3.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.3.dense.bias    |    512     |\n",
      "|   global_model.blocks.res_blocks.4.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.4.dense.bias    |    512     |\n",
      "|    global_model.final_projection_linear.weight    |    3072    |\n",
      "|     global_model.final_projection_linear.bias     |     6      |\n",
      "|    local_model.blocks.res_blocks.0.dense.weight   |   136192   |\n",
      "|     local_model.blocks.res_blocks.0.dense.bias    |    512     |\n",
      "|  local_model.blocks.res_blocks.0.projector.weight |   136192   |\n",
      "|    local_model.blocks.res_blocks.1.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.1.dense.bias    |    512     |\n",
      "|    local_model.blocks.res_blocks.2.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.2.dense.bias    |    512     |\n",
      "|    local_model.blocks.res_blocks.3.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.3.dense.bias    |    512     |\n",
      "|    local_model.blocks.res_blocks.4.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.4.dense.bias    |    512     |\n",
      "|     local_model.final_projection_linear.weight    |    1536    |\n",
      "|      local_model.final_projection_linear.bias     |     3      |\n",
      "+---------------------------------------------------+------------+\n",
      "Total Trainable Params: 3176969\n",
      "FLI_hierarchical_score_model_v_variance_preserving_cosine_likelihood_weighting_factorized10\n",
      "FLI_hierarchical_score_model_v_variance_preserving_cosine_likelihood_weighting_factorized10\n",
      "+---------------------------------------------------+------------+\n",
      "|                      Modules                      | Parameters |\n",
      "+---------------------------------------------------+------------+\n",
      "|           summary_net.lstm.weight_ih_l0           |    1024    |\n",
      "|           summary_net.lstm.weight_hh_l0           |   262144   |\n",
      "|            summary_net.lstm.bias_ih_l0            |    1024    |\n",
      "|            summary_net.lstm.bias_hh_l0            |    1024    |\n",
      "|   global_model.blocks.res_blocks.0.dense.weight   |   266240   |\n",
      "|    global_model.blocks.res_blocks.0.dense.bias    |    512     |\n",
      "| global_model.blocks.res_blocks.0.projector.weight |   266240   |\n",
      "|   global_model.blocks.res_blocks.1.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.1.dense.bias    |    512     |\n",
      "|   global_model.blocks.res_blocks.2.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.2.dense.bias    |    512     |\n",
      "|   global_model.blocks.res_blocks.3.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.3.dense.bias    |    512     |\n",
      "|   global_model.blocks.res_blocks.4.dense.weight   |   262144   |\n",
      "|    global_model.blocks.res_blocks.4.dense.bias    |    512     |\n",
      "|    global_model.final_projection_linear.weight    |    3072    |\n",
      "|     global_model.final_projection_linear.bias     |     6      |\n",
      "|    local_model.blocks.res_blocks.0.dense.weight   |   136192   |\n",
      "|     local_model.blocks.res_blocks.0.dense.bias    |    512     |\n",
      "|  local_model.blocks.res_blocks.0.projector.weight |   136192   |\n",
      "|    local_model.blocks.res_blocks.1.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.1.dense.bias    |    512     |\n",
      "|    local_model.blocks.res_blocks.2.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.2.dense.bias    |    512     |\n",
      "|    local_model.blocks.res_blocks.3.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.3.dense.bias    |    512     |\n",
      "|    local_model.blocks.res_blocks.4.dense.weight   |   262144   |\n",
      "|     local_model.blocks.res_blocks.4.dense.bias    |    512     |\n",
      "|     local_model.final_projection_linear.weight    |    1536    |\n",
      "|      local_model.final_projection_linear.bias     |     3      |\n",
      "+---------------------------------------------------+------------+\n",
      "Total Trainable Params: 3176969\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-27T17:54:40.685163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid, hierarchical=True,\n",
    "                                              epochs=2000, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Training', color=\"#132a70\", lw=2.0, alpha=0.9)\n",
    "plt.plot(loss_history[:, 1], label='Validation', linestyle=\"--\", marker=\"o\", color='black')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('Training epoch #')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')"
   ],
   "id": "7ee0d728a42e40fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training v-model for 2000 epochs with learning rate 0.0005 and likelihood_weighting weighting.\n",
      "Epoch 24/2000, Loss: 14.2251, Valid Loss: 14.9868\r"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "ef923235fe4b33b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_target = {}\n",
    "loss_list_score = {}\n",
    "loss_list_error_w_global = {}\n",
    "loss_list_error_w_local = {}\n",
    "loss_list_error = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=100, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        loss_list_target[t.item()] = 0\n",
    "        loss_list_score[t.item()] = 0\n",
    "        loss_list_error_w_global[t.item()] = 0\n",
    "        loss_list_error_w_local[t.item()] = 0\n",
    "        loss_list_error[t.item()] = 0\n",
    "\n",
    "        for theta_global_batch, _, theta_local_batch, _, x_batch in dataloader_valid:\n",
    "            theta_global_batch = theta_global_batch.to(torch_device)\n",
    "            theta_local_batch = theta_local_batch.to(torch_device)\n",
    "            x_batch = x_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon_global = torch.randn_like(theta_global_batch, dtype=torch.float32, device=torch_device)\n",
    "            epsilon_local = torch.randn_like(theta_local_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_global_batch.shape[0], 1), t,\n",
    "                                  dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            snr = score_model.sde.get_snr(t=t_tensor)\n",
    "            alpha, sigma = score_model.sde.kernel(log_snr=snr)\n",
    "            z_global = alpha * theta_global_batch + sigma * epsilon_global\n",
    "            if score_model.max_number_of_obs > 1:\n",
    "                # global params are not factorized to the same level as local params\n",
    "                alpha_local = alpha.unsqueeze(1)\n",
    "                sigma_local = sigma.unsqueeze(1)\n",
    "            else:\n",
    "                alpha_local = alpha\n",
    "                sigma_local = sigma\n",
    "            z_local = alpha_local * theta_local_batch + sigma_local * epsilon_local\n",
    "\n",
    "            # predict from perturbed theta\n",
    "            pred_epsilon_global, pred_epsilon_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                       time=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score_global, pred_score_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                     time=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score_global = score_model.sde.grad_log_kernel(x=z_global,\n",
    "                                                                x0=theta_global_batch, t=t_tensor)\n",
    "            if score_model.max_number_of_obs == 1:\n",
    "                true_score_local = score_model.sde.grad_log_kernel(x=z_local,\n",
    "                                                                   x0=theta_local_batch,\n",
    "                                                                   t=t_tensor)\n",
    "            else:\n",
    "                true_score_local = []\n",
    "                for i in range(score_model.max_number_of_obs):\n",
    "                    score_local = score_model.sde.grad_log_kernel(x=z_local[:, i],\n",
    "                                                                   x0=theta_local_batch[:, i],\n",
    "                                                                   t=t_tensor)\n",
    "                    true_score_local.append(score_local.unsqueeze(1))\n",
    "                true_score_local = torch.concatenate(true_score_local, dim=1)\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target_global = -epsilon_global / sigma\n",
    "                pred_target_global = -pred_epsilon_global / sigma\n",
    "                target_local = -epsilon_local / sigma_local\n",
    "                pred_target_local = -pred_epsilon_local / sigma_local\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                target_global = epsilon_global\n",
    "                pred_target_global = pred_epsilon_global\n",
    "                target_local = epsilon_local\n",
    "                pred_target_local = pred_epsilon_local\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target_global = alpha*epsilon_global - sigma * theta_global_batch\n",
    "                pred_target_global = alpha*pred_epsilon_global - sigma * theta_global_batch\n",
    "                target_local = alpha_local*epsilon_local - sigma_local * theta_local_batch\n",
    "                pred_target_local = alpha_local*pred_epsilon_local - sigma_local * theta_local_batch\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target_global = theta_global_batch\n",
    "                pred_target_global = (z_global - pred_epsilon_global * sigma) / alpha\n",
    "                target_local = theta_local_batch\n",
    "                pred_target_local = (z_local - pred_epsilon_local * sigma_local) / alpha_local\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_target_global - target_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_target_local - target_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_target[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_score_global - true_score_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_score_local - true_score_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_score[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            w = weighting_function(t_tensor, sde=score_model.sde,\n",
    "                                   weighting_type=score_model.weighting_type, prediction_type=score_model.prediction_type)\n",
    "            loss_global = torch.mean(w * torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(w * torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            #loss = loss_global + loss_local\n",
    "            loss_list_error_w_global[t.item()] += loss_global.item()\n",
    "            loss_list_error_w_local[t.item()] += loss_local.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_error[t.item()] += loss.item()"
   ],
   "id": "588ce3619ad80176",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_target = pd.DataFrame(loss_list_target.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_local = pd.DataFrame(loss_list_error_w_local.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_global = pd.DataFrame(loss_list_error_w_global.items(), columns=['Time', 'Loss'])\n",
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "# compute snr\n",
    "snr = score_model.sde.get_snr(diffusion_time)\n",
    "#upper_bound_loss = (np.sqrt(2) + 1) / (std.numpy()**2)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, sharex=True, figsize=(16, 3), tight_layout=True)\n",
    "ax[0].plot(df_target['Time'], np.log(df_target['Loss']), label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], np.log(df_score['Loss']), label='Score Loss')\n",
    "#ax[1].plot(df_score['Time'], df_score['Loss'] / upper_bound_loss, label='Score Loss')\n",
    "ax[1].plot(diffusion_time, snr, label='log snr', alpha=0.5)\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']), label='Local Weighted Loss')\n",
    "ax[2].plot(df_error_w_global['Time'], np.log(df_error_w_global['Loss']), label='Global Weighted Loss')\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']+df_error_w_global['Loss']), label='Weighted Loss (as in Optimization)')\n",
    "ax[3].plot(df_error['Time'], np.log(df_error['Loss']), label='Loss on Error')\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Log Loss')\n",
    "    a.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/losses_diffusion_time.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time.cpu(),\n",
    "         weighting_function(diffusion_time, sde=score_model.sde, weighting_type=score_model.weighting_type,\n",
    "                            prediction_type=score_model.prediction_type).cpu(),\n",
    "         label='weighting')\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "76a52d3b02dc76f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "b4df928ac8ce9cfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 4\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior=prior, n_data=100, n_local_samples=n_grid*n_grid,\n",
    "                                                                            as_grid=True,\n",
    "                                                                            random_seed=0)\n",
    "n_post_samples = 20\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(n_grid*n_grid)\n",
    "#score_model.current_number_of_obs = 4  # we can choose here, how many observations are passed together through the score"
   ],
   "id": "2025f449937da875",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 10\n",
    "t1_value = 0.1 #/( (n_grid*n_grid) //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    'size': mini_batch_size,\n",
    "    #'damping_factor': lambda t: t1_value + (t0_value - t1_value) * 0.5 * (1 + torch.cos(torch.pi * t)),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * score_model.sde.kernel(log_snr=score_model.sde.get_snr(t))[1],\n",
    "    #'damping_factor': lambda t: 0.1, #t1_value,\n",
    "    'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    'damping_factor_prior': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * torch.sigmoid(20*(t-0.3))\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "b6beb94daf12115e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.sde.s_shift_cosine = 4\n",
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data, n_post_samples,\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "a9ecff4368f5da80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler.png')"
   ],
   "id": "5b3e0546bd196017",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conditions_global = (np.median(posterior_global_samples_valid, axis=0), posterior_global_samples_valid)[1]\n",
    "score_model.sde.s_shift_cosine = 0\n",
    "posterior_local_samples_valid = euler_maruyama_sampling(score_model, valid_data,\n",
    "                                                        n_post_samples=n_post_samples, conditions=conditions_global,\n",
    "                                                        diffusion_steps=50, device=torch_device, verbose=True)"
   ],
   "id": "a0afa10c0eae06db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          variable_names=local_param_names);"
   ],
   "id": "396d980ecc94ee94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(posterior_global_samples_valid[:12], posterior_local_samples_valid[:12], min_max=(-10, 10))",
   "id": "fb4ff93ba40e5f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 0\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 0]), np.std(posterior_global_samples_valid[valid_id, :, 0]))\n",
    "print('log sigma:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][0].item())\n",
    "print('log sigma:', valid_prior_global[valid_id][1].item())"
   ],
   "id": "14919d3f7a2c375e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ps = posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid, 3).copy()\n",
    "true = valid_prior_local[valid_id].numpy().copy()\n",
    "ps[:, :, :, 0] = np.exp(ps[:, :, :, 0])\n",
    "true[:, :, 0] = np.exp(true[:, :, 0])\n",
    "ps[:, :, :, 1] = ps[:, :, :, 0] + np.exp(ps[:, :, :, 1])\n",
    "true[:, :, 1] = true[:, :, 0] + np.exp(true[:, :, 1])\n",
    "ps[:, :, :, 2] = expit(ps[:, :, :, 2])\n",
    "true[:, :, 2] = expit(true[:, :, 2])\n",
    "transf_local_param_names = [r'$\\tau_1^L$', r'$\\tau_2^L$', r'$A^L$']\n",
    "\n",
    "med = np.median(ps, axis=0)\n",
    "std = np.std(ps, axis=0)\n",
    "error = (med-true)**2\n",
    "visualize_simulation_output(med, title_prefix=['Posterior Median ' + p for p in transf_local_param_names], cmap='turbo')\n",
    "visualize_simulation_output(true, title_prefix=['True ' + p for p in transf_local_param_names], cmap='turbo')\n",
    "\n",
    "visualize_simulation_output(std, title_prefix=['Posterior Std ' + p for p in transf_local_param_names])\n",
    "visualize_simulation_output(error, title_prefix=['Error ' + p for p in transf_local_param_names])"
   ],
   "id": "22186b2ba02c2afe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4), tight_layout=True)\n",
    "for i in range(3):\n",
    "    ax[i].errorbar(x=true[:, :, i].flatten(), y=med[:, :, i].flatten(), yerr=1.96*std[:, :, i].flatten(), fmt='o')\n",
    "    #ax[i].plot([np.min(true[:, :, i]), np.max(true[:, :, i])], [np.min(true[:, :, i]), np.max(true[:, :, i])], 'k--')\n",
    "    ax[i].axhline(np.median(posterior_global_samples_valid[valid_id, :, i], axis=0), color='red', linestyle='--',\n",
    "                label='Global posterior mean', alpha=0.75)\n",
    "    ax[i].set_ylabel('Prediction')\n",
    "    ax[i].set_xlabel('True')\n",
    "    ax[i].legend()\n",
    "plt.show()"
   ],
   "id": "ba0c1b429bf02d3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "36d38b81a0f739b7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical-abi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
