{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gaussian on a Grid Test with compositional score matching\n",
    "\n",
    "In this notebook, we will test the compositional score matching on a hierarchical problem defined on a grid.\n",
    "- The observations are on grid with `n_grid` x `n_grid` points.\n",
    "- The global parameters are the same for all grid points with hyper-priors:\n",
    "$$ \\mu \\sim \\mathcal{N}(0, 3^2),\\qquad \\log\\sigma \\sim \\mathcal{N}(0, 1^2)$$\n",
    "\n",
    "- The local parameters are different for each grid point\n",
    "$$ \\theta_{i,j} \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "\n",
    "-  In each grid point, we have a Brownian motion with drift:\n",
    "$$ dx_t = \\theta \\cdot dt + \\sqrt{dt} \\cdot dW_t$$\n",
    "- We observe $T=10$ time points for each grid point over a time horizon of `max_time=1`. We can also amortize over the time dimension."
   ],
   "id": "dd43055268ee0b4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from bayesflow import diagnostics\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, euler_maruyama_sampling, adaptive_sampling, \\\n",
    "    probability_ode_solving, langevin_sampling, \\\n",
    "    generate_diffusion_time, count_parameters, train_score_model\n",
    "from diffusion_model.helper_networks import LSTM, GaussianFourierProjection\n",
    "from problems.gaussian_grid import GaussianGridProblem, Prior, Simulator, visualize_simulation_output, plot_shrinkage, \\\n",
    "    generate_synthetic_data"
   ],
   "id": "91bfaa75f9d526d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cuda\")",
   "id": "1598c4784577dcf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior(n_time_points=10)\n",
    "simulator_test = Simulator(n_time_points=10)\n",
    "\n",
    "# test the simulator\n",
    "sim_test = simulator_test(prior.sample_full(1, n_grid=8))['observable']\n",
    "visualize_simulation_output(sim_test)"
   ],
   "id": "ad2857cf4caf239",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "number_of_obs = 1 #[64]\n",
    "\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "dataset = GaussianGridProblem(\n",
    "    n_data=100000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    online_learning=False,\n",
    "    number_of_obs=number_of_obs,\n",
    ")\n",
    "\n",
    "dataset_valid = GaussianGridProblem(\n",
    "    n_data=1000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    number_of_obs=number_of_obs,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for test in dataloader:\n",
    "    print(test[0].shape, test[1].shape, test[2].shape, test[3].shape, test[4].shape)\n",
    "    break"
   ],
   "id": "24efd9f43bfcc3eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "summary_dim = 8\n",
    "#summary_net = LSTM(input_size=1, hidden_dim=summary_dim)\n",
    "\n",
    "time_embedding_local = nn.Sequential(\n",
    "    GaussianFourierProjection(8),\n",
    "    nn.Linear(8, 8),\n",
    "    nn.Mish()\n",
    ")\n",
    "time_embedding_global = nn.Sequential(\n",
    "    GaussianFourierProjection(8),\n",
    "    nn.Linear(8, 8),\n",
    "    nn.Mish()\n",
    ")\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x=10,\n",
    "    #summary_net=summary_net,\n",
    "    time_embedding_local=time_embedding_local,\n",
    "    time_embedding_global=time_embedding_global,\n",
    "    hidden_dim=256,\n",
    "    n_blocks=5,\n",
    "    max_number_of_obs=number_of_obs if isinstance(number_of_obs, int) else max(number_of_obs),\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior,\n",
    "    name_prefix='grid_'\n",
    ")\n",
    "\n",
    "print(score_model.name)\n",
    "count_parameters(score_model)\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "36fb48f6042da3b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid, hierarchical=True,\n",
    "                                              epochs=500, device=torch_device, lr=1e-4)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Training', color=\"#132a70\", lw=2.0, alpha=0.9)\n",
    "plt.plot(loss_history[:, 1], label='Validation', linestyle=\"--\", marker=\"o\", color='black')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('Training epoch #')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')"
   ],
   "id": "d87cbe350f53e4be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "8351f8477b583769",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_target = {}\n",
    "loss_list_score = {}\n",
    "loss_list_error_w_global = {}\n",
    "loss_list_error_w_local = {}\n",
    "loss_list_error = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=100, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        loss_list_target[t.item()] = 0\n",
    "        loss_list_score[t.item()] = 0\n",
    "        loss_list_error_w_global[t.item()] = 0\n",
    "        loss_list_error_w_local[t.item()] = 0\n",
    "        loss_list_error[t.item()] = 0\n",
    "\n",
    "        for theta_global_batch, _, theta_local_batch, _, x_batch in dataloader_valid:\n",
    "            theta_global_batch = theta_global_batch.to(torch_device)\n",
    "            theta_local_batch = theta_local_batch.to(torch_device)\n",
    "            x_batch = x_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon_global = torch.randn_like(theta_global_batch, dtype=torch.float32, device=torch_device)\n",
    "            epsilon_local = torch.randn_like(theta_local_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_global_batch.shape[0], 1), t,\n",
    "                                  dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            snr = score_model.sde.get_snr(t=t_tensor)\n",
    "            alpha, sigma = score_model.sde.kernel(log_snr=snr)\n",
    "            z_global = alpha * theta_global_batch + sigma * epsilon_global\n",
    "            if score_model.max_number_of_obs > 1:\n",
    "                # global params are not factorized to the same level as local params\n",
    "                alpha_local = alpha.unsqueeze(1)\n",
    "                sigma_local = sigma.unsqueeze(1)\n",
    "            else:\n",
    "                alpha_local = alpha\n",
    "                sigma_local = sigma\n",
    "            z_local = alpha_local * theta_local_batch + sigma_local * epsilon_local\n",
    "\n",
    "            # predict from perturbed theta\n",
    "            pred_epsilon_global, pred_epsilon_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                       time=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score_global, pred_score_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                     time=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score_global = score_model.sde.grad_log_kernel(x=z_global,\n",
    "                                                                x0=theta_global_batch, t=t_tensor)\n",
    "            if score_model.max_number_of_obs == 1:\n",
    "                true_score_local = score_model.sde.grad_log_kernel(x=z_local,\n",
    "                                                                   x0=theta_local_batch,\n",
    "                                                                   t=t_tensor)\n",
    "            else:\n",
    "                true_score_local = []\n",
    "                for i in range(score_model.max_number_of_obs):\n",
    "                    score_local = score_model.sde.grad_log_kernel(x=z_local[:, i],\n",
    "                                                                   x0=theta_local_batch[:, i],\n",
    "                                                                   t=t_tensor)\n",
    "                    true_score_local.append(score_local.unsqueeze(1))\n",
    "                true_score_local = torch.concatenate(true_score_local, dim=1)\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target_global = -epsilon_global / sigma\n",
    "                pred_target_global = -pred_epsilon_global / sigma\n",
    "                target_local = -epsilon_local / sigma_local\n",
    "                pred_target_local = -pred_epsilon_local / sigma_local\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                target_global = epsilon_global\n",
    "                pred_target_global = pred_epsilon_global\n",
    "                target_local = epsilon_local\n",
    "                pred_target_local = pred_epsilon_local\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target_global = alpha*epsilon_global - sigma * theta_global_batch\n",
    "                pred_target_global = alpha*pred_epsilon_global - sigma * theta_global_batch\n",
    "                target_local = alpha_local*epsilon_local - sigma_local * theta_local_batch\n",
    "                pred_target_local = alpha_local*pred_epsilon_local - sigma_local * theta_local_batch\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target_global = theta_global_batch\n",
    "                pred_target_global = (z_global - pred_epsilon_global * sigma) / alpha\n",
    "                target_local = theta_local_batch\n",
    "                pred_target_local = (z_local - pred_epsilon_local * sigma_local) / alpha_local\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_target_global - target_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_target_local - target_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_target[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_score_global - true_score_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_score_local - true_score_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_score[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            w = weighting_function(t_tensor, sde=score_model.sde,\n",
    "                                   weighting_type=score_model.weighting_type, prediction_type=score_model.prediction_type)\n",
    "            loss_global = torch.mean(w * torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(w * torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            #loss = loss_global + loss_local\n",
    "            loss_list_error_w_global[t.item()] += loss_global.item()\n",
    "            loss_list_error_w_local[t.item()] += loss_local.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_error[t.item()] += loss.item()"
   ],
   "id": "9439f693dc7c342a"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "df_target = pd.DataFrame(loss_list_target.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_local = pd.DataFrame(loss_list_error_w_local.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_global = pd.DataFrame(loss_list_error_w_global.items(), columns=['Time', 'Loss'])\n",
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "# compute snr\n",
    "snr = score_model.sde.get_snr(diffusion_time)\n",
    "#upper_bound_loss = (np.sqrt(2) + 1) / (std.numpy()**2)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, sharex=True, figsize=(16, 3), tight_layout=True)\n",
    "ax[0].plot(df_target['Time'], np.log(df_target['Loss']), label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], np.log(df_score['Loss']), label='Score Loss')\n",
    "#ax[1].plot(df_score['Time'], df_score['Loss'] / upper_bound_loss, label='Score Loss')\n",
    "ax[1].plot(diffusion_time, snr, label='log snr', alpha=0.5)\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']), label='Local Weighted Loss')\n",
    "ax[2].plot(df_error_w_global['Time'], np.log(df_error_w_global['Loss']), label='Global Weighted Loss')\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']+df_error_w_global['Loss']), label='Weighted Loss (as in Optimization)')\n",
    "ax[3].plot(df_error['Time'], np.log(df_error['Loss']), label='Loss on Error')\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Log Loss')\n",
    "    a.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/losses_diffusion_time.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time.cpu(),\n",
    "         weighting_function(diffusion_time, sde=score_model.sde, weighting_type=score_model.weighting_type,\n",
    "                            prediction_type=score_model.prediction_type).cpu(),\n",
    "         label='weighting')\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7b4be84f15c98163"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "256b34c67fec63ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 8\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_samples=20, grid_size=n_grid,\n",
    "                                                                            n_time_points=10,\n",
    "                                                                            normalize=False, random_seed=0)\n",
    "n_post_samples = 100\n",
    "param_names = [r'$\\mu$', r'$\\log \\sigma$']\n",
    "#score_model.current_number_of_obs = 4  # we can choose here, how many observations are passed together through the score"
   ],
   "id": "14935db2d706e220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(valid_data)",
   "id": "221629eb0b1bcd11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "from importlib import reload\n",
    "import diffusion_sampling\n",
    "reload(diffusion_sampling)\n",
    "from diffusion_sampling import langevin_sampling, adaptive_sampling, euler_maruyama_sampling, probability_ode_solving"
   ],
   "id": "645218d0cae036ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = langevin_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                   diffusion_steps=300, langevin_steps=5, step_size_factor=0.05,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "9c4215aa8162e3c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_langevin_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_langevin_sampler.png')"
   ],
   "id": "57d31b5a355f6187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = euler_maruyama_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                        diffusion_steps=1000, device=torch_device, verbose=True)"
   ],
   "id": "3face4537f5dec9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sampler.png')"
   ],
   "id": "8cc204643a1c02a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 10\n",
    "t1_value = mini_batch_size /( (n_grid*n_grid) //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    'size': mini_batch_size,\n",
    "    #'damping_factor': lambda t: t1_value + (t0_value - t1_value) * 0.5 * (1 + torch.cos(torch.pi * t)),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * score_model.sde.kernel(log_snr=score_model.sde.get_snr(t))[1],\n",
    "    'damping_factor': lambda t: 0.1, #t1_value,\n",
    "    #'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * torch.sigmoid(20*(t-0.3))\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "9c6c34e9cdfa898f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = euler_maruyama_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                         mini_batch_arg=mini_batch_arg,\n",
    "                                                         diffusion_steps=200, device=torch_device, verbose=True)"
   ],
   "id": "f19ece3f088b18cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler.png')"
   ],
   "id": "4d28388204d5e6e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   #n_scores_update=10,\n",
    "                                                                   pareto_smooth_fraction=0.3,\n",
    "                                                                   diffusion_steps=100, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "222ad64f89402155"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\sigma$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_pareto_sampler.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\sigma$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_pareto_sampler.png')"
   ],
   "id": "a935a9a49bedc972"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data, n_post_samples,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "512a78f9e869bcae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler.png')"
   ],
   "id": "5f8c057c1ade4924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = probability_ode_solving(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                         run_sampling_in_parallel=False,\n",
    "                                                         device=torch_device, verbose=True)"
   ],
   "id": "1ab8276e2a4536ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_ode.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_ode.png')"
   ],
   "id": "19c4d2969ea62222",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conditions_global = (np.median(posterior_global_samples_valid, axis=0), posterior_global_samples_valid)[1]\n",
    "posterior_local_samples_valid = euler_maruyama_sampling(score_model, valid_data,\n",
    "                                                        n_post_samples=n_post_samples, conditions=conditions_global,\n",
    "                                                        diffusion_steps=50, device=torch_device, verbose=True)"
   ],
   "id": "2c717d3846b7c224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          variable_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "45c52a07bf52cc5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#conditions_global = np.median(posterior_global_samples_valid, axis=1)\n",
    "posterior_local_samples_valid = probability_ode_solving(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                        run_sampling_in_parallel=False,\n",
    "                                                        conditions=posterior_global_samples_valid, device=torch_device, verbose=True)"
   ],
   "id": "762d404a28f4c373",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          variable_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "d4e6d9f8b3685a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(posterior_global_samples_valid, posterior_local_samples_valid)",
   "id": "1a19eb978ef1cabf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 2\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 0]), np.std(posterior_global_samples_valid[valid_id, :, 0]))\n",
    "print('log sigma:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][0].item())\n",
    "print('log sigma:', valid_prior_global[valid_id][1].item())"
   ],
   "id": "1d0aa4152cb5a1fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "med = np.median(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "std = np.std(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "error = (med-valid_prior_local[valid_id].numpy())**2\n",
    "visualize_simulation_output(np.stack((med, valid_prior_local[valid_id], )),\n",
    "                            title_prefix=['Posterior Median', 'True'])\n",
    "\n",
    "visualize_simulation_output(np.stack((std, error)), title_prefix=['Uncertainty', 'Error'], same_scale=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4), tight_layout=True)\n",
    "plt.errorbar(x=valid_prior_local[valid_id].flatten(), y=med.flatten(), yerr=1.96*std.flatten(), fmt='o')\n",
    "plt.plot([np.min(med), np.max(med)], [np.min(med), np.max(med)], 'k--')\n",
    "plt.axhline(np.median(posterior_global_samples_valid[valid_id, :], axis=0)[0], color='red', linestyle='--',\n",
    "            label='Global posterior mean', alpha=0.75)\n",
    "plt.ylabel('Prediction')\n",
    "plt.xlabel('True')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "210db47a64860e37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare to STAN",
   "id": "d84533cd17f7a4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_posterior_stan = np.load('problems/grid/global_posterior.npy')#[:, -100:]\n",
    "local_posterior_stan = np.load('problems/grid/local_posterior.npy')#[:, -100:]\n",
    "true_global = np.load('problems/grid/true_global.npy')\n",
    "true_local = np.load('problems/grid/true_local.npy')\n",
    "\n",
    "n_grid_stan = int(np.sqrt(true_local.shape[1]))\n",
    "\n",
    "test_data = []\n",
    "for g, l in zip(true_global, true_local):\n",
    "    sim_dict = {\n",
    "        'mu': g[0].reshape(1,1),\n",
    "        'log_sigma': g[1].reshape(1,1),\n",
    "        'theta': l.reshape(1, n_grid_stan, n_grid_stan)\n",
    "    }\n",
    "    td = simulator_test(sim_dict)['observable']\n",
    "    test_data.append(td)\n",
    "test_data = np.concatenate(test_data)\n",
    "test_data.shape"
   ],
   "id": "5408d23f382ee765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t0_value = 1\n",
    "t1_value = 0.01\n",
    "mini_batch_arg = {\n",
    "    #'size': 2,\n",
    "    #'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t)\n",
    "}\n",
    "#score_model.sde.s_shift_cosine = 6\n",
    "n_post_samples = 25\n",
    "param_names = [r'$\\mu$', r'$\\log \\sigma$']\n",
    "local_param_names = [r'$\\theta_{'+str(i)+'}$' for i in range(n_grid_stan**2)]\n",
    "param_names_stan = ['STAN '+ p for p in param_names]\n",
    "score_model.max_number_of_obs"
   ],
   "id": "b148238acaad67a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#score_model.sde.s_shift_cosine = 0\n",
    "posterior_global_samples_test = adaptive_sampling(score_model, test_data, n_post_samples,\n",
    "                                                  mini_batch_arg=mini_batch_arg,\n",
    "                                                  run_sampling_in_parallel=False,\n",
    "                                                  device=torch_device, verbose=True)"
   ],
   "id": "5022c769aa3f6f1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#score_model.sde.s_shift_cosine = 0\n",
    "posterior_local_samples_test = euler_maruyama_sampling(score_model, test_data, n_post_samples,\n",
    "                                                 conditions=posterior_global_samples_test,\n",
    "                                                 #run_sampling_in_parallel=False,\n",
    "                                                 diffusion_steps=50,\n",
    "                                                 device=torch_device, verbose=True)"
   ],
   "id": "7e6c9b56ee4433e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_global_samples_test, true_global, variable_names=param_names)\n",
    "#diagnostics.recovery(posterior_global_samples_test, np.median(global_posterior_stan, axis=1),\n",
    "#                     variable_names=param_names, xlabel='STAN Median Estimate')\n",
    "diagnostics.recovery(global_posterior_stan, true_global, variable_names=param_names_stan);"
   ],
   "id": "f55aaf3dab4ebb02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.calibration_ecdf(posterior_global_samples_test, true_global, difference=True, variable_names=param_names)\n",
    "diagnostics.calibration_ecdf(global_posterior_stan, true_global, difference=True, variable_names=param_names_stan);"
   ],
   "id": "86a078b457fc8ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_var = np.exp(np.median(posterior_global_samples_test, axis=1)[:, 1])[:, np.newaxis] ** 2\n",
    "shrinkage = 1-np.var(np.median(posterior_local_samples_test, axis=1), axis=1)/global_var\n",
    "\n",
    "global_var_stan = np.exp(np.median(global_posterior_stan, axis=1)[:, 1])**2\n",
    "shrinkage_stan = 1-np.var(np.median(local_posterior_stan, axis=1), axis=1)/global_var_stan\n",
    "\n",
    "true_var = np.exp(true_global)[:, 1]**2\n",
    "shrinkage_true = 1-np.var(true_local, axis=1)/true_var\n",
    "\n",
    "s_order = np.argsort(shrinkage_true)\n",
    "shrinkage = shrinkage.flatten()[s_order]\n",
    "shrinkage_stan = shrinkage_stan.flatten()[s_order]\n",
    "shrinkage_true = shrinkage_true[s_order]\n",
    "\n",
    "min_s = -10\n",
    "shrinkage[shrinkage < min_s] = min_s\n",
    "shrinkage_stan[shrinkage_stan < min_s] = min_s\n",
    "shrinkage_true[shrinkage_true < min_s] = min_s\n",
    "\n",
    "plt.title('Shrinkage')\n",
    "plt.plot(shrinkage, label='score', alpha=0.75)\n",
    "plt.plot(shrinkage_stan, label='STAN', alpha=0.75)\n",
    "plt.plot(shrinkage_true, label='true', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Correlation shrinkage score and STAN:', np.corrcoef(shrinkage, shrinkage_stan)[0, 1])\n",
    "print('Correlation shrinkage true and score:', np.corrcoef(shrinkage_true, shrinkage)[0, 1])\n",
    "print('Correlation shrinkage true and STAN:', np.corrcoef(shrinkage_true, shrinkage_stan)[0, 1])\n",
    "\n",
    "print(f\"Score shrinkage < STAN shrinkage: {(shrinkage < shrinkage_stan).sum() / shrinkage.shape[0]*100}%\")"
   ],
   "id": "c0955002252fe8f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1), true_local,\n",
    "                     variable_names=local_param_names)\n",
    "diagnostics.recovery(local_posterior_stan, true_local, variable_names=local_param_names);"
   ],
   "id": "75bd854cbcbf8a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1),\n",
    "                     np.median(local_posterior_stan, axis=1), ylabel='Score Based Estimates', xlabel='STAN Median Estimate');"
   ],
   "id": "2a150206b37ca215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(posterior_global_samples_test[:10], posterior_local_samples_test[:, :, :, np.newaxis][:10], min_max=(-10,10))",
   "id": "726ce846c5689f13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(global_posterior_stan[:10], local_posterior_stan[:, :, :, np.newaxis][:10], min_max=(-10,10))",
   "id": "9ef8293c8245a03a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize the Score",
   "id": "34be074a14035b99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 8\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_samples=10, grid_size=n_grid,\n",
    "                                                                            normalize=False, random_seed=0)\n",
    "\n",
    "valid_id = 3\n",
    "\n",
    "diffusion_time = generate_diffusion_time(size=10, device=torch_device)\n",
    "x_valid = valid_data[valid_id].to(torch_device)\n",
    "x_valid_norm = score_model.prior.normalize_data(x_valid)\n",
    "theta_global = score_model.prior.normalize_theta(valid_prior_global[valid_id], global_params=True).cpu().numpy()  # we normalize as the score is normalized space\n",
    "print(valid_id, 'theta global', theta_global)"
   ],
   "id": "2bb4365052f36ccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sample = adaptive_sampling(score_model, x_valid[None], conditions=None, n_post_samples=1, #e_abs=0.00078,\n",
    "                                t_end=0, random_seed=0, device=torch_device)\n",
    "test_sample = score_model.prior.normalize_theta(torch.tensor(test_sample), global_params=True).cpu().numpy()\n",
    "print(test_sample)"
   ],
   "id": "6834acb453d2cbc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_sample_path = np.array([adaptive_sampling(score_model, x_valid[None],\n",
    "                                                    conditions=None, n_post_samples=1, t_end=t,\n",
    "                                                    random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path = score_model.prior.normalize_theta(torch.tensor(posterior_sample_path), global_params=True).cpu().numpy()\n",
    "\n",
    "posterior_sample_path2 = np.array([probability_ode_solving(score_model, x_valid[None],\n",
    "                                                           conditions=None, n_post_samples=1, t_end=t, random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path2 = score_model.prior.normalize_theta(torch.tensor(posterior_sample_path2), global_params=True).cpu().numpy()\n",
    "\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "a39c10afe97222b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define grid boundaries and resolution for your 2D space.\n",
    "x_min, x_max, y_min, y_max = -1.5, 1.5, -1.5, 1.5\n",
    "grid_res = 10  # Number of points per dimension\n",
    "\n",
    "# Create a meshgrid of points\n",
    "x_vals = np.linspace(x_min, x_max, grid_res)\n",
    "y_vals = np.linspace(y_min, y_max, grid_res)\n",
    "xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "# Stack into (N,2) where N = grid_res*grid_res\n",
    "grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Convert grid to a torch tensor and move to device\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32, device=torch_device)\n",
    "x_valid_norm_e = x_valid_norm.reshape(10, -1).to(torch_device)\n",
    "x_valid_norm_ext = x_valid_norm_e.unsqueeze(0).repeat(grid_tensor.shape[0], 1, 1)\n",
    "\n",
    "# Dictionary to hold score outputs for each time\n",
    "scores = {}\n",
    "scores_smoothed = {}\n",
    "drifts = {}\n",
    "drifts_smoothed = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Evaluate the score model for each time value\n",
    "    for t in diffusion_time:\n",
    "        # Create a tensor of time values for each grid point\n",
    "        t_tensor = torch.full((grid_tensor.shape[0], 1), t.item(), dtype=torch.float32, device=torch_device)\n",
    "        epsilon = torch.randn_like(grid_tensor, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "        # perturb theta\n",
    "        snr = score_model.sde.get_snr(t=t_tensor)\n",
    "        alpha, sigma = score_model.sde.kernel(log_snr=snr)\n",
    "        z = grid_tensor #alpha * grid_tensor + sigma * epsilon\n",
    "\n",
    "        # Evaluate the score model\n",
    "        score_indv = torch.zeros((x_valid_norm_ext.shape[2], grid_tensor.shape[0], 2))\n",
    "\n",
    "        prior_scores = (1 - t) * score_model.prior.score_global_batch(z)\n",
    "        prior_scores_indv = prior_scores.unsqueeze(0)\n",
    "        for i in range(x_valid_norm_ext.shape[2]):\n",
    "            score_indv[i] = score_model.forward_global(theta_global=z, time=t_tensor, x=x_valid_norm_ext[:, :, i].unsqueeze(-1),\n",
    "                                                       pred_score=True, clip_x=False)\n",
    "        score_indv = score_indv - prior_scores_indv\n",
    "\n",
    "        score = score_indv.sum(axis=0)\n",
    "        score = score + prior_scores\n",
    "        scores[t.item()] = score.cpu().numpy()\n",
    "\n",
    "        # score_pareto = torch.zeros_like(score)\n",
    "        # for i in range(score_indv.shape[1]):\n",
    "        #     score_pareto[i] = pareto_smooth_sum(score_indv[:, i].unsqueeze(0),\n",
    "        #                                            tail_fraction=0.3)[0]  # expects dim to be the batch\n",
    "        #\n",
    "        # score_pareto = score_pareto + prior_scores\n",
    "        # scores_smoothed[t.item()] = score_pareto.cpu().numpy()\n",
    "\n",
    "        f, g = score_model.sde.get_f_g(x=z, t=t_tensor)\n",
    "        drift = f - 0.5 * torch.square(g) * scores[t.item()]\n",
    "        drifts[t.item()] = drift.cpu().numpy()\n",
    "\n",
    "        # drift_pareto = f - 0.5 * torch.square(g) * scores_smoothed[t.item()]\n",
    "        # drifts_smoothed[t.item()] = drift_pareto.cpu().numpy()"
   ],
   "id": "7fc6f658fa650a39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(scores.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)  # negative since we are plotting the reverse score\n",
    "\n",
    "    ax = axes[i]\n",
    "\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    if i != 0:\n",
    "        j = i\n",
    "        h2, = ax.plot(posterior_sample_path[:-j, 0, 0, 0], posterior_sample_path[:-j, 0, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "        h4, = ax.plot(posterior_sample_path2[:-j, 0, 0, 0], posterior_sample_path2[:-j, 0, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=5*n_grid*n_grid, alpha=.75, label='Score')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5, h0, h1, h2, h4], loc='lower center', ncols=5, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/score.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "2bc17186f7f85865",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(drifts.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)\n",
    "\n",
    "    ax = axes[i]\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    j = len(diffusion_time)-i-1\n",
    "    h2, = ax.plot(posterior_sample_path[j:, 0, 0, 0], posterior_sample_path[j:, 0, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "    h4, = ax.plot(posterior_sample_path2[j:, 0, 0, 0], posterior_sample_path2[j:, 0, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=5*n_grid*n_grid, alpha=.75,\n",
    "                   label='Probability Flow')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5, h0, h1, h2, h4], loc='lower center', ncols=5, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/drift.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "e1ba6686e7bab099",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "445a5176d3cf662a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical-abi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
