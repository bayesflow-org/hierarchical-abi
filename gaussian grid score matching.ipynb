{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gaussian on a Grid Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, weighting_function\n",
    "from diffusion_sampling import euler_maruyama_sampling, adaptive_sampling, probability_ode_solving, langevin_sampling, \\\n",
    "    pareto_smooth_sum\n",
    "from gaussian_grid_simulator import Prior, Simulator, visualize_simulation_output, generate_synthetic_data\n",
    "from helper_functions import generate_diffusion_time, count_parameters"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cpu\")",
   "id": "70c931844e56c698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior()\n",
    "simulator_test = Simulator()\n",
    "\n",
    "# test the simulator\n",
    "sim_test = simulator_test(prior.sample_full(1, n_grid=8))['observable']\n",
    "visualize_simulation_output(sim_test)"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the different snrs\n",
    "t = torch.linspace(0, 1, 1000)\n",
    "bins = 100\n",
    "\n",
    "sub_vp = ['', 'sub_'][0]\n",
    "\n",
    "fig, ax = plt.subplots(2, 6, sharex='col', sharey='col', figsize=(12, 6), tight_layout=True)\n",
    "for a, sub_vp in zip(ax, ['', 'sub_']):\n",
    "    snr = SDE(sub_vp+'variance_preserving', 'linear').get_snr(t)\n",
    "    h1 = a[0].plot(t, snr, label='linear', alpha=0.75)\n",
    "    a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "    alpha, sigma = SDE(sub_vp+'variance_preserving', 'linear').kernel(t)\n",
    "    a[2].plot(t, alpha, label='linear', alpha=0.75)\n",
    "    a[3].plot(t, sigma, label='linear', alpha=0.75)\n",
    "    a[4].plot(t, alpha**2+sigma**2, label='linear', alpha=0.75)\n",
    "    a[5].plot(t, alpha+sigma, label='linear', alpha=0.75)\n",
    "\n",
    "    snr = SDE(sub_vp+'variance_preserving', 'cosine').get_snr(t)\n",
    "    h2 = a[0].plot(t, snr, label='cosine', alpha=0.75)\n",
    "    a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "    alpha, sigma = SDE(sub_vp+'variance_preserving', 'cosine').kernel(t)\n",
    "    a[2].plot(t, alpha, label='cosine', alpha=0.75)\n",
    "    a[3].plot(t, sigma, label='cosine', alpha=0.75)\n",
    "    a[4].plot(t, alpha**2+sigma**2, label='cosine', alpha=0.75)\n",
    "    a[5].plot(t, alpha+sigma, label='cosine', alpha=0.75)\n",
    "\n",
    "    snr = SDE(sub_vp+'variance_preserving', 'cosine', s_shift_cosine=0.8).get_snr(t)\n",
    "    h3 = a[0].plot(t, snr, label='shifted_cosine', alpha=0.75)\n",
    "    a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "    alpha, sigma = SDE(sub_vp+'variance_preserving', 'cosine', s_shift_cosine=0.8).kernel(t)\n",
    "    a[2].plot(t, alpha, label='shifted_cosine', alpha=0.75)\n",
    "    a[3].plot(t, sigma, label='shifted_cosine', alpha=0.75)\n",
    "    a[4].plot(t, alpha**2+sigma**2, label='shifted_cosine', alpha=0.75)\n",
    "    a[5].plot(t, alpha+sigma, label='shifted_cosine', alpha=0.75)\n",
    "\n",
    "    if sub_vp == 'sub_':\n",
    "        snr = SDE(sub_vp+'variance_preserving', 'flow_matching').get_snr(t)\n",
    "        h4 = a[0].plot(t, snr, label='flow_matching', alpha=0.75)\n",
    "        a[1].hist(snr, bins=bins, density=True, alpha=0.75)\n",
    "        alpha, sigma = SDE(sub_vp+'variance_preserving', 'flow_matching').kernel(t)\n",
    "        a[2].plot(t, alpha, label='flow_matching', alpha=0.75)\n",
    "        a[3].plot(t, sigma, label='flow_matching', alpha=0.75)\n",
    "        a[4].plot(t, alpha**2+sigma**2, label='flow_matching', alpha=0.75)\n",
    "        a[5].plot(t, alpha+sigma, label='flow_matching', alpha=0.75)\n",
    "\n",
    "    # dotted line at 0\n",
    "    a[0].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for axis in a:\n",
    "        axis.set_xlabel('t')\n",
    "    a[1].set_xlabel('snr')\n",
    "    a[0].set_ylabel(f'snr\\n{sub_vp}variance_preserving')\n",
    "    a[0].set_title(f'Signal-to-noise ratio')\n",
    "    a[1].set_title(f'Signal-to-noise ratio')\n",
    "    a[2].set_title(f'alpha(t)')\n",
    "    a[3].set_title(f'sigma(t)')\n",
    "    a[4].set_title(f'alpha(t)^2 + sigma(t)^2')\n",
    "    a[5].set_title(f'alpha(t) + sigma(t)')\n",
    "fig.legend(handles=h1+h2+h3+h4, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.05))\n",
    "#plt.savefig('plots/noise schedules.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "4be4f20621659931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t = torch.linspace(0, 1, 1000)\n",
    "x_0 = torch.tensor([0.01])\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, sharex='col', sharey='col', figsize=(5, 4), tight_layout=True)\n",
    "for a, sub_vp in zip(ax, ['', 'sub_']):\n",
    "    f, g = SDE(sub_vp+'variance_preserving', 'linear').get_f_g(t, x=x_0)\n",
    "    h1 = a[0].plot(t, f, label='linear', alpha=0.75)\n",
    "    a[1].plot(t, g, label='linear', alpha=0.75)\n",
    "\n",
    "    f, g = SDE(sub_vp+'variance_preserving', 'cosine').get_f_g(t, x=x_0)\n",
    "    h2 = a[0].plot(t, f, label='cosine', alpha=0.75)\n",
    "    a[1].plot(t, g, label='cosine', alpha=0.75)\n",
    "\n",
    "    #f, g = SDE(sub_vp+'variance_preserving', 'cosine', s_shift_cosine=0.8).get_f_g(t, x=x_0)\n",
    "    #h3 = a[0].plot(t, f, label='shifted_cosine', alpha=0.75)\n",
    "    #a[1].plot(t, g, label='shifted_cosine', alpha=0.75)\n",
    "\n",
    "    if sub_vp == 'sub_':\n",
    "        f, g = SDE(sub_vp+'variance_preserving', 'flow_matching').get_f_g(t, x=x_0)\n",
    "        h4 = a[0].plot(t, f, label='flow_matching', alpha=0.75)\n",
    "        a[1].plot(t, g, label='flow_matching', alpha=0.75)\n",
    "\n",
    "    for axis in a:\n",
    "        axis.set_xlabel('t')\n",
    "    a[0].set_ylabel(f'snr\\n{sub_vp}variance_preserving')\n",
    "    a[0].set_title(f'f(z={round(x_0.item(), 2)},t)')\n",
    "    a[1].set_title(f'g(t)')\n",
    "fig.legend(handles=h1+h2+h4, loc='lower center', ncol=4, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.show()"
   ],
   "id": "9b1736d64b850509",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the kernel\n",
    "t = generate_diffusion_time(1000)\n",
    "sde_test = SDE('variance_preserving', 'cosine')\n",
    "snr = sde_test.get_snr(t)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, sharex='row', sharey='row', figsize=(12, 6), tight_layout=True)\n",
    "for a, wt in zip(ax.T, [None, 'likelihood_weighting', 'flow_matching', 'sigmoid', 'min-snr']):\n",
    "    w = weighting_function(t, sde_test, weighting_type=wt)\n",
    "    a[0].plot(t, w / max(w))\n",
    "    a[0].set_xlabel(r'$t$')\n",
    "    a[0].set_ylabel('Normalized weight')\n",
    "    a[0].set_title(wt)\n",
    "\n",
    "    a[1].plot(snr, w / max(w))\n",
    "    a[1].set_xlabel(r'$\\lambda$')\n",
    "    a[1].set_ylabel('Normalized weight')\n",
    "    a[0].set_title(wt)\n",
    "#plt.savefig(f'plots/weighting_functions_{sde_test.noise_schedule}.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "3985a1dbd9349209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_hierarchical_score_loss(theta_global_batch, theta_local_batch, x_batch, model, epsilon_global_batch=None, epsilon_local_batch=None):\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=theta_global_batch.shape[0],\n",
    "                                             return_batch=True, device=theta_global_batch.device)\n",
    "\n",
    "    # sample from the Gaussian kernel, just learn the noise\n",
    "    if epsilon_global_batch is None:\n",
    "        epsilon_global_batch = torch.randn_like(theta_global_batch, dtype=theta_global_batch.dtype,\n",
    "                                                device=theta_global_batch.device)\n",
    "    if epsilon_local_batch is None:\n",
    "        epsilon_local_batch = torch.randn_like(theta_local_batch, dtype=theta_local_batch.dtype,\n",
    "                                               device=theta_local_batch.device)\n",
    "\n",
    "    # perturb the theta batch\n",
    "    alpha, sigma = model.sde.kernel(t=diffusion_time)\n",
    "    z_global = alpha * theta_global_batch + sigma * epsilon_global_batch\n",
    "    if model.global_number_of_obs > 1:\n",
    "        # global params are not factorized to the same level as local params\n",
    "        z_local = alpha.unsqueeze(1) * theta_local_batch + sigma.unsqueeze(1) * epsilon_local_batch\n",
    "    else:\n",
    "        z_local = alpha * theta_local_batch + sigma * epsilon_local_batch\n",
    "    # predict from perturbed theta\n",
    "    pred_global, pred_local = model(theta_global=z_global, theta_local=z_local,\n",
    "                                    time=diffusion_time, x=x_batch, pred_score=False)\n",
    "\n",
    "    effective_weight = weighting_function(diffusion_time, sde=model.sde, weighting_type=model.weighting_type,\n",
    "                                          prediction_type=model.prediction_type)\n",
    "    # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "    loss_global = torch.mean(effective_weight * torch.sum(torch.square(pred_global - epsilon_global_batch), dim=-1))\n",
    "    loss_local = torch.mean(effective_weight * torch.sum(torch.square(pred_local - epsilon_local_batch), dim=-1))\n",
    "    return loss_global + loss_local\n",
    "\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_hierarchical_score_model(model, dataloader, dataloader_valid=None, epochs=100, lr=1e-4, rectified_flow=False, device=None):\n",
    "    print(f\"Training {model.prediction_type}-model for {epochs} epochs with learning rate {lr} and {model.sde.kernel_type},\"\n",
    "          f\" {model.sde.noise_schedule} schedule and {model.weighting_type} weighting.\")\n",
    "    if model.sde.noise_schedule == 'flow_matching':\n",
    "        rectified_flow = True\n",
    "    if rectified_flow:\n",
    "        print(f'Using rectified flow.')\n",
    "    print(f\"Model has {model.n_params_global} global parameters and {model.n_params_local} local parameters and is uses compositional conditioning with {model.max_number_of_obs} factors.\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Add Cosine Annealing Scheduler\n",
    "    #scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    loss_history = np.zeros((epochs, 2))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        # for each sample in the batch, calculate the loss for a random diffusion time\n",
    "        for theta_global_batch, epsilon_global_batch, theta_local_batch, epsilon_local_batch, x_batch in dataloader:\n",
    "            # initialize the gradients\n",
    "            optimizer.zero_grad()\n",
    "            theta_global_batch = theta_global_batch.to(device)\n",
    "            theta_local_batch = theta_local_batch.to(device)\n",
    "            x_batch = x_batch.to(device)\n",
    "            if rectified_flow:\n",
    "                epsilon_global_batch = epsilon_global_batch.to(device)\n",
    "                epsilon_local_batch = epsilon_local_batch.to(device)\n",
    "                # calculate the loss\n",
    "                loss = compute_hierarchical_score_loss(theta_global_batch=theta_global_batch,\n",
    "                                                       theta_local_batch=theta_local_batch,\n",
    "                                                       epsilon_global_batch=epsilon_global_batch,\n",
    "                                                       epsilon_local_batch=epsilon_local_batch,\n",
    "                                                       x_batch=x_batch, model=model)\n",
    "            else:\n",
    "                # calculate the loss\n",
    "                loss = compute_hierarchical_score_loss(theta_global_batch=theta_global_batch,\n",
    "                                                       theta_local_batch=theta_local_batch,\n",
    "                                                       x_batch=x_batch, model=model)\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "            dataloader.dataset.on_batch_end()\n",
    "        #scheduler.step()\n",
    "        dataloader.dataset.on_epoch_end()\n",
    "\n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        if dataloader_valid is not None:\n",
    "            with torch.no_grad():\n",
    "                for theta_global_batch, epsilon_global_batch, theta_local_batch, epsilon_local_batch, x_batch in dataloader_valid:\n",
    "                    theta_global_batch = theta_global_batch.to(device)\n",
    "                    theta_local_batch = theta_local_batch.to(device)\n",
    "                    x_batch = x_batch.to(device)\n",
    "                    if rectified_flow:\n",
    "                        epsilon_global_batch = epsilon_global_batch.to(device)\n",
    "                        epsilon_local_batch = epsilon_local_batch.to(device)\n",
    "                        # calculate the loss\n",
    "                        loss = compute_hierarchical_score_loss(theta_global_batch=theta_global_batch,\n",
    "                                                               theta_local_batch=theta_local_batch,\n",
    "                                                               epsilon_global_batch=epsilon_global_batch,\n",
    "                                                               epsilon_local_batch=epsilon_local_batch,\n",
    "                                                               x_batch=x_batch, model=model)\n",
    "                    else:\n",
    "                        # calculate the loss\n",
    "                        loss = compute_hierarchical_score_loss(theta_global_batch=theta_global_batch,\n",
    "                                                               theta_local_batch=theta_local_batch,\n",
    "                                                               x_batch=x_batch, model=model)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "        loss_history[epoch] = [np.mean(total_loss), np.mean(valid_loss)]\n",
    "        print_str = f\"Epoch {epoch+1}/{epochs}, Loss: {np.mean(total_loss):.4f}, \" \\\n",
    "                    f\"Valid Loss: {np.mean(valid_loss):.4f}\"\n",
    "        print(print_str, end='\\r')\n",
    "        # Update the checkpoint after each epoch of training.\n",
    "        #torch.save(model.state_dict(), 'ckpt.pth')\n",
    "    return loss_history"
   ],
   "id": "3e4488af88ad4738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GaussianGridProblem(Dataset):\n",
    "    def __init__(self, n_data, prior, online_learning=False, max_number_of_obs=1):\n",
    "        # Create model and dataset\n",
    "        self.prior = prior\n",
    "        self.n_data = n_data\n",
    "        self.max_number_of_obs = max_number_of_obs\n",
    "        self.n_obs = self.max_number_of_obs  # this can change for each batch\n",
    "        self.online_learning = online_learning\n",
    "        self.generate_data()\n",
    "\n",
    "    def generate_data(self):\n",
    "        # Create model and dataset\n",
    "        self.thetas_global, self.thetas_local, self.xs = generate_synthetic_data(\n",
    "            prior,\n",
    "            grid_size=int(np.ceil(np.sqrt(self.max_number_of_obs))) if self.max_number_of_obs > 1 else None,  # no need to simulate the full grid\n",
    "            data_size=self.max_number_of_obs if self.max_number_of_obs > 1 else None,\n",
    "            n_samples=self.n_data, normalize=True\n",
    "        )\n",
    "        self.epsilon_global = torch.randn_like(self.thetas_global, dtype=torch.float32)\n",
    "        self.epsilon_local = torch.randn_like(self.thetas_local, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # this should return the size of the dataset\n",
    "        return len(self.thetas_global)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this should return one sample from the dataset\n",
    "        features_global = self.thetas_global[idx]\n",
    "        if self.max_number_of_obs > 1:\n",
    "            features_local = self.thetas_local[idx, :self.n_obs]\n",
    "            target = self.xs[idx, :self.n_obs]\n",
    "        else:\n",
    "            features_local = self.thetas_local[idx]\n",
    "            target = self.xs[idx]\n",
    "        noise_global = self.epsilon_global[idx]\n",
    "        noise_local = self.epsilon_local[idx]\n",
    "        return features_global, noise_global, features_local, noise_local, target\n",
    "\n",
    "    def on_epoch_end(self):  # for online learning\n",
    "        # Regenerate data at the end of each epoch\n",
    "        if self.online_learning:\n",
    "            self.generate_data()\n",
    "\n",
    "    def on_batch_end(self):\n",
    "        # Called at the end of each batch\n",
    "        if self.max_number_of_obs > 1:\n",
    "            # sample number of observations\n",
    "            self.n_obs = np.random.randint(1, self.max_number_of_obs+1)"
   ],
   "id": "c7c102e87028cc24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "max_number_of_obs = 10\n",
    "\n",
    "dataset = GaussianGridProblem(\n",
    "    n_data=25000,\n",
    "    prior=prior,\n",
    "    online_learning=False,\n",
    "    max_number_of_obs=max_number_of_obs\n",
    ")\n",
    "\n",
    "dataset_valid = GaussianGridProblem(\n",
    "    n_data=batch_size*2,\n",
    "    prior=prior,\n",
    "    max_number_of_obs=max_number_of_obs\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)"
   ],
   "id": "69e6aacbe018a801",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][0]\n",
    ")\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x=1,\n",
    "    hidden_dim=64,\n",
    "    n_blocks=3,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    time_embed_dim=16,\n",
    "    use_film=True,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior\n",
    ")\n",
    "\n",
    "print(score_model.name)\n",
    "count_parameters(score_model)\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "ded414838e34143f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_hierarchical_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 epochs=500, lr=1e-4, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Mean Train')\n",
    "plt.plot(loss_history[:, 1], label='Mean Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')\n",
    "plt.show()"
   ],
   "id": "ab5eef7cdb5ec27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score_model.global_model.embed.state_dict()",
   "id": "545d868a40face75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "75a70bd9458b2655",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_target = {}\n",
    "loss_list_score = {}\n",
    "loss_list_error_w_global = {}\n",
    "loss_list_error_w_local = {}\n",
    "loss_list_error = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=100, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        loss_list_target[t.item()] = 0\n",
    "        loss_list_score[t.item()] = 0\n",
    "        loss_list_error_w_global[t.item()] = 0\n",
    "        loss_list_error_w_local[t.item()] = 0\n",
    "        loss_list_error[t.item()] = 0\n",
    "\n",
    "        for theta_global_batch, _, theta_local_batch, _, x_batch in dataloader_valid:\n",
    "            theta_global_batch = theta_global_batch.to(torch_device)\n",
    "            theta_local_batch = theta_local_batch.to(torch_device)\n",
    "            x_batch = x_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon_global = torch.randn_like(theta_global_batch, dtype=torch.float32, device=torch_device)\n",
    "            epsilon_local = torch.randn_like(theta_local_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_global_batch.shape[0], 1), t,\n",
    "                                  dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            alpha, sigma = score_model.sde.kernel(t=t_tensor)\n",
    "            snr = torch.log(torch.square(alpha)) - torch.log(torch.square(sigma))\n",
    "            z_global = alpha * theta_global_batch + sigma * epsilon_global\n",
    "            if score_model.max_number_of_obs > 1:\n",
    "                # global params are not factorized to the same level as local params\n",
    "                alpha_local = alpha.unsqueeze(1)\n",
    "                sigma_local = sigma.unsqueeze(1)\n",
    "            else:\n",
    "                alpha_local = alpha\n",
    "                sigma_local = sigma\n",
    "            z_local = alpha_local * theta_local_batch + sigma_local * epsilon_local\n",
    "\n",
    "            # predict from perturbed theta\n",
    "            pred_epsilon_global, pred_epsilon_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                       time=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score_global, pred_score_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                     time=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score_global = score_model.sde.grad_log_kernel(x=z_global,\n",
    "                                                                x0=theta_global_batch, t=t_tensor)\n",
    "            if score_model.max_number_of_obs == 1:\n",
    "                true_score_local = score_model.sde.grad_log_kernel(x=z_local,\n",
    "                                                                   x0=theta_local_batch,\n",
    "                                                                   t=t_tensor)\n",
    "            else:\n",
    "                true_score_local = []\n",
    "                for i in range(score_model.max_number_of_obs):\n",
    "                    score_local = score_model.sde.grad_log_kernel(x=z_local[:, i],\n",
    "                                                                   x0=theta_local_batch[:, i],\n",
    "                                                                   t=t_tensor)\n",
    "                    true_score_local.append(score_local.unsqueeze(1))\n",
    "                true_score_local = torch.concatenate(true_score_local, dim=1)\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target_global = -epsilon_global / sigma\n",
    "                pred_target_global = -pred_epsilon_global / sigma\n",
    "                target_local = -epsilon_local / sigma_local\n",
    "                pred_target_local = -pred_epsilon_local / sigma_local\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                target_global = epsilon_global\n",
    "                pred_target_global = pred_epsilon_global\n",
    "                target_local = epsilon_local\n",
    "                pred_target_local = pred_epsilon_local\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target_global = alpha*epsilon_global - sigma * theta_global_batch\n",
    "                pred_target_global = alpha*pred_epsilon_global - sigma * theta_global_batch\n",
    "                target_local = alpha_local*epsilon_local - sigma_local * theta_local_batch\n",
    "                pred_target_local = alpha_local*pred_epsilon_local - sigma_local * theta_local_batch\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target_global = theta_global_batch\n",
    "                pred_target_global = (z_global - pred_epsilon_global * sigma) / alpha\n",
    "                target_local = theta_local_batch\n",
    "                pred_target_local = (z_local - pred_epsilon_local * sigma_local) / alpha_local\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_target_global - target_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_target_local - target_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_target[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_score_global - true_score_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_score_local - true_score_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_score[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            w = weighting_function(t_tensor, sde=score_model.sde,\n",
    "                                   weighting_type=score_model.weighting_type, prediction_type=score_model.prediction_type)\n",
    "            loss_global = torch.mean(w * torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(w * torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            #loss = loss_global + loss_local\n",
    "            loss_list_error_w_global[t.item()] += loss_global.item()\n",
    "            loss_list_error_w_local[t.item()] += loss_local.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_error[t.item()] += loss.item()"
   ],
   "id": "3e3644f3daaa5484",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_target = pd.DataFrame(loss_list_target.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_local = pd.DataFrame(loss_list_error_w_local.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_global = pd.DataFrame(loss_list_error_w_global.items(), columns=['Time', 'Loss'])\n",
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "# compute snr\n",
    "snr = score_model.sde.get_snr(diffusion_time)\n",
    "#upper_bound_loss = (np.sqrt(2) + 1) / (std.numpy()**2)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, sharex=True, figsize=(16, 3), tight_layout=True)\n",
    "ax[0].plot(df_target['Time'], np.log(df_target['Loss']), label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], np.log(df_score['Loss']), label='Score Loss')\n",
    "#ax[1].plot(df_score['Time'], df_score['Loss'] / upper_bound_loss, label='Score Loss')\n",
    "ax[1].plot(diffusion_time, snr, label='log snr', alpha=0.5)\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']), label='Local Weighted Loss')\n",
    "ax[2].plot(df_error_w_global['Time'], np.log(df_error_w_global['Loss']), label='Global Weighted Loss')\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']+df_error_w_global['Loss']), label='Weighted Loss (as in Optimization)')\n",
    "ax[3].plot(df_error['Time'], np.log(df_error['Loss']), label='Loss on Error')\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Log Loss')\n",
    "    a.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/losses_diffusion_time.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time.cpu(),\n",
    "         weighting_function(diffusion_time, sde=score_model.sde, weighting_type=score_model.weighting_type,\n",
    "                            prediction_type=score_model.prediction_type).cpu(),\n",
    "         label='weighting')\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "75adebcae950de41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 10\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_samples=10, grid_size=n_grid,\n",
    "                                                                            normalize=False, random_seed=0)\n",
    "n_post_samples = 10\n",
    "#score_model.current_number_of_obs = 2  # we can choose here, how many observations are passed together through the score"
   ],
   "id": "28d63d6aa09d9245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(valid_data)",
   "id": "d7da728796fa5bde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "from importlib import reload\n",
    "import diffusion_sampling\n",
    "reload(diffusion_sampling)\n",
    "from diffusion_sampling import langevin_sampling, adaptive_sampling, euler_maruyama_sampling, probability_ode_solving"
   ],
   "id": "26c940355a3425ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                             #n_scores_update=10,\n",
    "                                                            diffusion_steps=100, langevin_steps=5, step_size_factor=0.05,\n",
    "                                           device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "fd3d0805fc460d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_langevin_sampler.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_langevin_sampler.png')"
   ],
   "id": "118c0de3d3ab0b1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   #n_scores_update=10,\n",
    "                                                                   diffusion_steps=1000, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "e906aed050d0714b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sampler.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sampler.png')"
   ],
   "id": "874d1359427acaa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   n_scores_update=10,\n",
    "                                                                   diffusion_steps=200, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "1a691225901a6b91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler.png')"
   ],
   "id": "cc9389a3bdf1a30d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   #n_scores_update=10,\n",
    "                                                                   pareto_smooth_fraction=0.3,\n",
    "                                                                   diffusion_steps=100, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "1d21f28f75ee1802",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_pareto_sampler.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_pareto_sampler.png')"
   ],
   "id": "213ef9dd610e9df5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([adaptive_sampling(score_model, vd, n_post_samples,\n",
    "                                                             #n_scores_update=10,\n",
    "                                                             e_rel=1.5, max_steps=2000, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "ab55fadb21762acc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler.png')"
   ],
   "id": "509691ae9e76856a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.zeros((len(valid_data), n_post_samples, 2))\n",
    "for i, vd in enumerate(valid_data):\n",
    "    # solve for every sample individually, much slower, and still most of the samples were similar\n",
    "    #for j in range(n_post_samples):\n",
    "    #    posterior_global_samples_valid[i, j] = probability_ode_solving(score_model, vd, n_post_samples=1,\n",
    "    #                                                                    device=torch_device)\n",
    "    # solve for all samples at once\n",
    "    posterior_global_samples_valid[i] = probability_ode_solving(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                #n_scores_update=10,\n",
    "                                                                device=torch_device)"
   ],
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_ode.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_ode.png')"
   ],
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#conditions_global = np.median(posterior_global_samples_valid, axis=0)\n",
    "posterior_local_samples_valid = np.array([euler_maruyama_sampling(score_model, vd,\n",
    "                                                                  n_post_samples=n_post_samples, conditions=c,\n",
    "                                                                  diffusion_steps=50, device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, posterior_global_samples_valid)])"
   ],
   "id": "9de5da14663e13c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          param_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "5c954e8ad82c5e89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#conditions_global = np.median(posterior_global_samples_valid, axis=1)\n",
    "posterior_local_samples_valid = np.array([probability_ode_solving(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                  conditions=c, device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, posterior_global_samples_valid)])"
   ],
   "id": "3a45ca1277eefae1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          param_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "44320e5b15ea6349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_shrinkage(global_samples, local_samples, ci=95):\n",
    "    \"\"\"\n",
    "    Plots the shrinkage of local estimates toward the global mean for each n_data.\n",
    "\n",
    "    Parameters:\n",
    "      global_samples: np.ndarray of shape (n_data, n_samples, 2)\n",
    "                      The last dimension holds [global_mean, log_std].\n",
    "      local_samples:  np.ndarray of shape (n_data, n_samples, n_individuals, 1)\n",
    "                      The last dimension holds the local parameter.\n",
    "      ci:             Confidence interval percentage (default 95).\n",
    "    \"\"\"\n",
    "    n_data, n_samples, _ = global_samples.shape\n",
    "    n_individuals = local_samples.shape[2]\n",
    "\n",
    "    # Create a subplot for each n_data\n",
    "    nrows, ncols = int(np.ceil(n_data / 4)), 4\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(12, int(np.ceil(n_data / 4))*2),\n",
    "                             sharex=True, sharey=True, tight_layout=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # If there is only one subplot, wrap it in a list for consistent indexing.\n",
    "    if n_data == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(n_data):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Process global posterior for this n_data:\n",
    "        global_mean_samples = global_samples[i, :, 0]\n",
    "        global_mean_est = np.mean(global_mean_samples)\n",
    "        global_ci = [global_mean_est-1.96*np.mean(np.exp(global_samples[i, :, 1])),\n",
    "                     global_mean_est+1.96*np.mean(np.exp(global_samples[i, :, 1]))]\n",
    "\n",
    "        # Process local posterior for each individual at data index i:\n",
    "        local_means = np.zeros(n_individuals)\n",
    "        local_cis = np.zeros((n_individuals, 2))\n",
    "\n",
    "        for j in range(n_individuals):\n",
    "            samples_j = local_samples[i, :, j, 0]\n",
    "            local_means[j] = np.mean(samples_j)\n",
    "            local_cis[j, :] = np.percentile(samples_j, [50 - ci/2, 50 + ci/2])\n",
    "\n",
    "        indices = np.arange(n_individuals)\n",
    "        # Plot local estimates with error bars\n",
    "        h1 = ax.errorbar(indices, local_means,\n",
    "                    yerr=[local_means - local_cis[:, 0], local_cis[:, 1] - local_means],\n",
    "                    fmt='o', capsize=5, label='Local posterior mean')\n",
    "\n",
    "        # Plot the global estimate as a horizontal dashed line\n",
    "        h2 = ax.axhline(global_mean_est, color='red', linestyle='--', label='Global posterior mean')\n",
    "        # Shade the global CI\n",
    "        h3 = ax.fill_between(indices, global_ci[0], global_ci[1],\n",
    "                        color='red', alpha=0.2, label='Global 95% CI')\n",
    "\n",
    "        ax.set_ylabel(\"Parameter Value\")\n",
    "        ax.set_title(f\"Data {i}\")\n",
    "    fig.legend(handles=[h1, h2, h3], loc='lower center', ncols=3, bbox_to_anchor=(0.5, -0.05))\n",
    "    axes[-1].set_xlabel(\"Individual Index\")\n",
    "    for i in range(n_data, len(axes)):\n",
    "        # disable axis\n",
    "        axes[i].set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "plot_shrinkage(posterior_global_samples_valid, posterior_local_samples_valid)"
   ],
   "id": "ccad359370327978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 0\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 0]), np.std(posterior_global_samples_valid[valid_id, :, 0]))\n",
    "print('log tau:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][0].item())\n",
    "print('log tau:', valid_prior_global[valid_id][1].item())"
   ],
   "id": "c4bb5620888ab007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "med = np.median(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "std = np.std(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "error = (med-valid_prior_local[valid_id].numpy())**2\n",
    "visualize_simulation_output(np.stack((med, valid_prior_local[valid_id], )),\n",
    "                            title_prefix=['Posterior Median', 'True'])\n",
    "\n",
    "visualize_simulation_output(np.stack((std, error)), title_prefix=['Uncertainty', 'Error'], same_scale=False)"
   ],
   "id": "c9daf84cada1b0a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize the Score",
   "id": "bc9c59ae04231887"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 8\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_samples=10, grid_size=n_grid,\n",
    "                                                                            normalize=False, random_seed=0)\n",
    "\n",
    "valid_id = 5 #0\n",
    "\n",
    "diffusion_time = generate_diffusion_time(size=10, device=torch_device)\n",
    "x_valid = valid_data[valid_id].to(torch_device)\n",
    "x_valid_norm = score_model.prior.normalize_data(x_valid)\n",
    "theta_global = score_model.prior.normalize_theta(valid_prior_global[valid_id], global_params=True).cpu().numpy()  # we normalize as the score is normalized space\n",
    "print(valid_id, 'theta global', theta_global)"
   ],
   "id": "36b9381866daf22e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sample = adaptive_sampling(score_model, x_valid, conditions=None, n_post_samples=1, #e_abs=0.00078,\n",
    "                                e_rel=1.5, max_steps=1000, t_end=diffusion_time[0], random_seed=0, device=torch_device)\n",
    "test_sample = score_model.prior.normalize_theta(torch.tensor(test_sample), global_params=True).cpu().numpy()\n",
    "print(test_sample)"
   ],
   "id": "e9dff25c3741f7d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# posterior_sample_path = np.array([adaptive_sampling(score_model, x_valid, conditions=None, n_post_samples=1,\n",
    "#                                                                   e_rel=1.5, max_steps=1000, t_end=t, random_seed=0, device=torch_device)\n",
    "#                                   for t in diffusion_time[:-1]])\n",
    "posterior_sample_path = np.array([euler_maruyama_sampling(score_model, x_valid, diffusion_steps=1000,\n",
    "                                                           conditions=None, n_post_samples=1, t_end=t, random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path = score_model.prior.normalize_theta(torch.tensor(posterior_sample_path), global_params=True).cpu().numpy()\n",
    "\n",
    "posterior_sample_path2 = np.array([euler_maruyama_sampling(score_model, x_valid, diffusion_steps=200,\n",
    "                                                           pareto_smooth_fraction=0.1,\n",
    "                                                           conditions=None, n_post_samples=1, t_end=t, random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path2 = prior.normalize_theta(torch.tensor(posterior_sample_path2), global_params=True).cpu().numpy()\n",
    "\n",
    "posterior_sample_path3 = np.array([probability_ode_solving(score_model, x_valid,\n",
    "                                                           conditions=None, n_post_samples=1, t_end=t, random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path3 = score_model.prior.normalize_theta(torch.tensor(posterior_sample_path3), global_params=True).cpu().numpy()\n",
    "\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "e9053cd01f24ecf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define grid boundaries and resolution for your 2D space.\n",
    "x_min, x_max, y_min, y_max = -1.5, 1.5, -1.5, 1.5\n",
    "grid_res = 10  # Number of points per dimension\n",
    "\n",
    "# Create a meshgrid of points\n",
    "x_vals = np.linspace(x_min, x_max, grid_res)\n",
    "y_vals = np.linspace(y_min, y_max, grid_res)\n",
    "xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "# Stack into (N,2) where N = grid_res*grid_res\n",
    "grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Convert grid to a torch tensor and move to device\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32, device=torch_device)\n",
    "x_valid_norm_e = x_valid_norm.reshape(10, -1).to(torch_device)\n",
    "x_valid_norm_ext = x_valid_norm_e.unsqueeze(0).repeat(grid_tensor.shape[0], 1, 1)\n",
    "\n",
    "# Dictionary to hold score outputs for each time\n",
    "scores = {}\n",
    "scores_smoothed = {}\n",
    "drifts = {}\n",
    "drifts_smoothed = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Evaluate the score model for each time value\n",
    "    for t in diffusion_time:\n",
    "        # Create a tensor of time values for each grid point\n",
    "        t_tensor = torch.full((grid_tensor.shape[0], 1), t.item(), dtype=torch.float32, device=torch_device)\n",
    "        epsilon = torch.randn_like(grid_tensor, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "        # perturb theta\n",
    "        alpha, sigma = score_model.sde.kernel(t=t_tensor)\n",
    "        z = grid_tensor #alpha * grid_tensor + sigma * epsilon\n",
    "\n",
    "        # Evaluate the score model\n",
    "        score_indv = torch.zeros((x_valid_norm_ext.shape[2], grid_tensor.shape[0], 2))\n",
    "\n",
    "        prior_scores = (1 - t) * score_model.prior.score_global_batch(z)\n",
    "        prior_scores_indv = prior_scores.unsqueeze(0)\n",
    "        for i in range(x_valid_norm_ext.shape[2]):\n",
    "            score_indv[i] = score_model.forward_global(theta_global=z, time=t_tensor, x=x_valid_norm_ext[:, :, i].unsqueeze(-1),\n",
    "                                                       pred_score=True, clip_x=False)\n",
    "        score_indv = score_indv - prior_scores_indv\n",
    "\n",
    "        score = score_indv.sum(axis=0)\n",
    "        score = score + prior_scores\n",
    "        scores[t.item()] = score.cpu().numpy()\n",
    "\n",
    "        score_pareto = torch.zeros_like(score)\n",
    "        for i in range(score_indv.shape[1]):\n",
    "            score_pareto[i] = pareto_smooth_sum(score_indv[:, i].unsqueeze(0),\n",
    "                                                   tail_fraction=0.3)[0]  # expects dim to be the batch\n",
    "\n",
    "        score_pareto = score_pareto + prior_scores\n",
    "        scores_smoothed[t.item()] = score_pareto.cpu().numpy()\n",
    "\n",
    "        f, g = score_model.sde.get_f_g(x=z, t=t_tensor)\n",
    "        drift = f - 0.5 * torch.square(g) * scores[t.item()]\n",
    "        drifts[t.item()] = drift.cpu().numpy()\n",
    "\n",
    "        drift_pareto = f - 0.5 * torch.square(g) * scores_smoothed[t.item()]\n",
    "        drifts_smoothed[t.item()] = drift_pareto.cpu().numpy()"
   ],
   "id": "474671e4362d31cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(scores.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)  # negative since we are plotting the reverse score\n",
    "\n",
    "    ax = axes[i]\n",
    "\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    if i != 0:\n",
    "        j = i\n",
    "        h2, = ax.plot(posterior_sample_path[:-j, 0, 0], posterior_sample_path[:-j, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "        #h3, = ax.plot(posterior_sample_path2[j:, 0, 0], posterior_sample_path2[j:, 0, 1], 'o-', label='Posterior Path Euler', alpha=0.5)\n",
    "        h4, = ax.plot(posterior_sample_path3[:-j, 0, 0], posterior_sample_path3[:-j, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=5*n_grid*n_grid, alpha=.75, label='Score')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5, h0, h1, h2, h4], loc='lower center', ncols=5, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/score.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "8458bd11effe5a4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(drifts.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)\n",
    "\n",
    "    ax = axes[i]\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    j = len(diffusion_time)-i-1\n",
    "    h2, = ax.plot(posterior_sample_path[j:, 0, 0], posterior_sample_path[j:, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "    #h3, = ax.plot(posterior_sample_path2[j:, 0, 0], posterior_sample_path2[j:, 0, 1], 'o-', label='Posterior Path Euler', alpha=0.5)\n",
    "    h4, = ax.plot(posterior_sample_path3[j:, 0, 0], posterior_sample_path3[j:, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=5*n_grid*n_grid, alpha=.75,\n",
    "                   label='Probability Flow')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5, h0, h1, h2, h4], loc='lower center', ncols=5, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/drift.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "287ac0f1bf8a8ee7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(drifts_smoothed.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)\n",
    "\n",
    "    ax = axes[i]\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    j = len(diffusion_time)-i-1\n",
    "    h2, = ax.plot(posterior_sample_path[j:, 0, 0], posterior_sample_path[j:, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "    h3, = ax.plot(posterior_sample_path2[j:, 0, 0], posterior_sample_path2[j:, 0, 1], 'o-', label='Posterior Path Pareto-Euler', alpha=0.5)\n",
    "    h4, = ax.plot(posterior_sample_path3[j:, 0, 0], posterior_sample_path3[j:, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=5*n_grid*n_grid, alpha=.75,\n",
    "                   label='Probability Flow')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5, h0, h1, h2, h3, h4], loc='lower center', ncols=6, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/drift_pareto.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "c24c6fe6e48c5fc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ((t_val, score_val_smoothed), (t_val, score_val)) in enumerate(zip(sorted(drifts_smoothed.items(), reverse=True),\n",
    "                                           sorted(drifts.items(), reverse=True))):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = (score_val_smoothed[:, 0].reshape(grid_res, grid_res) - score_val[:, 0].reshape(grid_res, grid_res))\n",
    "    V = (score_val_smoothed[:, 1].reshape(grid_res, grid_res) - score_val[:, 1].reshape(grid_res, grid_res))\n",
    "\n",
    "    ax = axes[i]\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', #scale=10*n_grid*n_grid, alpha=.75,\n",
    "                   label='Difference in Probability Flow (Smoothed - Non-Smoothed)')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5], loc='lower center', ncols=6, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/drift_pareto_difference.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "id": "f3dc83c27b486ea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step Size for different Grid Sizes",
   "id": "be293168e0abdda5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check number of steps needed for different number of data points\n",
    "n_steps = {}\n",
    "n_steps_per_grid = {}\n",
    "n_steps_error = {}\n",
    "\n",
    "score_model.current_number_of_obs = 10 #max_number_of_obs\n",
    "valid_id = 0\n",
    "#grid_sizes = [4, 8, 12, 16, 32]\n",
    "grid_sizes = [10, 20, 30, 40, 100] #[5, 10, 15, 30, 70] # [1, 3, 4, 10, 32]\n",
    "if score_model.max_number_of_obs > 1:\n",
    "    sampling_types = [('normal', None, 0.), ('subsample', 10, 0.)] #, ('pareto', None, 0.3)]\n",
    "else:\n",
    "    sampling_types = [('normal', None, 0.), ('subsample', 10, 0.)] #, ('pareto', None, 0.3)]\n",
    "    grid_sizes = [1,2,3] + grid_sizes\n",
    "\n",
    "for sampling_type in sampling_types:\n",
    "    print(sampling_type)\n",
    "    n_steps[sampling_type[0]] = []\n",
    "    n_steps_per_grid[sampling_type[0]] = []\n",
    "    n_steps_error[sampling_type[0]] = []\n",
    "    for n in grid_sizes:\n",
    "        true_params, _, valid_data = generate_synthetic_data(prior, n_samples=10, grid_size=n,\n",
    "                                                   normalize=False, random_seed=0)\n",
    "        x_valid = valid_data[valid_id].to(torch_device)\n",
    "        test_sample, list_steps = adaptive_sampling(score_model, x_valid, conditions=None, n_post_samples=1,\n",
    "                                                    n_scores_update=sampling_type[1],\n",
    "                                                    pareto_smooth_fraction=sampling_type[2],\n",
    "                                    e_rel=1.5, max_steps=10000, t_end=0, random_seed=0, device=torch_device,\n",
    "                                                    return_steps=True)\n",
    "        #diffusion_steps = 1000#n**5\n",
    "        #test_sample = euler_maruyama_sampling(score_model, x_valid, conditions=None, n_post_samples=1, diffusion_steps=diffusion_steps,\n",
    "        #                                     t_end=0, random_seed=0, device=torch_device)\n",
    "        #list_steps = [1/diffusion_steps] * diffusion_steps\n",
    "\n",
    "        #plt.title(f'Number of data points: {n}x{n} Grid')\n",
    "        #plt.plot(list_steps)\n",
    "        #plt.show()\n",
    "        n_steps_per_grid[sampling_type[0]].append(list_steps)\n",
    "        n_steps[sampling_type[0]].append(len(list_steps))\n",
    "        n_steps_error[sampling_type[0]].append(np.mean((test_sample-true_params[valid_id].numpy())**2))"
   ],
   "id": "4936602cfa640068",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "for sampling_type in sampling_types:\n",
    "    plt.plot(grid_sizes, n_steps[sampling_type[0]], 'o-', label=sampling_type[0])\n",
    "\n",
    "if score_model.max_number_of_obs > 1:\n",
    "    plt.title(f'Number of Steps for Different Grid Sizes ({score_model.current_number_of_obs} Obs)')\n",
    "else:\n",
    "    plt.title('Number of Steps for Different Grid Sizes')\n",
    "#plt.plot(grid_sizes, n_steps, 'o-', label='#Steps')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Number of Steps')\n",
    "plt.yscale('log')\n",
    "plt.ylim(1, 1e4)\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/number_of_steps_vs_grid_size{score_model.current_number_of_obs}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "for sampling_type in sampling_types:\n",
    "    plt.plot(np.array(grid_sizes)**2/score_model.current_number_of_obs,\n",
    "             n_steps[sampling_type[0]], 'o-', label=sampling_type[0])\n",
    "\n",
    "if score_model.max_number_of_obs > 1:\n",
    "    plt.title(f'Number of Steps for Different Grid Sizes ({score_model.current_number_of_obs} Obs)')\n",
    "else:\n",
    "    plt.title('Number of Steps for Different Grid Sizes')\n",
    "#plt.plot(grid_sizes, n_steps, 'o-', label='#Steps')\n",
    "plt.xlabel('#Observations (divided by number of obs used to predict score)')\n",
    "plt.ylabel('Number of Steps')\n",
    "plt.yscale('log')\n",
    "plt.ylim(1, 1e4)\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/number_of_steps_vs_n_obs{score_model.current_number_of_obs}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "for sampling_type in sampling_types:\n",
    "    plt.plot(grid_sizes, n_steps_error[sampling_type[0]], 'o-', label=sampling_type[0])\n",
    "plt.title('Error for Different Grid Sizes')\n",
    "#plt.plot(grid_sizes, n_steps, 'o-', label='#Steps')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/error_vs_grid_size{score_model.current_number_of_obs}.png')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, len(sampling_types), sharey=True, figsize=(4*len(sampling_types), 4), tight_layout=True)\n",
    "for a, sampling_type in zip(ax, sampling_types):\n",
    "    a.set_title(sampling_type[0])\n",
    "    for i, n in zip(grid_sizes, n_steps_per_grid[sampling_type[0]]):\n",
    "        a.plot(n, label=f'{i}x{i}')\n",
    "    a.set_xlabel('Step')\n",
    "    a.set_ylabel('Step Size')\n",
    "    a.set_yscale('log')\n",
    "    a.legend(loc='lower right')\n",
    "    a.set_ylim(1e-6, 1)\n",
    "plt.savefig(f'plots/{score_model.name}/n_steps_per_grid{score_model.current_number_of_obs}.png')\n",
    "plt.show()"
   ],
   "id": "12aa82da545123ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "319131e047b1445b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
