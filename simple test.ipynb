{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:36.880807Z",
     "start_time": "2025-02-12T16:25:36.019271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:37.339417Z",
     "start_time": "2025-02-12T16:25:37.336048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# eight schools problem\n",
    "J = 8\n",
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])[:, np.newaxis]  # our groups, one observation per group\n",
    "sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])  # assumed to be known\n",
    "n_obs_per_group = 2  # sigma is known, so part of the observation"
   ],
   "id": "18833b2ff27d2d9f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:37.806416Z",
     "start_time": "2025-02-12T16:25:37.799909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simulator example\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        self.mu_mean = 0\n",
    "        self.mu_std = 10\n",
    "        self.log_tau_mean = 5\n",
    "        self.log_tau_std = 1\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample(batch_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mu = np.random.normal(loc=self.mu_mean, scale=self.mu_std, size=(batch_size,1))\n",
    "        log_tau = np.random.normal(loc=self.log_tau_mean, scale=self.log_tau_std, size=(batch_size,1))\n",
    "        theta_j = np.random.normal(loc=mu, scale=np.exp(log_tau), size=(batch_size, J))\n",
    "        return dict(mu=mu, log_tau=log_tau, theta_j=theta_j)\n",
    "\n",
    "    def log_score_global(self, theta):\n",
    "        \"\"\" Computes the global score for a single parameter set (mu, log_tau). \"\"\"\n",
    "        mu, log_tau = theta  # Assuming theta is a 1D tensor of shape (2,)\n",
    "        # Gradient w.r.t mu\n",
    "        grad_logp_mu = -(mu - self.mu_mean) / self.mu_std**2\n",
    "        # Gradient w.r.t log_tau\n",
    "        grad_logp_tau = -(log_tau - self.log_tau_mean) / self.log_tau_std**2\n",
    "        return torch.tensor([grad_logp_mu, grad_logp_tau], dtype=torch.float32)\n",
    "\n",
    "    def score_global_batch(self, theta_batch_norm):\n",
    "        \"\"\" Computes the global score for a batch of parameters. \"\"\"\n",
    "        theta_batch = theta_batch_norm * prior_std + prior_mean\n",
    "        score = torch.stack([self.log_score_global(theta) for theta in theta_batch])\n",
    "        # correct the score for the normalization\n",
    "        score = score / prior_std\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def log_score_local(theta):\n",
    "        \"\"\" Computes the local score for a single sample theta_j. \"\"\"\n",
    "        mu, log_tau, theta_j = theta[0], theta[1], theta[2:]\n",
    "        # Gradient w.r.t theta_j\n",
    "        grad_logp_theta_j = -(theta_j - mu) / np.exp(log_tau)**2\n",
    "        return torch.tensor([grad_logp_theta_j], dtype=torch.float32)\n",
    "\n",
    "    def score_local_batch(self, theta_batch_norm):\n",
    "        \"\"\" Computes the local score for a batch of samples. \"\"\"\n",
    "        theta_batch = theta_batch_norm * prior_std + prior_mean  # todo: differ between global and local prior\n",
    "        score = torch.stack([self.log_score_local(theta) for theta in theta_batch])\n",
    "        # correct the score for the normalization\n",
    "        score = score / prior_std\n",
    "        return score\n",
    "\n",
    "\n",
    "def simulator(params, school_i=None):\n",
    "    batch_size = params['mu'].shape[0]\n",
    "    y_j = np.random.normal(loc=params['theta_j'], scale=sigma, size=(batch_size, J))\n",
    "    if school_i is None:\n",
    "        return dict(observable=y_j, sigma=np.tile(sigma, (batch_size, 1)))\n",
    "    return dict(observable=y_j[:, school_i][:, np.newaxis], sigma=(np.ones(batch_size)*sigma[school_i])[:, np.newaxis])\n",
    "\n",
    "prior = Prior()\n",
    "n_params = 2"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:41.750546Z",
     "start_time": "2025-02-12T16:25:41.743731Z"
    }
   },
   "cell_type": "code",
   "source": "prior(2)",
   "id": "557005f1016c5ded",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu': array([[ -3.82224415],\n",
       "        [-10.0235505 ]]),\n",
       " 'log_tau': array([[6.14595316],\n",
       "        [5.34086745]]),\n",
       " 'theta_j': array([[-864.1243656 ,   55.77897497, -431.99502548, -232.78178142,\n",
       "          172.54683189,  108.3177034 ,  320.25161304,  932.4100146 ],\n",
       "        [ -50.15602267,  341.73692912, -107.45779436,   23.69100532,\n",
       "          105.58065034,   18.15640263,  -38.67894974, -200.81083325]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:41.998985Z",
     "start_time": "2025-02-12T16:25:41.992792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0)\n",
    "test_prior = prior(100)\n",
    "test = simulator(test_prior, school_i=None)\n",
    "x_mean = torch.tensor(np.array([np.mean(test['observable']), np.mean(test['sigma'])]), dtype=torch.float32)\n",
    "x_std = torch.tensor(np.array([np.std(test['observable']), np.std(test['sigma'])]), dtype=torch.float32)\n",
    "prior_mean = torch.tensor(np.array([np.mean(test_prior['mu']), np.mean(test_prior['log_tau'])]), dtype=torch.float32)\n",
    "prior_std = torch.tensor(np.array([np.std(test_prior['mu']), np.std(test_prior['log_tau'])]), dtype=torch.float32)\n",
    "prior_mean"
   ],
   "id": "c827f463acd40dc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5981, 5.0820])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:42.937641Z",
     "start_time": "2025-02-12T16:25:42.932638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def positional_encoding(t, d_model):\n",
    "    \"\"\"\n",
    "    Computes the sinusoidal positional encoding for a given time t.\n",
    "\n",
    "    Args:\n",
    "        t (torch.Tensor): The input time tensor of shape (batch_size, 1).\n",
    "        d_model (int): The dimensionality of the embedding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The positional encoding of shape (batch_size, d_model).\n",
    "    \"\"\"\n",
    "    half_dim = d_model // 2\n",
    "    div_term = torch.exp(torch.arange(half_dim, dtype=torch.float32) *\n",
    "                         -(math.log(10000.0) / (half_dim - 1)))\n",
    "    t_proj = t * div_term\n",
    "    pos_enc = torch.cat([torch.sin(t_proj), torch.cos(t_proj)], dim=-1)\n",
    "    return pos_enc\n",
    "\n",
    "# Define the Score Model based on F-NPSE\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.norm = nn.LayerNorm(out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.proj(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.activation(out)\n",
    "        return out + identity\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network model that computes score estimates.\n",
    "\n",
    "        Args:\n",
    "            input_dim_theta (int): Input dimension for theta.\n",
    "            hidden_dim_theta (int): Hidden dimension for theta network.\n",
    "            input_dim_x (int): Input dimension for x.\n",
    "            hidden_dim_x (int): Hidden dimension for x network.\n",
    "            hidden_dim_emb (int): Hidden dimension for the embedding network.\n",
    "            time_embed_dim (int, optional): Dimension of time embedding. Defaults to 4.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim_theta, hidden_dim_theta,\n",
    "                 input_dim_x, hidden_dim_x,\n",
    "                 hidden_dim_emb, time_embed_dim=4):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.net_theta = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.net_x = nn.Sequential(\n",
    "            ResidualBlock(input_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.net_emb = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta*2 + time_embed_dim, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, input_dim_theta)\n",
    "        )\n",
    "\n",
    "    def forward(self, theta, t, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ScoreModel.\n",
    "\n",
    "        Args:\n",
    "            theta (torch.Tensor): Input theta tensor of shape (batch_size, input_dim_theta).\n",
    "            t (torch.Tensor): Input time tensor of shape (batch_size, 1).\n",
    "            x (torch.Tensor): Input x tensor of shape (batch_size, input_dim_x).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the score model.\n",
    "        \"\"\"\n",
    "        theta_emb = self.net_theta(theta)\n",
    "        x_emb = self.net_x(x)\n",
    "        t_emb = positional_encoding(t, self.time_embed_dim)\n",
    "        return self.net_emb(torch.cat([theta_emb, x_emb, t_emb], dim=-1))"
   ],
   "id": "6d13bcb1279b6f74",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:44.796425Z",
     "start_time": "2025-02-12T16:25:44.790308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_synthetic_data(n_samples, schools_joint=False):\n",
    "    thetas = []\n",
    "    xs = []\n",
    "    for i in range(n_samples):\n",
    "        batch_params = prior(1)\n",
    "        if schools_joint:\n",
    "            sim_batch = simulator(batch_params)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1), dtype=torch.float32)\n",
    "            x = torch.tensor(np.stack((sim_batch['observable'], sim_batch['sigma']), axis=-1), dtype=torch.float32)\n",
    "        else:\n",
    "            sim_batch = simulator(batch_params, school_i=i % J)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1), dtype=torch.float32)\n",
    "            x = torch.tensor(np.concatenate((sim_batch['observable'], sim_batch['sigma']), axis=-1), dtype=torch.float32)\n",
    "\n",
    "        thetas.append(theta)\n",
    "        xs.append(x)\n",
    "    thetas = torch.concatenate(thetas)\n",
    "    xs = torch.concatenate(xs)\n",
    "    return thetas, xs\n",
    "\n",
    "\n",
    "# Gaussian kernel for log pdf and sampling\n",
    "def gaussian_kernel_score(theta, theta_prime, gamma):\n",
    "    return -(theta - torch.sqrt(gamma) * theta_prime) / (1 - gamma)\n",
    "\n",
    "\n",
    "def gaussian_kernel_sample(theta_prime, gamma):\n",
    "    noise = torch.randn_like(theta_prime) * torch.sqrt(1 - gamma)\n",
    "    return torch.sqrt(gamma) * theta_prime + noise\n",
    "\n",
    "# Generate diffusion time and step size\n",
    "def generate_diffusion_time(T):\n",
    "    time = np.linspace(0, 1, T+1)[1:-1]  # exclude 0 and T\n",
    "    gamma = time[::-1].copy()  # first index is close to 1\n",
    "\n",
    "    alpha_t = np.concatenate(([gamma[0]], gamma[1:] / gamma[:-1]))\n",
    "    delta_t = 0.3 * (1-alpha_t) / np.sqrt(alpha_t)  # weighting for loss, and later used as step size in Langevin dynamics\n",
    "    return time, gamma, delta_t"
   ],
   "id": "b506683f73d8dd26",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T16:25:45.967969Z",
     "start_time": "2025-02-12T16:25:45.961739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, T, epochs=100, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Add Cosine Annealing Scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(T)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    diffusion_time = torch.tensor(diffusion_time, dtype=torch.float32)\n",
    "    gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for theta_prime_batch, x_batch in dataloader:\n",
    "            loss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            # for each sample in the batch, calculate the loss for each diffusion time\n",
    "            for g, t, w in zip(gamma, diffusion_time, delta_t):\n",
    "                # sample from the Gaussian kernel\n",
    "                theta_batch = gaussian_kernel_sample(theta_prime_batch, g)\n",
    "                # calculate the score for the sampled theta\n",
    "                score_pred = model(theta=theta_batch, t=torch.ones((theta_batch.shape[0], 1))*t, x=x_batch)\n",
    "                # calculate the reference score for the sampled theta\n",
    "                score_ref = gaussian_kernel_score(theta_batch, theta_prime_batch, g)\n",
    "                # calculate the loss\n",
    "                loss += w * criterion(score_pred, score_ref)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
   ],
   "id": "3e4488af88ad4738",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-12T16:26:02.569329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "n_samples = 10000\n",
    "batch_size = 128\n",
    "T = 100\n",
    "\n",
    "score_model = ScoreModel(\n",
    "    input_dim_theta=n_params, hidden_dim_theta=32,\n",
    "    input_dim_x=n_obs_per_group, hidden_dim_x=32,\n",
    "    hidden_dim_emb=16\n",
    ")\n",
    "\n",
    "# Create model and dataset\n",
    "thetas, xs = generate_synthetic_data(n_samples)\n",
    "# Normalize data\n",
    "thetas = (thetas - prior_mean) / prior_std\n",
    "xs = (xs - x_mean) / x_std\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(thetas, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "train_score_model(score_model, dataloader, T=T, epochs=100, lr=1e-3)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.4488\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Annealed Langevin Dynamics for Sampling\n",
    "def langevin_sampling(model, x_obs, n_post_samples, steps=5):\n",
    "    x_obs_norm = (x_obs - x_mean) / x_std  # assumes x_obs is not standardized\n",
    "\n",
    "    # Initialize parameters\n",
    "    n_obs = x_obs_norm.shape[0]\n",
    "    theta = torch.randn(n_post_samples, n_params) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32))\n",
    "\n",
    "    # Generate diffusion time parameters\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(T)\n",
    "\n",
    "    # Ensure x_obs_norm is a PyTorch tensor\n",
    "    if not isinstance(x_obs_norm, torch.Tensor):\n",
    "        x_obs_norm = torch.tensor(x_obs_norm, dtype=torch.float32)\n",
    "\n",
    "    # Expand x_obs_norm to match the number of posterior samples\n",
    "    x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, -1, -1)  # Shape: (n_post_samples, n_obs, d)\n",
    "    x_expanded = x_exp.reshape(-1, x_obs_norm.shape[-1])\n",
    "\n",
    "    # Reverse iterate over diffusion times and step sizes\n",
    "    for step_size, t in zip(delta_t[::-1], diffusion_time[::-1]):\n",
    "        # Create tensor for current time step\n",
    "        t_tensor = torch.full((n_post_samples, 1), t, dtype=torch.float32)\n",
    "        t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Sample Gaussian noise\n",
    "            eps = torch.randn_like(theta)\n",
    "\n",
    "            # Compute prior score\n",
    "            prior_score = prior.score_global_batch(theta)\n",
    "\n",
    "            # Compute model scores\n",
    "            theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, n_params)\n",
    "            model_scores = model(theta_exp, t=t_exp, x=x_expanded)\n",
    "            model_scores = model_scores.reshape(n_post_samples, n_obs, -1).sum(dim=1)\n",
    "\n",
    "            # Compute updated scores and perform Langevin step\n",
    "            scores = (1 - n_obs) * (1 - t) * prior_score + model_scores\n",
    "            theta = theta + (step_size / 2) * scores + np.sqrt(step_size) * eps\n",
    "     # correct for normalization\n",
    "    theta = theta * prior_std + prior_mean\n",
    "    # convert to numpy\n",
    "    theta = theta.detach().numpy()\n",
    "    return theta"
   ],
   "id": "bd3241835d991fc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from bayesflow import diagnostics",
   "id": "78c843703cb70e83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_prior, valid_data = generate_synthetic_data(1000, schools_joint=True)",
   "id": "eafc62be726c8506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=100) for vd in valid_data])",
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_samples_valid, np.array(valid_prior), param_names=[r'$\\mu$', r'$\\log \\tau$']);",
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_samples_valid, np.array(valid_prior),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$']);"
   ],
   "id": "c12c415e037a9277",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Apply on Data",
   "id": "b4a8277f98f6eae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate posterior samples\n",
    "test_data = torch.tensor(np.concatenate((y, sigma[:, np.newaxis]), axis=-1), dtype=torch.float32)\n",
    "posterior_samples = langevin_sampling(score_model, test_data, n_post_samples=valid_prior.shape[0])\n",
    "print(\"Sampled posterior parameters:\", posterior_samples)"
   ],
   "id": "36e95b8ccefc31f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_posterior_2d(posterior_samples, prior_draws=valid_prior, param_names=[r'$\\mu$', r'$\\log \\tau$']);",
   "id": "d497c90b97f71a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_samples.mean(axis=0)",
   "id": "8b93f9d3e79792a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stan inference Results\n",
    "$\\mu = 5.836806$\n",
    "\n",
    "$\\log\\tau = 2.450053$\n",
    "\n",
    "$\\theta =\n",
    "[ 0.64940756,  0.09001582, -0.23279844,  0.04471902, -0.33542507, -0.2041105,  0.53249937,  0.14456798]$"
   ],
   "id": "9a13cf5a9239b208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "17edbe3260ac3ea0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
