{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:30:29.734523Z",
     "start_time": "2025-02-12T15:30:28.706526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:30:29.741342Z",
     "start_time": "2025-02-12T15:30:29.739259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# eight schools problem\n",
    "J = 8\n",
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])[:, np.newaxis]  # our groups, one observation per group\n",
    "sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])  # assumed to be known\n",
    "n_obs_per_group = 2  # sigma is known, so part of the observation"
   ],
   "id": "18833b2ff27d2d9f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:30:29.844332Z",
     "start_time": "2025-02-12T15:30:29.839373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simulator example\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        self.mu_mean = 0\n",
    "        self.mu_std = 10\n",
    "        self.log_tau_mean = 5\n",
    "        self.log_tau_std = 1\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample(batch_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mu = np.random.normal(loc=self.mu_mean, scale=self.mu_std, size=(batch_size,1))\n",
    "        log_tau = np.random.normal(loc=self.log_tau_mean, scale=self.log_tau_std, size=(batch_size,1))\n",
    "        theta_j = np.random.normal(loc=mu, scale=np.exp(log_tau), size=(batch_size, J))\n",
    "        return dict(mu=mu, log_tau=log_tau, theta_j=theta_j)\n",
    "\n",
    "    def log_score_global(self, theta):\n",
    "        \"\"\" Computes the global score for a single parameter set (mu, log_tau). \"\"\"\n",
    "        mu, log_tau = theta  # Assuming theta is a 1D tensor of shape (2,)\n",
    "        # Gradient w.r.t mu\n",
    "        grad_logp_mu = -(mu - self.mu_mean) / self.mu_std**2\n",
    "        # Gradient w.r.t log_tau\n",
    "        grad_logp_tau = -(log_tau - self.log_tau_mean) / self.log_tau_std**2\n",
    "\n",
    "        return torch.tensor([grad_logp_mu, grad_logp_tau], dtype=torch.float32)\n",
    "\n",
    "    def score_global_batch(self, theta_batch):\n",
    "        \"\"\" Computes the global score for a batch of parameters. \"\"\"\n",
    "        return torch.stack([self.log_score_global(theta) for theta in theta_batch])\n",
    "\n",
    "    @staticmethod\n",
    "    def log_score_local(theta):\n",
    "        \"\"\" Computes the local score for a single sample theta_j. \"\"\"\n",
    "        mu, log_tau, theta_j = theta[0], theta[1], theta[2:]\n",
    "        # Gradient w.r.t theta_j\n",
    "        grad_logp_theta_j = -(theta_j - mu) / np.exp(log_tau)**2\n",
    "        return torch.tensor([grad_logp_theta_j], dtype=torch.float32)\n",
    "\n",
    "    def score_local_batch(self, theta_batch):\n",
    "        \"\"\" Computes the local score for a batch of samples. \"\"\"\n",
    "        return torch.stack([self.log_score_local(theta) for theta in theta_batch])\n",
    "\n",
    "\n",
    "def simulator(params, school_i=None):\n",
    "    batch_size = params['mu'].shape[0]\n",
    "    y_j = np.random.normal(loc=params['theta_j'], scale=sigma, size=(batch_size, J))\n",
    "    if school_i is None:\n",
    "        return dict(observable=y_j, sigma=np.tile(sigma, (batch_size, 1)))\n",
    "    return dict(observable=y_j[:, school_i][:, np.newaxis], sigma=(np.ones(batch_size)*sigma[school_i])[:, np.newaxis])\n",
    "\n",
    "prior = Prior()\n",
    "n_params = 2"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:30:31.508397Z",
     "start_time": "2025-02-12T15:30:31.502318Z"
    }
   },
   "cell_type": "code",
   "source": "prior(2)",
   "id": "557005f1016c5ded",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu': array([[ 17.73374141],\n",
       "        [-11.69914719]]),\n",
       " 'log_tau': array([[6.14189388],\n",
       "        [3.32845105]]),\n",
       " 'theta_j': array([[-343.78364777, -240.35043144, -147.70232476,  -88.72958342,\n",
       "         -476.47217915, -310.55506431, -217.53907339,  695.7704519 ],\n",
       "        [ -11.89013417,    3.62698016,  -15.28757231,  -38.68051892,\n",
       "          -27.38451374,   18.97765665,  -33.15087166,  -14.43514689]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:30:32.605433Z",
     "start_time": "2025-02-12T15:30:32.601686Z"
    }
   },
   "cell_type": "code",
   "source": "simulator(prior(2), school_i=None)",
   "id": "c827f463acd40dc7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observable': array([[ -923.87378975,  -413.6135652 ,  -555.35685577,  1330.28983981,\n",
       "          1016.50080995,  -406.26270439, -1115.9590014 ,   928.0417485 ],\n",
       "        [   89.37077683,    36.74712177,  -214.62680435,   104.55501516,\n",
       "            57.245894  ,   -18.87897402,   -13.36876791,  -231.88165267]]),\n",
       " 'sigma': array([[15, 10, 16, 11,  9, 11, 10, 18],\n",
       "        [15, 10, 16, 11,  9, 11, 10, 18]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:30:34.669910Z",
     "start_time": "2025-02-12T15:30:34.664116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the Score Model based on F-NPSE\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.norm = nn.LayerNorm(out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.proj(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.activation(out)\n",
    "        return out + identity\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, input_dim_theta, hidden_dim_theta,\n",
    "                 input_dim_x, hidden_dim_x,\n",
    "                 hidden_dim_emb):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.net_theta = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.net_x = nn.Sequential(\n",
    "            ResidualBlock(input_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.net_emb = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta*2+1, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, input_dim_theta)\n",
    "        )\n",
    "\n",
    "    def forward(self, theta, t, x):\n",
    "        theta_emb = self.net_theta(theta)\n",
    "        x_emb = self.net_x(x)\n",
    "        t_emb = t # todo: add positional encoding\n",
    "        return self.net_emb(torch.cat([theta_emb, x_emb, t_emb], axis=-1))"
   ],
   "id": "6d13bcb1279b6f74",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:30:35.777281Z",
     "start_time": "2025-02-12T15:30:35.771237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_synthetic_data(n_samples, schools_joint=False):\n",
    "    thetas = []\n",
    "    xs = []\n",
    "    for i in range(n_samples):\n",
    "        batch_params = prior(1)\n",
    "        if schools_joint:\n",
    "            sim_batch = simulator(batch_params)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1), dtype=torch.float32)\n",
    "            x = torch.tensor(np.stack((sim_batch['observable'], sim_batch['sigma']), axis=-1), dtype=torch.float32)\n",
    "        else:\n",
    "            sim_batch = simulator(batch_params, school_i=i % J)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1), dtype=torch.float32)\n",
    "            x = torch.tensor(np.concatenate((sim_batch['observable'], sim_batch['sigma']), axis=-1), dtype=torch.float32)\n",
    "\n",
    "        thetas.append(theta)\n",
    "        xs.append(x)\n",
    "    thetas = torch.concatenate(thetas)\n",
    "    xs = torch.concatenate(xs)\n",
    "    return thetas, xs\n",
    "\n",
    "\n",
    "# Gaussian kernel for log pdf and sampling\n",
    "def gaussian_kernel_score(theta, theta_prime, gamma):\n",
    "    return -(theta - torch.sqrt(gamma) * theta_prime) / (1 - gamma)\n",
    "\n",
    "\n",
    "def gaussian_kernel_sample(theta_prime, gamma):\n",
    "    noise = torch.randn_like(theta_prime) * torch.sqrt(1 - gamma)\n",
    "    return torch.sqrt(gamma) * theta_prime + noise\n",
    "\n",
    "# Generate diffusion time and step size\n",
    "def generate_diffusion_time(T):\n",
    "    time = np.linspace(0, 1, T+1)[1:-1]  # exclude 0 and T\n",
    "    gamma = time[::-1].copy()  # first index is close to 1\n",
    "\n",
    "    alpha_t = np.concatenate(([gamma[0]], gamma[1:] / gamma[:-1]))\n",
    "    delta_t = 0.3 * (1-alpha_t) / np.sqrt(alpha_t)  # weighting for loss, and later used as step size in Langevin dynamics\n",
    "    return time, gamma, delta_t"
   ],
   "id": "b506683f73d8dd26",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-12T15:30:37.044407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Weighted MSE loss\n",
    "def weighted_mse_loss(pred, target, weights):\n",
    "    return torch.sum(weights * (pred - target)**2) / torch.sum(weights)\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, T, epochs=100, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(T)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    diffusion_time = torch.tensor(diffusion_time, dtype=torch.float32)\n",
    "    gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for theta_prime_batch, x_batch in dataloader:\n",
    "            loss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            # for each sample in the batch, calculate the loss for each diffusion time\n",
    "            for g, t in zip(gamma, diffusion_time):\n",
    "                # sample from the Gaussian kernel\n",
    "                theta_batch = gaussian_kernel_sample(theta_prime_batch, g)\n",
    "                # calculate the score for the sampled theta\n",
    "                score_pred = model(theta=theta_batch, t=torch.ones((theta_batch.shape[0], 1))*t, x=x_batch)\n",
    "                # calculate the reference score for the sampled theta\n",
    "                score_ref = gaussian_kernel_score(theta_batch, theta_prime_batch, g)\n",
    "                # calculate the loss\n",
    "                loss += criterion(score_pred, score_ref)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Hyperparameters\n",
    "n_samples = 1000\n",
    "batch_size = 128\n",
    "T = 100\n",
    "\n",
    "score_model = ScoreModel(\n",
    "    input_dim_theta=n_params, hidden_dim_theta=16,\n",
    "    input_dim_x=n_obs_per_group, hidden_dim_x=16,\n",
    "    hidden_dim_emb=16\n",
    ")\n",
    "\n",
    "# Create model and dataset\n",
    "thetas, xs = generate_synthetic_data(n_samples)\n",
    "dataset = TensorDataset(thetas, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "train_score_model(score_model, dataloader, T=T, epochs=100, lr=1e-3)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 514.8203\n",
      "Epoch 20/100, Loss: 501.5399\n",
      "Epoch 30/100, Loss: 500.4289\n",
      "Epoch 40/100, Loss: 492.0512\n",
      "Epoch 50/100, Loss: 485.9502\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Annealed Langevin Dynamics for Sampling\n",
    "def langevin_sampling(model, x_obs, n_post_samples, steps=5):\n",
    "    # Initialize parameters\n",
    "    n_obs = x_obs.shape[0]\n",
    "    theta = torch.randn(n_post_samples, n_params) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32))\n",
    "\n",
    "    # Generate diffusion time parameters\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(T)\n",
    "\n",
    "    # Ensure x_obs is a PyTorch tensor\n",
    "    if not isinstance(x_obs, torch.Tensor):\n",
    "        x_obs = torch.tensor(x_obs, dtype=torch.float32)\n",
    "\n",
    "    # Expand x_obs to match the number of posterior samples\n",
    "    x_exp = x_obs.unsqueeze(0).expand(n_post_samples, -1, -1)  # Shape: (n_post_samples, n_obs, d)\n",
    "    x_expanded = x_exp.reshape(-1, x_obs.shape[-1])\n",
    "\n",
    "    # Reverse iterate over diffusion times and step sizes\n",
    "    for step_size, t in zip(delta_t[::-1], diffusion_time[::-1]):\n",
    "        # Create tensor for current time step\n",
    "        t_tensor = torch.full((n_post_samples, 1), t, dtype=torch.float32)\n",
    "        t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Sample Gaussian noise\n",
    "            eps = torch.randn_like(theta)\n",
    "\n",
    "            # Compute prior score\n",
    "            prior_score = prior.score_batch(theta)\n",
    "\n",
    "            # Compute model scores\n",
    "            theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, n_params)\n",
    "            model_scores = model(theta_exp, t=t_exp, x=x_expanded)\n",
    "            model_scores = model_scores.reshape(n_post_samples, n_obs, -1).sum(dim=1)\n",
    "\n",
    "            # Compute updated scores and perform Langevin step\n",
    "            scores = (1 - n_obs) * (1 - t) * prior_score + model_scores\n",
    "            theta = theta + (step_size / 2) * scores + np.sqrt(step_size) * eps\n",
    "\n",
    "    return theta.detach().numpy()"
   ],
   "id": "bd3241835d991fc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from bayesflow import diagnostics",
   "id": "78c843703cb70e83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_prior, valid_data = generate_synthetic_data(10, schools_joint=True)",
   "id": "eafc62be726c8506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=100) for vd in valid_data])",
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_samples_valid, np.array(valid_prior), param_names=['mu', 'log_tau']);",
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_sbc_ecdf(posterior_samples_valid, np.array(valid_prior), difference=True, param_names=['mu', 'log_tau']);",
   "id": "c12c415e037a9277",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Apply on Data",
   "id": "b4a8277f98f6eae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate posterior samples\n",
    "test_data = torch.tensor(np.concatenate((y, sigma[:, np.newaxis]), axis=-1), dtype=torch.float32)\n",
    "theta_samples = langevin_sampling(score_model, test_data, n_post_samples=100)\n",
    "print(\"Sampled posterior parameters:\", theta_samples)"
   ],
   "id": "36e95b8ccefc31f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d497c90b97f71a68",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
