{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# eight schools problem\n",
    "J = 8\n",
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])[:, np.newaxis]  # our groups, one observation per group\n",
    "sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])  # assumed to be known\n",
    "n_obs_per_group = 2  # sigma is known, so part of the observation"
   ],
   "id": "18833b2ff27d2d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulator example\n",
    "class Prior:\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample(batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(batch_size):\n",
    "        log_mu = np.random.normal(loc=0, scale=1, size=(batch_size,1))\n",
    "        log_tau = np.random.normal(loc=0, scale=1, size=(batch_size,1))\n",
    "        return dict(log_mu=log_mu, log_tau=log_tau)\n",
    "\n",
    "    @staticmethod\n",
    "    def log_score(theta):\n",
    "        log_mu = theta['log_mu']\n",
    "        log_tau = theta['log_tau']\n",
    "        grad_logp_mu = -log_mu\n",
    "        grad_logp_tau = -log_tau\n",
    "        return np.array([grad_logp_mu, grad_logp_tau])\n",
    "\n",
    "    def score_batch(self, theta_batch):\n",
    "        return np.concatenate([[self.log_score(dict(log_mu=theta[0], log_tau=theta[1]))]\n",
    "                               for theta in theta_batch], axis=0)\n",
    "\n",
    "def simulator(params, school_i=None):\n",
    "    batch_size = params['log_mu'].shape[0]\n",
    "    theta_j = np.random.normal(loc=np.exp(params['log_mu']), scale=np.exp(params['log_tau']), size=(batch_size, J))\n",
    "    y_j = np.random.normal(loc=theta_j, scale=sigma, size=(batch_size, J))\n",
    "    if school_i is None:\n",
    "        return dict(observable=y_j, sigma=np.tile(sigma, (batch_size, 1)))\n",
    "    return dict(observable=y_j[:, school_i][:, np.newaxis], sigma=(np.ones(batch_size)*sigma[school_i])[:, np.newaxis])\n",
    "\n",
    "prior = Prior()\n",
    "n_params = len(prior(2).keys())"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prior(2)",
   "id": "557005f1016c5ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "simulator(prior(2), school_i=None)",
   "id": "c827f463acd40dc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the Score Model based on F-NPSE\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.norm = nn.LayerNorm(out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.proj(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.activation(out)\n",
    "        return out + identity\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, input_dim_theta, hidden_dim_theta,\n",
    "                 input_dim_x, hidden_dim_x,\n",
    "                 hidden_dim_emb):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.net_theta = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.net_x = nn.Sequential(\n",
    "            ResidualBlock(input_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.net_emb = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta*2+1, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, input_dim_theta)\n",
    "        )\n",
    "\n",
    "    def forward(self, theta, t, x):\n",
    "        theta_emb = self.net_theta(theta)\n",
    "        x_emb = self.net_x(x)\n",
    "        t_emb = t # todo: add positional encoding\n",
    "        return self.net_emb(torch.cat([theta_emb, x_emb, t_emb], axis=-1))"
   ],
   "id": "6d13bcb1279b6f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_synthetic_data(n_samples, schools_joint=False):\n",
    "    thetas = []\n",
    "    xs = []\n",
    "    for i in range(n_samples):\n",
    "        batch_params = prior(1)\n",
    "        if schools_joint:\n",
    "            sim_batch = simulator(batch_params)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['log_mu'], batch_params['log_tau']], axis=-1), dtype=torch.float32)\n",
    "            x = torch.tensor(np.stack((sim_batch['observable'], sim_batch['sigma']), axis=-1), dtype=torch.float32)\n",
    "        else:\n",
    "            sim_batch = simulator(batch_params, school_i=i % J)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['log_mu'], batch_params['log_tau']], axis=-1), dtype=torch.float32)\n",
    "            x = torch.tensor(np.concatenate((sim_batch['observable'], sim_batch['sigma']), axis=-1), dtype=torch.float32)\n",
    "\n",
    "        thetas.append(theta)\n",
    "        xs.append(x)\n",
    "    thetas = torch.concatenate(thetas)\n",
    "    xs = torch.concatenate(xs)\n",
    "    return thetas, xs\n",
    "\n",
    "\n",
    "# Gaussian kernel for log pdf and sampling\n",
    "def gaussian_kernel_log_pdf(theta, theta_prime, gamma):\n",
    "    d = theta.size(1)\n",
    "    mean = torch.sqrt(gamma) * theta_prime\n",
    "    diff = theta - mean\n",
    "    exponent = -0.5 * diff**2 / (1 - gamma)\n",
    "    norm = (2 * math.pi * (1 - gamma)) ** (d / 2)\n",
    "    return exponent - torch.log(norm)\n",
    "\n",
    "def gaussian_kernel_sample(theta_prime, gamma):\n",
    "    noise = torch.randn_like(theta_prime) * torch.sqrt(1 - gamma)\n",
    "    return torch.sqrt(gamma) * theta_prime + noise\n",
    "\n",
    "# Generate diffusion time and step size\n",
    "def generate_diffusion_time(T):\n",
    "    time = np.linspace(0, 1, T+1)[1:-1]  # exclude 0 and T\n",
    "    gamma = time[::-1].copy()  # first index is close to 1\n",
    "\n",
    "    alpha_t = np.concatenate(([gamma[0]], gamma[1:] / gamma[:-1]))\n",
    "    delta_t = 0.3 * (1-alpha_t) / np.sqrt(alpha_t)  # weighting for loss, and later used as step size in Langevin dynamics\n",
    "    return time, gamma, delta_t"
   ],
   "id": "b506683f73d8dd26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Weighted MSE loss\n",
    "def weighted_mse_loss(pred, target, weights):\n",
    "    return torch.sum(weights * (pred - target)**2) / torch.sum(weights)\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, T, epochs=100, lr=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(T)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    diffusion_time = torch.tensor(diffusion_time, dtype=torch.float32)\n",
    "    gamma = torch.tensor(gamma, dtype=torch.float32)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for theta_prime_batch, x_batch in dataloader:\n",
    "            loss = 0.0\n",
    "            # for each sample in the batch, calculate the loss for each diffusion time\n",
    "            for g, t in zip(gamma, diffusion_time):\n",
    "                optimizer.zero_grad()\n",
    "                # sample from the Gaussian kernel\n",
    "                theta_batch = gaussian_kernel_sample(theta_prime_batch, g)\n",
    "                # calculate the score for the sampled theta\n",
    "                score_pred = model(theta=theta_batch, t=torch.ones((theta_batch.shape[0], 1))*t, x=x_batch)\n",
    "                # calculate the reference score for the sampled theta\n",
    "                score_ref = gaussian_kernel_log_pdf(theta_batch, theta_prime_batch, t)\n",
    "                # calculate the loss\n",
    "                loss += criterion(score_pred, score_ref)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Hyperparameters\n",
    "n_samples = 1000\n",
    "batch_size = 128\n",
    "T = 100\n",
    "\n",
    "score_model = ScoreModel(\n",
    "    input_dim_theta=n_params, hidden_dim_theta=16,\n",
    "    input_dim_x=n_obs_per_group, hidden_dim_x=16,\n",
    "    hidden_dim_emb=16\n",
    ")\n",
    "\n",
    "# Create model and dataset\n",
    "thetas, xs = generate_synthetic_data(n_samples)\n",
    "dataset = TensorDataset(thetas, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "train_score_model(score_model, dataloader, T=T, epochs=100, lr=1e-3)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Annealed Langevin Dynamics for Sampling\n",
    "def langevin_sampling(model, x_obs, n_post_samples, steps=5):\n",
    "    n = x_obs.shape[0]\n",
    "    theta = np.random.normal(loc=0, scale=np.ones(n_params)/np.sqrt(n), size=(n_post_samples, n_params))\n",
    "    theta_torch = torch.tensor(theta, dtype=torch.float32)\n",
    "\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(T)\n",
    "    for step_size, t in zip(delta_t[::-1], diffusion_time[::-1]):  # reverse order\n",
    "        for step in range(steps):\n",
    "            # sample noise\n",
    "            eps = torch.tensor(np.random.normal(loc=0, scale=np.ones(n_params), size=(n_post_samples, n_params)),\n",
    "                               dtype=torch.float32)\n",
    "            t_tensor = torch.ones((n_post_samples, 1)) * t\n",
    "            # calculate the prior score\n",
    "            scores = torch.tensor((1-n)*(1-t) * prior.score_batch(theta), dtype=torch.float32)\n",
    "            for x in x_obs:\n",
    "                # concat x as often as n_post_samples\n",
    "                x_tensor = torch.ones((n_post_samples, 1)) * x\n",
    "                # calculate the model score for each observation at the current theta and diffusion time\n",
    "                scores += model(theta_torch, t=t_tensor, x=x_tensor)\n",
    "            # update theta using Langevin step\n",
    "            theta_torch = theta_torch + step_size/2 * scores + np.sqrt(step_size) * eps\n",
    "    return theta_torch.detach().numpy()"
   ],
   "id": "43b5b27082013562",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate posterior samples\n",
    "test_data = torch.tensor(np.concatenate((y, sigma[:, np.newaxis]), axis=-1), dtype=torch.float32)\n",
    "theta_samples = langevin_sampling(score_model, test_data, n_post_samples=5)\n",
    "print(\"Sampled posterior parameters:\", theta_samples)"
   ],
   "id": "e4d7095ec0783786",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_data.shape",
   "id": "afa280ca8e0dc778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_prior, valid_data = generate_synthetic_data(10, schools_joint=True)\n",
    "posterior_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=5) for vd in valid_data])"
   ],
   "id": "78c843703cb70e83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_samples_valid",
   "id": "12b2bd065572969d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from bayesflow import diagnostics",
   "id": "75362f1f4eafbc73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_samples_valid, np.array(valid_prior), param_names=['mu', 'log_tau']);",
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_sbc_ecdf(posterior_samples_valid, np.array(valid_prior), difference=True, param_names=['mu', 'log_tau']);",
   "id": "c12c415e037a9277",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "36e95b8ccefc31f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
