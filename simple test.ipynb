{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:50.741318Z",
     "start_time": "2025-02-13T00:27:49.792637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:50.747204Z",
     "start_time": "2025-02-13T00:27:50.744248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mps_device = torch.device(\"cpu\")\n",
    "mps_device"
   ],
   "id": "70c931844e56c698",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:50.809417Z",
     "start_time": "2025-02-13T00:27:50.807209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# eight schools problem\n",
    "J = 8\n",
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])[:, np.newaxis]  # our groups, one observation per group\n",
    "sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])  # assumed to be known\n",
    "n_obs_per_group = 2  # sigma is known, so part of the observation"
   ],
   "id": "18833b2ff27d2d9f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:50.828871Z",
     "start_time": "2025-02-13T00:27:50.821389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simulator example\n",
    "def simulator(params, school_i=None):\n",
    "    batch_size = params['theta_j'].shape[0]\n",
    "    y_j = np.random.normal(loc=params['theta_j'], scale=sigma, size=(batch_size, J))\n",
    "    if school_i is None:\n",
    "        return dict(observable=y_j, sigma=np.tile(sigma, (batch_size, 1)))\n",
    "    return dict(observable=y_j[:, school_i][:, np.newaxis], sigma=(np.ones(batch_size)*sigma[school_i])[:, np.newaxis])\n",
    "\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        self.mu_mean = 0\n",
    "        self.mu_std = 10\n",
    "        self.log_tau_mean = 5\n",
    "        self.log_tau_std = 1\n",
    "\n",
    "        np.random.seed(0)\n",
    "        test_prior = self.sample(1000)\n",
    "        test = simulator(test_prior, school_i=None)\n",
    "        self.x_mean = torch.tensor(np.array([np.mean(test['observable']), np.mean(test['sigma'])]),\n",
    "                                   dtype=torch.float32, device=mps_device)\n",
    "        self.x_std = torch.tensor(np.array([np.std(test['observable']), np.std(test['sigma'])]),\n",
    "                                  dtype=torch.float32, device=mps_device)\n",
    "        self.prior_mean = torch.tensor(np.array([np.mean(test_prior['mu']), np.mean(test_prior['log_tau'])]),\n",
    "                                       dtype=torch.float32, device=mps_device)\n",
    "        self.prior_std = torch.tensor(np.array([np.std(test_prior['mu']), np.std(test_prior['log_tau'])]),\n",
    "                                      dtype=torch.float32, device=mps_device)\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample(batch_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mu = np.random.normal(loc=self.mu_mean, scale=self.mu_std, size=(batch_size,1))\n",
    "        log_tau = np.random.normal(loc=self.log_tau_mean, scale=self.log_tau_std, size=(batch_size,1))\n",
    "        theta_j = np.random.normal(loc=mu, scale=np.exp(log_tau), size=(batch_size, J))\n",
    "        return dict(mu=mu, log_tau=log_tau, theta_j=theta_j)\n",
    "\n",
    "    def score_global_batch(self, theta_batch_norm):\n",
    "        \"\"\" Computes the global score for a batch of parameters without explicit looping. \"\"\"\n",
    "        theta_batch = theta_batch_norm * self.prior_std + self.prior_mean\n",
    "        mu, log_tau = theta_batch[..., 0], theta_batch[..., 1]\n",
    "        grad_logp_mu = -(mu - self.mu_mean) / (self.mu_std**2)\n",
    "        grad_logp_tau = -(log_tau - self.log_tau_mean) / (self.log_tau_std**2)\n",
    "        score = torch.stack([grad_logp_mu, grad_logp_tau], dim=-1)\n",
    "        return score / self.prior_std\n",
    "\n",
    "    def score_local_batch(self, theta_batch_norm):\n",
    "        \"\"\" Computes the local score for a batch of samples. \"\"\"\n",
    "        theta_batch = theta_batch_norm * self.prior_std + self.prior_mean  # todo: should differ between global and local prior\n",
    "        mu, log_tau, theta_j = theta_batch[..., 0], theta_batch[..., 1], theta_batch[..., 2:]\n",
    "        # Gradient w.r.t theta_j\n",
    "        grad_logp_theta_j = -(theta_j - mu) / (torch.exp(log_tau)**2)\n",
    "        # correct the score for the normalization\n",
    "        score = grad_logp_theta_j / self.prior_std\n",
    "        return score\n",
    "\n",
    "prior = Prior()\n",
    "n_params = 2"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:50.841431Z",
     "start_time": "2025-02-13T00:27:50.838369Z"
    }
   },
   "cell_type": "code",
   "source": "prior(2)",
   "id": "557005f1016c5ded",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu': array([[0.0601421 ],\n",
       "        [7.52771209]]),\n",
       " 'log_tau': array([[5.23244057],\n",
       "        [5.59150044]]),\n",
       " 'theta_j': array([[-191.14834832,   27.55194558,  186.33058514,  -21.5385896 ,\n",
       "         -412.61555756,  -68.55850401,  155.48726917,  141.2585979 ],\n",
       "        [-317.10900255,   78.89788105,  369.67340495,  527.58649296,\n",
       "          -44.04420209, -147.9635381 ,  304.84245453, -146.04505225]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:51.039454Z",
     "start_time": "2025-02-13T00:27:51.034731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def positional_encoding(t, d_model, max_t=10000.0):\n",
    "    \"\"\"\n",
    "    Computes the sinusoidal positional encoding for a given time t.\n",
    "\n",
    "    Args:\n",
    "        t (torch.Tensor): The input time tensor of shape (batch_size, 1).\n",
    "        d_model (int): The dimensionality of the embedding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The positional encoding of shape (batch_size, d_model).\n",
    "    \"\"\"\n",
    "    half_dim = d_model // 2\n",
    "    div_term = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=t.device) *\n",
    "                         -(math.log(max_t) / (half_dim - 1)))\n",
    "    t_proj = t * div_term\n",
    "    pos_enc = torch.cat([torch.sin(t_proj), torch.cos(t_proj)], dim=-1)\n",
    "    return pos_enc\n",
    "\n",
    "# Define the Score Model based on F-NPSE\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.norm = nn.LayerNorm(out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.proj(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.activation(out)\n",
    "        return out + identity\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network model that computes score estimates.\n",
    "\n",
    "        Args:\n",
    "            input_dim_theta (int): Input dimension for theta.\n",
    "            hidden_dim_theta (int): Hidden dimension for theta network.\n",
    "            input_dim_x (int): Input dimension for x.\n",
    "            hidden_dim_x (int): Hidden dimension for x network.\n",
    "            hidden_dim_emb (int): Hidden dimension for the embedding network.\n",
    "            time_embed_dim (int, optional): Dimension of time embedding. Defaults to 4.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim_theta, hidden_dim_theta,\n",
    "                 input_dim_x, hidden_dim_x,\n",
    "                 hidden_dim_emb, time_embed_dim=4):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.net_theta = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, hidden_dim_theta),\n",
    "            ResidualBlock(hidden_dim_theta, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.net_x = nn.Sequential(\n",
    "            ResidualBlock(input_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, hidden_dim_x),\n",
    "            ResidualBlock(hidden_dim_x, input_dim_theta)\n",
    "        )\n",
    "\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.net_emb = nn.Sequential(\n",
    "            ResidualBlock(input_dim_theta*2 + time_embed_dim, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, hidden_dim_emb),\n",
    "            ResidualBlock(hidden_dim_emb, input_dim_theta)\n",
    "        )\n",
    "\n",
    "    def forward(self, theta, t, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ScoreModel.\n",
    "\n",
    "        Args:\n",
    "            theta (torch.Tensor): Input theta tensor of shape (batch_size, input_dim_theta).\n",
    "            t (torch.Tensor): Input time tensor of shape (batch_size, 1).\n",
    "            x (torch.Tensor): Input x tensor of shape (batch_size, input_dim_x).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the score model.\n",
    "        \"\"\"\n",
    "        theta_emb = self.net_theta(theta)\n",
    "        x_emb = self.net_x(x)\n",
    "        t_emb = positional_encoding(t, self.time_embed_dim)\n",
    "        return self.net_emb(torch.cat([theta_emb, x_emb, t_emb], dim=-1))"
   ],
   "id": "6d13bcb1279b6f74",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:51.291064Z",
     "start_time": "2025-02-13T00:27:51.285992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_synthetic_data(n_samples, schools_joint=False, device=None):\n",
    "    thetas = []\n",
    "    xs = []\n",
    "    for i in range(n_samples):\n",
    "        batch_params = prior(1)\n",
    "        if schools_joint:\n",
    "            sim_batch = simulator(batch_params)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1),\n",
    "                                 dtype=torch.float32, device=device)\n",
    "            x = torch.tensor(np.stack((sim_batch['observable'], sim_batch['sigma']), axis=-1),\n",
    "                             dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            sim_batch = simulator(batch_params, school_i=i % J)\n",
    "            theta = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1),\n",
    "                                 dtype=torch.float32, device=device)\n",
    "            x = torch.tensor(np.concatenate((sim_batch['observable'], sim_batch['sigma']), axis=-1),\n",
    "                             dtype=torch.float32, device=device)\n",
    "\n",
    "        thetas.append(theta)\n",
    "        xs.append(x)\n",
    "    thetas = torch.concatenate(thetas)\n",
    "    xs = torch.concatenate(xs)\n",
    "    return thetas, xs\n",
    "\n",
    "\n",
    "# Gaussian kernel for log pdf and sampling\n",
    "def gaussian_kernel_score(theta, theta_prime, gamma):\n",
    "    return -(theta - torch.sqrt(gamma) * theta_prime) / (1 - gamma)\n",
    "\n",
    "\n",
    "def gaussian_kernel_sample(theta_prime, gamma, device=None):\n",
    "    noise = torch.randn_like(theta_prime, dtype=torch.float32, device=device) * torch.sqrt(1 - gamma)\n",
    "    return torch.sqrt(gamma) * theta_prime + noise\n",
    "\n",
    "# Generate diffusion time and step size\n",
    "def generate_diffusion_time(max_t, steps, device=None):\n",
    "    time = torch.linspace(0, max_t, steps+1, dtype=torch.float32, device=device)[1:-1]  # exclude T\n",
    "    gamma = torch.linspace(1, 0, steps+1, dtype=torch.float32, device=device)[1:-1]  # first index is close to 1, noise of the kernel\n",
    "\n",
    "    alpha_t = torch.cat((gamma[0:1], gamma[1:] / gamma[:-1]), dim=0)\n",
    "    delta_t = 0.3 * (1-alpha_t) / torch.sqrt(alpha_t)  # weighting for loss, and later used as step size in Langevin dynamics\n",
    "    return time, gamma, delta_t"
   ],
   "id": "b506683f73d8dd26",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T00:27:51.504479Z",
     "start_time": "2025-02-13T00:27:51.499288Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Loss function for weighted MSE\n",
    "def weighted_mse_loss(inputs, targets, weights):\n",
    "    return torch.sum(weights * (inputs - targets) ** 2)\n",
    "\n",
    "\n",
    "def compute_score_loss(x_batch, theta_prime_batch, model, diffusion_time, gamma, delta_t, device=None):\n",
    "    # sample a diffusion time for each sample in the batch\n",
    "    t_index = torch.randint(0, len(diffusion_time), size=(x_batch.shape[0],), device=device)\n",
    "    t = diffusion_time[t_index]\n",
    "    g = gamma[t_index]\n",
    "    w = delta_t[t_index]\n",
    "    # sample from the Gaussian kernel\n",
    "    theta_batch = gaussian_kernel_sample(theta_prime_batch, g, device=device)\n",
    "    # calculate the score for the sampled theta\n",
    "    score_pred = model(theta=theta_batch, t=t, x=x_batch)\n",
    "    # calculate the reference score for the sampled theta\n",
    "    score_ref = gaussian_kernel_score(theta_batch, theta_prime_batch, g)\n",
    "    # calculate the loss\n",
    "    loss = weighted_mse_loss(score_pred, score_ref, weights=w)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, dataloader_valid=None, T=400, epochs=100, lr=1e-3, steps_diffusion_time=100, device=None):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Add Cosine Annealing Scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(max_t=T, steps=steps_diffusion_time, device=device)\n",
    "\n",
    "    # Add a new dimension so that each tensor has shape (steps, 1)\n",
    "    diffusion_time = diffusion_time.unsqueeze(1)\n",
    "    gamma = gamma.unsqueeze(1)\n",
    "    delta_t = delta_t.unsqueeze(1)\n",
    "\n",
    "    # Training loop\n",
    "    loss_history = np.zeros((epochs, 2))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        # for each sample in the batch, calculate the loss for a random diffusion time\n",
    "        for theta_prime_batch, x_batch in dataloader:\n",
    "            # initialize the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # calculate the loss\n",
    "            loss = compute_score_loss(x_batch, theta_prime_batch, model, diffusion_time, gamma, delta_t, device=device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # validate the model\n",
    "        valid_loss = 0.0\n",
    "        if dataloader_valid is not None:\n",
    "            for theta_prime_batch, x_batch in dataloader_valid:\n",
    "                with torch.no_grad():\n",
    "                    loss = compute_score_loss(x_batch, theta_prime_batch, model, diffusion_time, gamma, delta_t, device=device)\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "        loss_history[epoch] = [total_loss/len(dataloader), valid_loss/len(dataloader_valid)]\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}, \"\n",
    "                   f\"Valid Loss: {valid_loss/len(dataloader_valid):.4f}\")\n",
    "    return loss_history"
   ],
   "id": "3e4488af88ad4738",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-13T00:28:16.033975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "n_samples = 250000\n",
    "batch_size = 256\n",
    "T = 400\n",
    "steps_time = T\n",
    "\n",
    "score_model = ScoreModel(\n",
    "    input_dim_theta=n_params, hidden_dim_theta=64,\n",
    "    input_dim_x=n_obs_per_group, hidden_dim_x=64,\n",
    "    hidden_dim_emb=64\n",
    ")\n",
    "score_model.to(mps_device)\n",
    "\n",
    "# Create model and dataset\n",
    "thetas, xs = generate_synthetic_data(n_samples, device=mps_device)\n",
    "# Normalize data\n",
    "thetas = (thetas - prior.prior_mean) / prior.prior_std\n",
    "xs = (xs - prior.x_mean) / prior.x_std\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(thetas, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create validation data\n",
    "valid_prior, valid_data = generate_synthetic_data(1000, device=mps_device)\n",
    "valid_data = (valid_data - prior.x_mean) / prior.x_std\n",
    "valid_prior = (valid_prior - prior.prior_mean) / prior.prior_std\n",
    "dataset_valid = TensorDataset(valid_prior, valid_data)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                  T=T, epochs=200, lr=1e-3, steps_diffusion_time=steps_time, device=mps_device)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot loss history\n",
    "plt.plot(loss_history[:, 0], label='Train')\n",
    "plt.plot(loss_history[:, 1], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "aca578e1fb265da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Annealed Langevin Dynamics for Sampling\n",
    "def langevin_sampling(model, x_obs, n_post_samples, steps=5, device=None):\n",
    "    x_obs_norm = (x_obs - prior.x_mean) / prior.x_std  # assumes x_obs is not standardized\n",
    "\n",
    "    # Initialize parameters\n",
    "    n_obs = x_obs_norm.shape[0]\n",
    "    theta = torch.randn(n_post_samples, n_params, device=device) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32, device=device))\n",
    "\n",
    "    # Generate diffusion time parameters\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(max_t=T, steps=steps_time)\n",
    "\n",
    "    # Ensure x_obs_norm is a PyTorch tensor\n",
    "    if not isinstance(x_obs_norm, torch.Tensor):\n",
    "        x_obs_norm = torch.tensor(x_obs_norm, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Expand x_obs_norm to match the number of posterior samples\n",
    "    x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, -1, -1)  # Shape: (n_post_samples, n_obs, d)\n",
    "    x_expanded = x_exp.reshape(-1, x_obs_norm.shape[-1])\n",
    "\n",
    "    # Reverse iterate over diffusion times and step sizes\n",
    "    for step_size, t in zip(delta_t.flip(0), diffusion_time.flip(0)):\n",
    "        # Create tensor for current time step\n",
    "        t_tensor = torch.full((n_post_samples, 1), t, dtype=torch.float32, device=device)\n",
    "        t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Sample Gaussian noise\n",
    "            eps = torch.randn_like(theta, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Compute prior score\n",
    "            prior_score = prior.score_global_batch(theta)\n",
    "\n",
    "            # Compute model scores\n",
    "            theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, n_params)\n",
    "            model_scores = model(theta_exp, t=t_exp, x=x_expanded)\n",
    "            model_scores = model_scores.reshape(n_post_samples, n_obs, -1).sum(dim=1)\n",
    "\n",
    "            # Compute updated scores and perform Langevin step\n",
    "            scores = (1 - n_obs) * (T - t) / T * prior_score + model_scores\n",
    "            theta = theta + (step_size / 2) * scores + torch.sqrt(step_size) * eps\n",
    "     # correct for normalization\n",
    "    theta = theta * prior.prior_std + prior.prior_mean\n",
    "    # convert to numpy\n",
    "    theta = theta.detach().numpy()\n",
    "    return theta"
   ],
   "id": "bd3241835d991fc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from bayesflow import diagnostics",
   "id": "78c843703cb70e83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_prior, valid_data = generate_synthetic_data(10, schools_joint=True, device=mps_device)",
   "id": "eafc62be726c8506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=100, device=mps_device)\n",
    "                                    for vd in valid_data])"
   ],
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_samples_valid, np.array(valid_prior), param_names=[r'$\\mu$', r'$\\log \\tau$']);",
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_samples_valid, np.array(valid_prior),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$']);"
   ],
   "id": "c12c415e037a9277",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Apply on Data",
   "id": "b4a8277f98f6eae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate posterior samples\n",
    "test_data = torch.tensor(np.concatenate((y, sigma[:, np.newaxis]), axis=-1), dtype=torch.float32, device=mps_device)\n",
    "posterior_samples = langevin_sampling(score_model, test_data, n_post_samples=valid_prior.shape[0])\n",
    "print(\"Sampled posterior parameters:\", posterior_samples)"
   ],
   "id": "36e95b8ccefc31f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_posterior_2d(posterior_samples, prior_draws=valid_prior, param_names=[r'$\\mu$', r'$\\log \\tau$']);",
   "id": "d497c90b97f71a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_samples.mean(axis=0)",
   "id": "8b93f9d3e79792a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Stan inference Results\n",
    "$\\mu = 5.836806$\n",
    "\n",
    "$\\log\\tau = 2.450053$\n",
    "\n",
    "$\\theta =\n",
    "[ 0.64940756,  0.09001582, -0.23279844,  0.04471902, -0.33542507, -0.2041105,  0.53249937,  0.14456798]$\n",
    "\n",
    "https://github.com/blei-lab/edward/blob/master/notebooks/eight_schools.ipynb"
   ],
   "id": "9a13cf5a9239b208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "17edbe3260ac3ea0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c25ff86084d52f8e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
