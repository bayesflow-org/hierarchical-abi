{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize Results",
   "id": "d51694b515dc025d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "1ff7bd4bc152c5b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- we compare score model with only one condition, and with $k$-conditions\n",
    "- we show that the scaling in the number of needed sampling steps only depends on the Bayesian Units used\n",
    "- error reduces when using more conditions, but since network size stays the same, increases at some point again\n",
    "- we show how mini batching effects the posterior\n",
    "\n",
    "Metrics:\n",
    "- KL divergence between true and estimated posterior samples\n",
    "- RMSE between the medians of true and estimated posterior samples\n",
    "- Posterior contraction: (1 - var_empirical_posterior / var_prior) / (1 - var_true_posterior / var_prior), and using the mean variances over all parameters"
   ],
   "id": "1af00c3bef0c07d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load results\n",
    "#score_model_names = lambda m_id, n_obs: f'gaussian_flat{m_id}_{n_obs}score_model_v_variance_preserving_cosine_likelihood_weighting'\n",
    "#score_model_name = f'gaussian_flat_score_model_v_variance_preserving_cosine_likelihood_weighting'\n",
    "score_model_names = lambda m_id, n_obs: f'ar1_{m_id}_{n_obs}hierarchical_score_model_v_variance_preserving_cosine_likelihood_weighting'\n",
    "score_model_name = f'ar1_hierarchical_score_model_v_variance_preserving_cosine_likelihood_weighting'\n",
    "var_index = 3\n",
    "\n",
    "if not os.path.exists('plots/'+score_model_name):\n",
    "    os.makedirs('plots/'+score_model_name)\n",
    "\n",
    "variables_of_interest = ['mini_batch', 'cosine_shift', 'damping_factor_t']\n",
    "variables_of_interest.append('n_conditions')\n",
    "variable_of_interest = variables_of_interest[var_index]\n",
    "print(variable_of_interest)\n",
    "\n",
    "m_ids = np.arange(10)\n",
    "n_obs = 1 if variable_of_interest != 'n_conditions' else (128 if score_model_name[:2] == 'ar' else 100) # todo: rename models and dict as well\n",
    "\n",
    "results_list = []\n",
    "for m_id in m_ids:\n",
    "    file_name = f'plots/{score_model_names(m_id, n_obs)}/df_results_{variable_of_interest}.csv'\n",
    "    results_list.append(pd.read_csv(file_name, index_col=0))\n",
    "df_results = pd.concat(results_list)\n",
    "\n",
    "# rename column name\n",
    "df_results['damping_factor_t'] = df_results['damping_factor']\n",
    "\n",
    "# Ensure we generate enough synthetic data samples.\n",
    "max_steps = 10000\n",
    "\n",
    "# settings for plotting\n",
    "mini_batch = ['10%']\n",
    "n_conditions = [1]\n",
    "cosine_shifts = [0]\n",
    "d_factors = [1]  # using the d factor depending on the mini batch size\n",
    "data_sizes = np.array([1, 10, 100, 1000, 10000, 100000])\n",
    "\n",
    "if variable_of_interest == 'mini_batch':\n",
    "    # Set up your data sizes and mini-batch parameters.\n",
    "    mini_batch = [1, 10, 100, 1000, 10000, None]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest == 'n_conditions':\n",
    "    n_conditions = [1, 5, 10, 20, 50, 100]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest == 'cosine_shift':\n",
    "    cosine_shifts = [0, -1, 1, 2, 5, 10]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest in ['damping_factor', 'damping_factor_prior', 'damping_factor_t']:\n",
    "    d_factors = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.5, 0.75, 0.9, 1]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "else:\n",
    "    raise ValueError('Unknown variable_of_interest')\n",
    "\n",
    "colors = ['#a6cee3', '#1f77b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a']\n",
    "\n",
    "if score_model_name[:2] != 'ar':\n",
    "    metrics = {\n",
    "        'kl': 'KL Divergence',\n",
    "        'median_rmse': 'RMSE',\n",
    "        'rel_contraction': 'Relative Posterior Contraction',\n",
    "        'c_error': 'Calibration Error'\n",
    "    }\n",
    "\n",
    "    # Define the metrics to plot: key is dataframe column, value is label for y-axis.\n",
    "    y_limits = {\n",
    "        #'kl': (0, 100),\n",
    "        'median_rmse': (-0.1, 1),\n",
    "        'rel_contraction': (-0.1, 1.2),\n",
    "        'c_error': (-0.05, 0.55)\n",
    "    }\n",
    "else:\n",
    "    metrics = {\n",
    "        'rmse_global': 'RMSE Global',\n",
    "        'c_error_global': 'Calibration Error Global',\n",
    "        'contractions_global': 'Posterior Contraction Global',\n",
    "        'rmse_local': 'RMSE Local',\n",
    "        'c_error_local': 'Calibration Error Local',\n",
    "        'contractions_local': 'Posterior Contraction Local'\n",
    "    }\n",
    "\n",
    "    # Define the metrics to plot: key is dataframe column, value is label for y-axis.\n",
    "    y_limits = {\n",
    "        #'kl': (0, 100),\n",
    "        'rmse_global': (-0.1, 1),\n",
    "        'rmse_local': (-0.1, 1),\n",
    "        'contractions_global': (-0.1, 1.2),\n",
    "        'contractions_local': (-0.1, 1.2),\n",
    "        'c_error_global': (-0.05, 0.55),\n",
    "        'c_error_local': (-0.05, 0.55)\n",
    "    }\n",
    "\n",
    "experiment_names = {\n",
    "    'damping_factor': 'Damping Factor',\n",
    "    'damping_factor_t': 'Damping Factor\\nTime Dependent',\n",
    "    'damping_factor_prior': 'Damping Factor Prior',\n",
    "    'n_conditions': 'Number of Conditions',\n",
    "    'cosine_shift': 'Cosine Shift',\n",
    "    'data_size': 'Data Size',\n",
    "    'mini_batch': 'Mini Batch Size'\n",
    "}"
   ],
   "id": "6b38f0a639021841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Group by both second_variable_of_interest and variable_of_interest to compute mean and standard deviation of n_steps.\n",
    "grouped_bar = df_results.groupby([second_variable_of_interest, variable_of_interest])['n_steps'].agg(['mean','std']).reset_index()\n",
    "\n",
    "# Determine unique second_variable_of_interest and variable_of_interest values.\n",
    "second_variable_of_interest_values = sorted(grouped_bar[second_variable_of_interest].unique())\n",
    "variable_batch_values = sorted(grouped_bar[variable_of_interest].unique())\n",
    "\n",
    "# Set up errorbar plot parameters.\n",
    "n_groups = len(second_variable_of_interest_values)\n",
    "n_series = len(variable_batch_values)\n",
    "x = np.arange(n_groups)  # base x locations for groups\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "# Plot an errorbar for each variable_of_interest value within each second_variable_of_interest group.\n",
    "for i, mb in enumerate(variable_batch_values):\n",
    "    subset = grouped_bar[grouped_bar[variable_of_interest] == mb]\n",
    "    means = []\n",
    "    stds = []\n",
    "    for ds in second_variable_of_interest_values:\n",
    "        row = subset[subset[second_variable_of_interest] == ds]\n",
    "        if not row.empty:\n",
    "            means.append(row['mean'].values[0])\n",
    "            stds.append(row['std'].values[0])\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            stds.append(0)\n",
    "\n",
    "    # Use 'o-' for markers connected by lines.\n",
    "    ax.errorbar(x, means, yerr=stds, fmt='o-', capsize=5, label=f'{mb}', alpha=0.75, color=colors[i])\n",
    "\n",
    "ax.axhline(max_steps, color='k', linestyle='--')\n",
    "ax.text(0.1, max_steps-3500, f\"Maximal Number of Steps\", fontsize=8, color='k')\n",
    "\n",
    "# Center the x-axis ticks and label them.\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(second_variable_of_interest_values)\n",
    "ax.set_xlabel(experiment_names[second_variable_of_interest])\n",
    "ax.set_ylabel('Number of Steps')\n",
    "#ax.set_title(f'Number of Steps by {experiment_names[second_variable_of_interest]} and {experiment_names[variable_of_interest]}')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(10, max_steps*2)\n",
    "ax.legend(title=experiment_names[variable_of_interest], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.savefig(f'plots/{score_model_name}/{variable_of_interest}_n_steps.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "322dfffed5714b9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if variable_batch_values == 'mini_batch_size':\n",
    "    # ------------------------------\n",
    "    # Plot 1: Bar plot of n_steps for the full-batch  case.\n",
    "    # ------------------------------\n",
    "\n",
    "    # Filter the full-batch rows\n",
    "    df_full = df_results[df_results['data_size'] == df_results['mini_batch']]\n",
    "\n",
    "    # Group by data_size and compute mean and standard deviation of n_steps.\n",
    "    grouped_full = df_full.groupby('data_size')['n_steps'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    plt.figure(figsize=(4, 3), tight_layout=True)\n",
    "    plt.bar(grouped_full['data_size'], grouped_full['mean'],\n",
    "            yerr=grouped_full['std'], capsize=5, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Number of Steps')\n",
    "    plt.title('Number of Steps (Full Batch) per Data Size')\n",
    "    plt.xticks(grouped_full['data_size'])\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(f'plots/{score_model_name}/{variable_of_interest}_steps_full_batch.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Plot 2: Errorbar plots for MMD, RMSE, and Contraction vs. variable_of_interest.\n",
    "# ------------------------------\n",
    "\n",
    "# Filter rows with a variable_of_interest value (skip full-batch rows).\n",
    "df_mb = df_results[df_results[variable_of_interest].notnull()].copy()\n",
    "# Convert mini_batch to float (if not already) to allow proper plotting on the x-axis.\n",
    "df_mb[variable_of_interest] = df_mb[variable_of_interest].astype(float)\n",
    "\n",
    "# Identify the unique data sizes (to plot different lines per data size).\n",
    "unique_second_variable_of_interest = sorted(df_mb[second_variable_of_interest].unique())\n",
    "\n",
    "# Create one figure per metric.\n",
    "for metric, metric_label in metrics.items():\n",
    "    plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "    for i, ds in enumerate(unique_second_variable_of_interest):\n",
    "        # Select the rows for this particular data size.\n",
    "        df_sub = df_mb[(df_mb[second_variable_of_interest] == ds) & (df_mb['n_steps'] != max_steps)]\n",
    "        # Group by variable_of_interest size to get mean and std of the metric.\n",
    "        grouped = df_sub.groupby(variable_of_interest)[metric].agg(['mean', 'std']).reset_index()\n",
    "        if not np.isfinite(grouped['mean']).all() or grouped.empty:\n",
    "            continue\n",
    "        plt.errorbar(grouped[variable_of_interest], grouped['mean'], yerr=grouped['std'],\n",
    "                     marker='o', capsize=5, label=f'{ds}', alpha=0.75, color=colors[i])\n",
    "    plt.xlabel(experiment_names[variable_of_interest])\n",
    "    plt.ylabel(metric_label)\n",
    "    #plt.title(f'{metric_label} vs {experiment_names[variable_of_interest]}')\n",
    "    plt.legend(title=experiment_names[second_variable_of_interest], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    # Using a logarithmic scale for the x-axis since mini-batch sizes vary widely.\n",
    "    if metric == 'rel_contraction':\n",
    "        plt.axhline(1, linestyle='--', color='k')\n",
    "        if variable_of_interest == 'mini_batch':\n",
    "            plt.text(1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "        elif variable_of_interest == 'n_conditions':\n",
    "            plt.text(1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "        elif variable_of_interest == 'cosine_shift':\n",
    "            plt.text(1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "        elif variable_of_interest == 'damping_factor_t':\n",
    "            plt.text(0.1, 1.05, f\"Optimal\", fontsize=8, color='k')\n",
    "\n",
    "    if variable_of_interest == 'mini_batch' or variable_of_interest == 'damping_factor_t':\n",
    "        plt.xscale('log')\n",
    "    if metric == 'kl':\n",
    "        plt.yscale('log')\n",
    "    else:\n",
    "        plt.ylim(y_limits[metric])\n",
    "    plt.savefig(f'plots/{score_model_name}/{variable_of_interest}_{metric}.png', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "a23cb90cc04bf44a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "85d257d92a803f3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
