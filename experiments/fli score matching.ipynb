{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# FLI with compositional score matching\n",
    "\n",
    "Before running this notebook, please unzip the data, IRF and noise in the folder `problems/FLI`."
   ],
   "id": "e928161dcccd36c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "from sklearn.metrics import r2_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, TimeSeriesNetwork, ShallowSet, euler_maruyama_sampling, train_score_model, probability_ode_solving\n",
    "from problems.fli import FLIProblem, FLI_Prior, generate_synthetic_data\n",
    "from problems import visualize_simulation_output"
   ],
   "id": "6df92a90e4f326bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"mps\")",
   "id": "350b17a5998f1981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "prior = FLI_Prior()\n",
    "batch_size = 64\n",
    "number_of_obs = 1\n",
    "max_number_of_obs = number_of_obs if isinstance(number_of_obs, int) else max(number_of_obs)\n",
    "\n",
    "current_sde = SDE(\n",
    "    kernel_type='variance_preserving',\n",
    "    noise_schedule='cosine'\n",
    ")\n",
    "\n",
    "dataset = FLIProblem(\n",
    "    n_data=30000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    online_learning=False,\n",
    "    number_of_obs=number_of_obs,\n",
    ")\n",
    "\n",
    "dataset_valid = FLIProblem(\n",
    "    n_data=1000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    number_of_obs=number_of_obs\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for test in dataloader:\n",
    "    print(test[0].shape)\n",
    "    print(test[2].shape)\n",
    "    print(test[4].shape)\n",
    "    break"
   ],
   "id": "9d8e0a98adf956f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "n_blocks = [5,6][0]\n",
    "hidden_dim = [256, 512][0]\n",
    "hidden_dim_summary = [10, 14, 18, 22][0]\n",
    "split_summary_vector = [True, False][0]\n",
    "\n",
    "summary_net = TimeSeriesNetwork(input_dim=1, recurrent_dim=256, summary_dim=hidden_dim_summary)\n",
    "\n",
    "global_summary_dim = hidden_dim_summary\n",
    "global_summary_net = ShallowSet(dim_input=hidden_dim_summary, dim_output=global_summary_dim, dim_hidden=128)\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x_global=global_summary_dim,\n",
    "    input_dim_x_local=hidden_dim_summary,\n",
    "    summary_net=summary_net,\n",
    "    global_summary_net=global_summary_net if isinstance(number_of_obs, list) else None,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_blocks=n_blocks,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    "    prediction_type='v',\n",
    "    sde=current_sde,\n",
    "    weighting_type='likelihood_weighting',\n",
    "    prior=prior,\n",
    "    dropout_rate=0.1,\n",
    "    name_prefix=f'FLI_{max_number_of_obs}_{hidden_dim_summary}_{hidden_dim}_{n_blocks}{\"_split\" if split_summary_vector else \"\"}_{summary_net.name}_',\n",
    "    split_summary_vector=split_summary_vector\n",
    ")\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "c03152658206c2ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.exists(f\"models/{score_model.name}.pt\"):\n",
    "    # train model\n",
    "    loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid, hierarchical=True,\n",
    "                                                  epochs=3000, device=torch_device)\n",
    "    score_model.eval()\n",
    "    torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "    # plot loss history\n",
    "    plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "    plt.plot(loss_history[:, 0], label='Training', color=\"#132a70\", lw=2.0, alpha=0.9)\n",
    "    plt.plot(loss_history[:, 1], label='Validation', linestyle=\"--\", marker=\"o\", color='black')\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.xlabel('Training epoch #')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'plots/{score_model.name}/loss_training.png')\n",
    "else:\n",
    "    score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", map_location=torch_device, weights_only=True))\n",
    "    score_model.eval()"
   ],
   "id": "b3cd5b37bc1d3dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "427bd98037bf8095"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_local_samples = 32**2\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior=prior, n_data=100,\n",
    "                                                                    n_local_samples=n_local_samples, random_seed=0)\n",
    "n_post_samples = 100\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(n_local_samples)\n",
    "#score_model.current_number_of_obs = 4  # we can choose here, how many observations are passed together through the score\n",
    "score_model.current_number_of_obs = max_number_of_obs\n",
    "print(valid_data.shape, score_model.current_number_of_obs)"
   ],
   "id": "ba4d477c8f330ab8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t1_value = 0.001\n",
    "t0_value = 0.4\n",
    "sampling_arg = {\n",
    "    'size': 2,\n",
    "    'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * t),\n",
    "    #'damping_factor': lambda t: (1-torch.ones_like(t)) * 1/(n_local_samples-500) + 0.01,\n",
    "    #'damping_factor': lambda t: (1-torch.ones_like(t)) * 1/(n_local_samples-900) + 0.01,\n",
    "    #'damping_factor': lambda t: torch.ones_like(t) * 1e-10 + 0.0001,\n",
    "    #'sampling_chunk_size': 512,\n",
    "}\n",
    "score_model.sde.s_shift_cosine = 0"
   ],
   "id": "7a3e813ebc2e9804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = probability_ode_solving(score_model, valid_data,\n",
    "                                                         n_post_samples=n_post_samples,\n",
    "                                                         sampling_arg=sampling_arg,\n",
    "                                                         diffusion_steps=500, device=torch_device, verbose=True)"
   ],
   "id": "24b66968ecb78629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global.png')"
   ],
   "id": "7d472e9452358335",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.sde.s_shift_cosine = 0\n",
    "score_model.current_number_of_obs = 1\n",
    "posterior_local_samples_valid = euler_maruyama_sampling(score_model, valid_data,\n",
    "                                                        n_post_samples=n_post_samples, conditions=posterior_global_samples_valid,\n",
    "                                                        diffusion_steps=100, device=torch_device, verbose=True)"
   ],
   "id": "ad1fdcfd90ecdb3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0],\n",
    "                                                                 n_post_samples, -1)[:, :, :12],\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1)[:, :12],\n",
    "                          variable_names=local_param_names[:12])"
   ],
   "id": "30208ea56e7b16e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot locals for a single grid\n",
    "for i in range(14, 15):\n",
    "    tau, tau_2, A = prior.transform_raw_params(\n",
    "        log_tau=posterior_local_samples_valid[i:i+1, :, :, 0],\n",
    "        log_delta_tau=posterior_local_samples_valid[i:i+1, :, :, 1],\n",
    "        a=posterior_local_samples_valid[i:i+1, :, :, 2],\n",
    "    )\n",
    "    tau_mean = A * tau + (1-A) * tau_2\n",
    "    posterior_local_samples_valid_transf = np.concatenate([tau[:, :, :, np.newaxis], tau_2[:, :, :, np.newaxis], A[:, :, :, np.newaxis], tau_mean[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "    tau, tau_2, A = prior.transform_raw_params(\n",
    "        log_tau=valid_prior_local[i:i+1, :, 0],\n",
    "        log_delta_tau=valid_prior_local[i:i+1, :, 1],\n",
    "        a=valid_prior_local[i:i+1, :, 2],\n",
    "    )\n",
    "    tau_mean = A * tau + (1-A) * tau_2\n",
    "    valid_prior_local_transf = np.concatenate([tau[:, :, np.newaxis], tau_2[:,  :, np.newaxis], A[:, :, np.newaxis], tau_mean[:, :, np.newaxis]], axis=-1)\n",
    "\n",
    "    fig = diagnostics.recovery(np.transpose(posterior_local_samples_valid_transf, (0,2,1,3)).reshape(-1, n_post_samples, 4)[:, :, :3],\n",
    "                         valid_prior_local_transf.reshape(-1, 4)[:, :3],\n",
    "                         variable_names=[r'$\\tau_1^L$', r'$\\tau_2^L$', r'$A^L$']) #, r'$\\tau_\\text{mean}^L$'])\n",
    "    #fig.savefig(\"plots/hierarchical_simulated_recovery.pdf\")\n",
    "    plt.show()"
   ],
   "id": "43df524cf9e258a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Apply the Model to Real Data",
   "id": "1e63320060c0e747"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "global_param_names = prior.global_param_names",
   "id": "f97424507df4688c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load MLE binary map\n",
    "mle_parameters = np.load(\"problems/FLI/mle_parameters.npy\")\n",
    "tau_mean = mle_parameters[:, :, 2] * mle_parameters[:, :, 0] + (1 - mle_parameters[:, :, 2]) * mle_parameters[:, :, 1]\n",
    "mle_estimates = np.concatenate((mle_parameters[:, :, :3], tau_mean[..., np.newaxis]), axis=-1)"
   ],
   "id": "568ff6d8e5298d6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grid_data = 512 #32\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(grid_data * grid_data)\n",
    "\n",
    "x_offset = 0#225\n",
    "y_offset = 0#245\n",
    "binned_data = np.load('problems/FLI-all/exp_binned_data.npy')[x_offset:x_offset + grid_data, y_offset:y_offset + grid_data]\n",
    "binned_data = binned_data.reshape(1, grid_data * grid_data, 256, 1)\n",
    "\n",
    "data = np.load('problems/FLI-all/final_Data.npy')[:, x_offset:x_offset + grid_data, y_offset:y_offset + grid_data]\n",
    "data = data.reshape(1, grid_data * grid_data, 256, 1)\n",
    "cut_off = 17\n",
    "binary_mask = (np.sum(data, axis=2, keepdims=True) > cut_off)\n",
    "binary_mask = binary_mask.flatten() & (mle_estimates[:, :, 0] != 0).flatten() & (mle_estimates[:, :, 1] != 0).flatten()  # binary mask sets estimates to 0\n",
    "binary_mask = binary_mask.reshape(1, binned_data.shape[1], 1, 1)\n",
    "\n",
    "real_data = binned_data\n",
    "norm = np.max(real_data, axis=2, keepdims=True)\n",
    "norm[~binary_mask] = 1\n",
    "real_data = real_data / norm\n",
    "\n",
    "plt.imshow(np.sum(data[0], axis=(1,2)).reshape(grid_data, grid_data), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ],
   "id": "92979e8d70b2bb30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_post_samples = 100\n",
    "sampling_arg = {\n",
    "    'size': 2,\n",
    "    'damping_factor': lambda t: torch.ones_like(t) * 1e-10 + 0.0001,\n",
    "    \"sampling_weights\": binary_mask.flatten() * 1,\n",
    "}\n",
    "score_model.sde.s_shift_cosine = 0"
   ],
   "id": "122ee21b44a3fd4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_real = euler_maruyama_sampling(score_model, real_data,\n",
    "                                                         n_post_samples=n_post_samples,\n",
    "                                                         sampling_arg=sampling_arg,\n",
    "                                                         diffusion_steps=300, device=torch_device, verbose=True)"
   ],
   "id": "71837e0c923c74e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior_dict = {}\n",
    "posterior_dict = {}\n",
    "prior_tranf_dict = {}\n",
    "posterior_tranf_dict = {}\n",
    "for i in range(len(global_param_names)):\n",
    "    prior_dict[global_param_names[i]] = valid_prior_global[:, i]\n",
    "    posterior_dict[global_param_names[i]] = posterior_global_samples_real[0, :, i]\n",
    "\n",
    "tau, tau_2, A = prior.transform_raw_params(\n",
    "        log_tau=prior_dict[global_param_names[0]],\n",
    "        log_delta_tau=prior_dict[global_param_names[2]],\n",
    "        a=prior_dict[global_param_names[4]]\n",
    "    )\n",
    "prior_tranf_dict = {\n",
    "    r'$\\tau$': tau,\n",
    "    r'$\\tau_2$': tau_2,\n",
    "    r'$A$': A\n",
    "}\n",
    "\n",
    "tau, tau_2, A = prior.transform_raw_params(\n",
    "        log_tau=posterior_dict[global_param_names[0]],\n",
    "        log_delta_tau=posterior_dict[global_param_names[2]],\n",
    "        a=posterior_dict[global_param_names[4]]\n",
    "    )\n",
    "posterior_tranf_dict = {\n",
    "    r'$\\tau$': tau,\n",
    "    r'$\\tau_2$': tau_2,\n",
    "    r'$A$': A\n",
    "}\n",
    "print(r'$\\tau$', np.median(tau))\n",
    "print(r'$\\tau_2$', np.median(tau_2))\n",
    "print(r'$A$', np.median(A))"
   ],
   "id": "390aae2682774186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.pairs_posterior(\n",
    "    posterior_dict,\n",
    "    priors=prior_dict,\n",
    ")\n",
    "#fig.savefig(f'plots/real_data_global_posterior.pdf')\n",
    "\n",
    "fig = diagnostics.pairs_posterior(\n",
    "    posterior_tranf_dict,\n",
    "    priors=prior_tranf_dict,\n",
    ")\n",
    "#fig.savefig(f'plots/real_data_global_posterior_transf.pdf')"
   ],
   "id": "fe01b271b9107e19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.median(posterior_global_samples_real[0], axis=0)",
   "id": "bb169ac3655b5e67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.sde.s_shift_cosine = 0\n",
    "posterior_local_samples_real = np.ones((1, n_post_samples, real_data.shape[1], 3)) * np.nan\n",
    "posterior_local_samples_real[:, :, binary_mask.flatten()] = euler_maruyama_sampling(\n",
    "    score_model, real_data[:, binary_mask.flatten()],\n",
    "    conditions=posterior_global_samples_real,\n",
    "    n_post_samples=n_post_samples,\n",
    "    diffusion_steps=100, device=torch_device, verbose=True\n",
    ")"
   ],
   "id": "16ad23624cab13a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#np.save('fli_real_local_samples.npy', posterior_local_samples_real)\n",
    "posterior_local_samples_real = np.load('fli_real_local_samples.npy')"
   ],
   "id": "acb421fc9883f0e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tau, tau_2, A = prior.transform_raw_params(\n",
    "    log_tau=posterior_local_samples_real[0, :, :, 0].reshape(n_post_samples, grid_data, grid_data),\n",
    "    log_delta_tau=posterior_local_samples_real[0, :, :, 1].reshape(n_post_samples, grid_data, grid_data),\n",
    "    a=posterior_local_samples_real[0, :, :, 2].reshape(n_post_samples, grid_data, grid_data),\n",
    ")\n",
    "tau_mean = A * tau + (1-A) * tau_2\n",
    "ps = np.concatenate([tau[:, :, :, np.newaxis], tau_2[:, :, :, np.newaxis], A[:, :, :, np.newaxis], tau_mean[:, :, :, np.newaxis]], axis=-1)\n",
    "transf_local_param_names = [r'$\\tau_1^L$', r'$\\tau_2^L$', r'$A^L$', r'$\\tau^\\text{mean}$']\n",
    "\n",
    "med = np.median(ps, axis=0)\n",
    "posterior_mad = mad(ps, axis=0)\n",
    "visualize_simulation_output(med,\n",
    "                            mask=binary_mask.reshape(grid_data, grid_data),\n",
    "                            title_prefix=['Posterior Median ' + p for p in transf_local_param_names],\n",
    "                            cmap='jet', scales=[(0,1), (0, 2), (0,1), (0, 2)], add_scale_bar=False)\n",
    "visualize_simulation_output(posterior_mad,\n",
    "                            mask=binary_mask.reshape(grid_data, grid_data),\n",
    "                            title_prefix=['Posterior MAD ' + p for p in transf_local_param_names],\n",
    "                            cmap='jet', scales=[(0,1), (0, 2), (0,1), (0, 2)], add_scale_bar=False)"
   ],
   "id": "293d6dc6858e95d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({\n",
    "    \"font.size\": 12,            # Base font size\n",
    "    \"axes.titlesize\": 12,       # Axes title\n",
    "    \"axes.labelsize\": 12,       # Axes labels\n",
    "    \"xtick.labelsize\": 10,      # Tick labels\n",
    "    \"ytick.labelsize\": 10,\n",
    "})\n",
    "\n",
    "fig, axis = plt.subplots(2, 1, figsize=(5, 4), layout='constrained', sharex=True, sharey=True)\n",
    "axis = axis.flatten()\n",
    "for i, ax in enumerate(axis):\n",
    "    while True:\n",
    "        pixel_ids = [np.random.randint(0, grid_data), np.random.randint(0, grid_data)]\n",
    "        #pixel_ids = [[16, 15], [21, 15]][i]\n",
    "        #pixel_ids = [[11, 0], [7, 0]][i]\n",
    "        pixel_ids = [[11, 0], [409, 362]][i]\n",
    "        if binary_mask.reshape(grid_data, grid_data)[pixel_ids[0], pixel_ids[1]]:\n",
    "            break  # only plot meaningful data\n",
    "    print(pixel_ids)\n",
    "    simulations = np.array([\n",
    "        prior.simulator.decay_gen_single(\n",
    "            tau_L=tau[post_index, pixel_ids[0], pixel_ids[1]],\n",
    "            tau_L_2=tau_2[post_index, pixel_ids[0], pixel_ids[1]],\n",
    "            A_L=A[post_index, pixel_ids[0], pixel_ids[1]]\n",
    "        ) for post_index in range(tau.shape[0])\n",
    "    ])\n",
    "\n",
    "    ax.plot(real_data.reshape(grid_data, grid_data, 256)[pixel_ids[0], pixel_ids[1]], label='data', color='black')\n",
    "    ax.plot(np.median(simulations, axis=0), label='posterior median', alpha=0.8, color='red')\n",
    "    alpha = 0.05\n",
    "    ax.fill_between(\n",
    "        np.arange(256),\n",
    "        np.percentile(simulations, 100*(alpha/2), axis=0),\n",
    "        np.percentile(simulations, 100*(1-alpha/2), axis=0),\n",
    "        color='red', alpha=0.3, label='posterior 95% CI'\n",
    "    )\n",
    "    if i == 0:\n",
    "        ax.legend(labels=[r'Real Data', r'Posterior Median', 'Posterior 95% CI'], ncol=1, loc='upper right')\n",
    "    if i == len(axis) - 1:\n",
    "        ax.set_xlabel(r'Time [s]')\n",
    "    ax.set_ylabel('Normalized\\nPhoton Count')\n",
    "\n",
    "#plt.savefig(f'plots/real_data_fit2.pdf', transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "77d0b6207714ca94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axis = plt.subplots(2, 4, figsize=(10, 4), layout='constrained', sharex=True, sharey=True)\n",
    "axis = axis.flatten()\n",
    "for i, ax in enumerate(axis):\n",
    "    while True:\n",
    "        pixel_ids = [np.random.randint(0, grid_data), np.random.randint(0, grid_data)]\n",
    "        #pixel_ids = [[16, 15], [21, 15]][i]\n",
    "        #pixel_ids = [[11, 0], [7, 0]][i]\n",
    "        if binary_mask.reshape(grid_data, grid_data)[pixel_ids[0], pixel_ids[1]]:\n",
    "            break  # only plot meaningful data\n",
    "    #print(pixel_ids)\n",
    "    simulations = np.array([\n",
    "        prior.simulator.decay_gen_single(\n",
    "            tau_L=tau[post_index, pixel_ids[0], pixel_ids[1]],\n",
    "            tau_L_2=tau_2[post_index, pixel_ids[0], pixel_ids[1]],\n",
    "            A_L=A[post_index, pixel_ids[0], pixel_ids[1]]\n",
    "        ) for post_index in range(25)\n",
    "    ])\n",
    "\n",
    "    ax.plot(real_data.reshape(grid_data, grid_data, 256)[pixel_ids[0], pixel_ids[1]], label='data', color='black')\n",
    "    ax.plot(np.median(simulations, axis=0), label='posterior median', alpha=0.8, color='red')\n",
    "    if i == len(axis) - 1 or i == len(axis) - 2 or i == len(axis) - 3 or i == len(axis) - 4:\n",
    "        ax.set_xlabel(r'Time [s]')\n",
    "    if i % 4 == 0:\n",
    "        ax.set_ylabel('Normalized\\nPhoton Count')\n",
    "\n",
    "    real_pixel = real_data.reshape(grid_data, grid_data, 256)[pixel_ids[0], pixel_ids[1]]\n",
    "    coverages = {}\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.2]:\n",
    "        lo = np.percentile(simulations, 100*(alpha/2), axis=0)\n",
    "        hi = np.percentile(simulations, 100*(1-alpha/2), axis=0)\n",
    "        covered = (real_pixel >= lo) & (real_pixel <= hi)\n",
    "        coverages[alpha] = covered.mean()\n",
    "    #print(coverages)\n",
    "    alpha = 0.05\n",
    "    ax.fill_between(\n",
    "        np.arange(256),\n",
    "        np.percentile(simulations, 100*(alpha/2), axis=0),\n",
    "        np.percentile(simulations, 100*(1-alpha/2), axis=0),\n",
    "        color='red', alpha=0.3, label='posterior 95% CI'\n",
    "    )\n",
    "\n",
    "    ax.text(0.4, 0.95, f'Coverage: {coverages[alpha].mean():.2f}', transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "fig.legend(labels=[r'Data', r'Posterior Median', r'Posterior 95% CI'], bbox_to_anchor=(0.5, -0.1),\n",
    "           ncol=3, loc='lower center')\n",
    "#plt.savefig(f'plots/real_data_fit_more.pdf', transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "996a74d642ab1a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_post_samples_sim = 20\n",
    "tau_mle = mle_estimates[:, :, 0]\n",
    "tau_2_mle = mle_estimates[:, :, 1]\n",
    "A_mle = mle_estimates[:, :, 2]\n",
    "\n",
    "@delayed\n",
    "def wrapper(pixel_i):\n",
    "    _simulations = np.ones((n_post_samples_sim, 1, grid_data, 256)) * np.nan\n",
    "    _simulations_mle = np.ones((1, 1, grid_data, 256)) * np.nan\n",
    "    for pixel_j in range(grid_data):\n",
    "        if not binary_mask.reshape(grid_data, grid_data)[pixel_i, pixel_j]:\n",
    "            continue  # not a valid pixel\n",
    "        _simulations[:, 0, pixel_j, :] = [\n",
    "            prior.simulator.decay_gen_single(\n",
    "                tau_L=tau[post_index, pixel_i, pixel_j],\n",
    "                tau_L_2=tau_2[post_index, pixel_i, pixel_j],\n",
    "                A_L=A[post_index, pixel_i, pixel_j]\n",
    "            ) for post_index in range(n_post_samples_sim)\n",
    "        ]\n",
    "        _simulations_mle[:, 0, pixel_j, :] = [\n",
    "            prior.simulator.decay_gen_single(\n",
    "                tau_L=tau_mle[pixel_i, pixel_j],\n",
    "                tau_L_2=tau_2_mle[pixel_i, pixel_j],\n",
    "                A_L=A_mle[pixel_i, pixel_j]\n",
    "            )\n",
    "        ]\n",
    "    return _simulations, _simulations_mle\n",
    "\n",
    "simulations = Parallel(n_jobs=10, verbose=1)(wrapper(pixel_i) for pixel_i in range(grid_data))\n",
    "simulations_mle = np.concatenate([s[1] for s in simulations], axis=1)  # shape: (1, grid_data, grid_data, 256)\n",
    "simulations = np.concatenate([s[0] for s in simulations], axis=1)  # shape: (n_post_samples, grid_data, grid_data, 256)"
   ],
   "id": "b8b58992478d2218",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ppc_chi2(real_flat, simulations):\n",
    "    \"\"\"\n",
    "    real_flat: array of shape (G, T) — observed values per group/pixel\n",
    "    simulations: array of shape (S, G, T) — posterior‐predictive draws\n",
    "    Returns:\n",
    "      D_obs:  array of shape (G,)    — observed discrepancies\n",
    "      p_ppc:  array of shape (G,)    — Bayesian p-values\n",
    "    \"\"\"\n",
    "    # 1. Compute summary f_t and empirical variance Var_t\n",
    "    f = np.median(simulations, axis=0)                # shape (G, T)\n",
    "    var_t = np.var(simulations, axis=0, ddof=1)       # shape (G, T)\n",
    "\n",
    "    # in tails simulations can be very small\n",
    "    var_t[var_t < 1e-10] = 1e-10                       # avoid division by zero\n",
    "\n",
    "    # 2. Discrepancy for observed data\n",
    "    D_obs = np.sum((real_flat - f)**2 / var_t, axis=1)  # shape (G,)\n",
    "\n",
    "    # 3. Discrepancies for replicated data\n",
    "    #    D_rep[s, g] = sum_t (simulations[s,g,t] - f[g,t])**2 / var_t[g,t]\n",
    "    diffs = (simulations - f[None, :, :])**2 / var_t[None, :, :]\n",
    "    D_rep = np.sum(diffs, axis=2)                      # shape (S, G)\n",
    "\n",
    "    # 4. Bayesian p-values\n",
    "    p_ppc = np.mean(D_rep >= D_obs[None, :], axis=0)   # shape (G,)\n",
    "\n",
    "    return D_obs, p_ppc"
   ],
   "id": "b6fedc5a060a2096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_pixel_metrics_ppc(real_flat, simulations, alphas=(0.01, 0.05, 0.1, 0.2)):\n",
    "    \"\"\"\n",
    "    real_flat: shape (G, T)  — observed values per group/pixel\n",
    "    simulations: shape (S, G, T) — S posterior‐predictive draws per group/pixel\n",
    "    alphas: list of coverage levels for bootstrap CIs\n",
    "    Returns:\n",
    "      - coverages: dict[alpha] → array of length G\n",
    "      - r2s:    array of length G\n",
    "      - p_ppc:  array of length G   (Bayesian p-value from PPC)\n",
    "    \"\"\"\n",
    "    S, G, T = simulations.shape\n",
    "\n",
    "    # 1) R² using sklearn (unchanged)\n",
    "    print('Computing R²')\n",
    "    median_pred = np.median(simulations, axis=0)  # (G, T)\n",
    "    r2s = np.zeros(G)\n",
    "    for g in range(G):\n",
    "        y = real_flat[g]\n",
    "        f = median_pred[g]\n",
    "        r2s[g] = r2_score(y, f)\n",
    "\n",
    "    if S == 1:\n",
    "        # compute only R2, everything else is not defined\n",
    "        return r2s\n",
    "\n",
    "    # 2) Coverage with bootstrap confidence intervals (unchanged)\n",
    "    print('Computing coverages')\n",
    "    coverages = {}\n",
    "    for alpha in alphas:\n",
    "        lo = np.percentile(simulations, 100*(alpha/2), axis=0)        # shape (G, T)\n",
    "        hi = np.percentile(simulations, 100*(1-alpha/2), axis=0)\n",
    "        covered = (real_flat >= lo) & (real_flat <= hi)\n",
    "        coverages[alpha] = covered.mean()                     # mean over time and data points\n",
    "\n",
    "    # 3) Posterior‐predictive\n",
    "    print('Computing posterior‐predictive check')\n",
    "    D_obs, ppc = ppc_chi2(real_flat, simulations)\n",
    "    return r2s, coverages, ppc"
   ],
   "id": "43e67b2897b0d593",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reshape real data and simulations\n",
    "real_flat = real_data[0].reshape(grid_data*grid_data, 256)[binary_mask.flatten()]          # (G_active, T)\n",
    "sim_flat  = simulations.reshape(n_post_samples_sim, grid_data*grid_data, 256)[:, binary_mask.flatten(), :]  # (S, G_active, T)\n",
    "sim_flat_mle  = simulations_mle.reshape(1, grid_data*grid_data, 256)[:, binary_mask.flatten(), :]  # (S, G_active, T)"
   ],
   "id": "21dd8f46e8ac12a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute pixel metrics\n",
    "r2s, coverages, ppc = compute_pixel_metrics_ppc(real_flat, sim_flat)\n",
    "r2s_mle = compute_pixel_metrics_ppc(real_flat, sim_flat_mle)\n",
    "# 2/3 of the gates are sufficient to capture the whole decay, if it is in the range of 1 ns\n",
    "r2s_short, coverages_short, ppc_short = compute_pixel_metrics_ppc(real_flat[:, :int(256/3*2)], sim_flat[:, :, :int(256/3*2)])"
   ],
   "id": "be23bb8ca57e40af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print summary:\n",
    "print(f\"Mean R² over valid pixels (MLE):           {r2s_mle.mean():.3f} ({r2s_mle.std():.3f})\")\n",
    "print(f'Min/Max R²  (MLE):                         {np.min(r2s_mle):.3f}, {np.max(r2s_mle):.3f}')\n",
    "print('\\n')\n",
    "\n",
    "for a in coverages.keys():\n",
    "    print(f\"Nominal CI = {(1-a)*100:.1f}% → empirical coverage = {coverages[a]:.3f}\")\n",
    "c_keys = (1-np.array(list(coverages.keys())))*100\n",
    "c_vals = np.array(list(coverages.values()))*100\n",
    "print(f\"Mean R² over valid pixels:           {r2s.mean():.3f} ({r2s.std():.3f})\")\n",
    "print(f'Min/Max R²:                         {np.min(r2s):.3f}, {np.max(r2s):.3f}')\n",
    "print(f\"Mean PPC over valid pixels:   {ppc.mean():.3f} ({ppc.std():.3f})\")\n",
    "print('\\n')\n",
    "\n",
    "print('Using only first 2/3 of the gates:')\n",
    "for a in coverages_short.keys():\n",
    "    print(f\"Nominal CI = {(1-a)*100:.1f}% → empirical coverage = {coverages_short[a]:.3f}\")\n",
    "c_keys = (1-np.array(list(coverages_short.keys())))*100\n",
    "c_vals = np.array(list(coverages_short.values()))*100\n",
    "print(f\"Mean R² over valid pixels:           {r2s_short.mean():.3f} ({r2s_short.std():.3f})\")\n",
    "print(f'Min/Max R²:                         {np.min(r2s_short):.3f}, {np.max(r2s_short):.3f}')\n",
    "print(f\"Mean PPC over valid pixels:   {ppc_short.mean():.3f} ({ppc_short.std():.3f})\")"
   ],
   "id": "11f2df90281f9a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "r2_map = np.ones(grid_data*grid_data) * np.nan\n",
    "r2_map[binary_mask.flatten()] = r2s\n",
    "r2_map = r2_map.reshape(grid_data, grid_data)\n",
    "\n",
    "r2_map_mle = np.ones(grid_data*grid_data) * np.nan\n",
    "r2_map_mle[binary_mask.flatten()] = r2s_mle\n",
    "r2_map_mle = r2_map_mle.reshape(grid_data, grid_data)\n",
    "\n",
    "p_ppc_map = np.ones(grid_data*grid_data) * np.nan\n",
    "p_ppc_map[binary_mask.flatten()] = ppc\n",
    "p_ppc_map = p_ppc_map.reshape(grid_data, grid_data)\n",
    "\n",
    "p_ppc_map_short = np.ones(grid_data*grid_data) * np.nan\n",
    "p_ppc_map_short[binary_mask.flatten()] = ppc_short\n",
    "p_ppc_map_short = p_ppc_map_short.reshape(grid_data, grid_data)\n",
    "\n",
    "cmap = plt.get_cmap('jet').copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "cmap.set_under(color='black')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, layout='constrained', figsize=(6, 2))\n",
    "im0 = ax[0].imshow(r2_map, cmap=cmap, vmin=0.8, vmax=1)\n",
    "c0 = fig.colorbar(im0, ax=ax[0])\n",
    "c0.set_label(r'$R^2$')\n",
    "ax[0].set_title(r'Hierarchical Bayesian')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "im1 = ax[1].imshow(r2_map_mle, cmap=cmap, vmin=0.8, vmax=1)\n",
    "c1 = fig.colorbar(im1, ax=ax[1])\n",
    "c1.set_label(r'$R^2$')\n",
    "ax[1].set_title(r'MLE')\n",
    "ax[1].sharex(ax[0])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "# Define the pixel-to-micron ratio\n",
    "microns_per_pixel = 135./512\n",
    "scale_bar_length_um = 10\n",
    "scale_bar_length_px = int(scale_bar_length_um / microns_per_pixel)\n",
    "\n",
    "# Position the scale bar in the upper-right corner\n",
    "x0 = r2_map_mle.shape[1] - scale_bar_length_px - 40\n",
    "y0 = 40\n",
    "\n",
    "# Add the scale bar\n",
    "ax[0].hlines(y=y0, xmin=x0, xmax=x0 + scale_bar_length_px, color='white', linewidth=2)\n",
    "ax[1].hlines(y=y0, xmin=x0, xmax=x0 + scale_bar_length_px, color='white', linewidth=2)\n",
    "\n",
    "# Add label\n",
    "ax[0].text(x0 + scale_bar_length_px / 2, y0 - 5, f'{scale_bar_length_um} µm',\n",
    "           color='white', ha='center', va='bottom', fontsize=8)\n",
    "ax[1].text(x0 + scale_bar_length_px / 2, y0 - 5, f'{scale_bar_length_um} µm',\n",
    "           color='white', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "#plt.savefig(f'plots/real_data_fit_diagnostics.pdf', transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "92f5e54f7212e948",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 2, layout='constrained', figsize=(7, 3))\n",
    "im2 = ax[0].imshow(p_ppc_map_short, cmap=cmap)\n",
    "c2 = fig.colorbar(im2, ax=ax[0])\n",
    "c2.set_label(r'Posterior Predictive $p$‑value')\n",
    "ax[0].set_title(r'Hierarchical Bayesian')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "ax[1].hist(ppc_short, density=True)\n",
    "ax[1].set_title(r'Hierarchical Bayesian')\n",
    "ax[1].set_xlabel(r'Posterior Predictive $p$‑value')\n",
    "ax[1].set_ylabel(r'Density')\n",
    "#plt.savefig(f'plots/real_data_fit_diagnostics_appendix.pdf', transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "91e96489a11a0f04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b13fa3cc341d9181",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical-abi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
