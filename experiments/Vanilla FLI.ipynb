{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "#os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from experiments.problems.fli import FLI_Prior\n",
    "from experiments.problems import visualize_simulation_output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parameterization = ['difference', 'ratios'][0]\n",
    "prior = FLI_Prior(parameterization)\n",
    "\n",
    "def prior_bf(n_local_samples=1):\n",
    "    global_sample, _ = prior._sample_global()\n",
    "    local_sample_raw, local_sample_trans = prior._sample_local(n_local_samples=n_local_samples)\n",
    "    local_sample = local_sample_raw\n",
    "    local_sample.update(local_sample_trans)\n",
    "    #local_sample.update(global_sample)\n",
    "    return local_sample\n",
    "\n",
    "def model_bf(tau_L, tau_L_2, A_L):\n",
    "    sim = prior.simulator.decay_gen_single(tau_L, tau_L_2, A_L).flatten()\n",
    "    return dict(sim=sim)"
   ],
   "id": "f9c81ea4eeaedd3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "simulator = bf.simulators.make_simulator([prior_bf, model_bf])",
   "id": "46ea29a56f6f60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "test = simulator.sample(128)"
   ],
   "id": "5d390333dce23040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .drop('tau_L')\n",
    "    .drop('tau_L_2')\n",
    "    .drop('A_L')\n",
    "    #.drop('log_tau_L')\n",
    "    #.drop('log_delta_tau_L')\n",
    "    #.drop('a_l')\n",
    "    .to_array()\n",
    "    .convert_dtype(from_dtype=\"float64\", to_dtype=\"float32\")\n",
    "    .as_time_series(\"sim\")\n",
    "    #.concatenate([\"log_r_L\", \"log_s_L\", \"a_l\"], into=\"inference_variables\")\n",
    "    .concatenate([\"log_tau_L\", \"log_delta_tau_L\", \"a_l\"], into=\"inference_variables\")\n",
    "    #.concatenate(['log_tau_G', 'log_sigma_tau_G', 'log_delta_tau_G', 'log_delta_sigma_tau_G', 'a_mean', 'a_log_std'], into=\"inference_variables\")\n",
    "    .rename(\"sim\", \"summary_variables\")\n",
    "    .standardize(include=\"summary_variables\")\n",
    ")"
   ],
   "id": "c504bb3bd9c6aa9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "workflow = bf.BasicWorkflow(\n",
    "    adapter=adapter,\n",
    "    inference_network=bf.networks.DiffusionModel(),\n",
    "    summary_network=bf.networks.TimeSeriesNetwork(recurrent_dim=256),\n",
    "    simulator=simulator\n",
    ")\n",
    "filepath = Path(\"bf_checkpoints\") / f\"vanilla_fli_time_series_{parameterization}.keras\"\n",
    "filepath.parent.mkdir(exist_ok=True)"
   ],
   "id": "a585b04442f90e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = workflow.fit_online(epochs=100, batch_size=128, num_batches_per_epoch=100, validation_data=1000)\n",
    "workflow.approximator.save(filepath=filepath)"
   ],
   "id": "3fc2640ac8ba6726",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#workflow.approximator = keras.saving.load_model(filepath)",
   "id": "d3985b88214736c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "workflow.plot_default_diagnostics(test_data=300, calibration_ecdf_kwargs={'difference': True})",
   "id": "69154a0006f46071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Apply the Model to Real Data",
   "id": "fefd17af9d47a96f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from joblib import Parallel, delayed"
   ],
   "id": "a146116cea3c2909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "global_param_names = prior.global_param_names",
   "id": "46fee0cf18c3892a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load MLE binary map\n",
    "mle_parameters = np.load(\"problems/FLI/mle_parameters.npy\")\n",
    "tau_mean = mle_parameters[:, :, 2] * mle_parameters[:, :, 0] + (1 - mle_parameters[:, :, 2]) * mle_parameters[:, :, 1]\n",
    "mle_estimates = np.concatenate((mle_parameters[:, :, :3], tau_mean[..., np.newaxis]), axis=-1)"
   ],
   "id": "946a30daf16025a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grid_data = 512\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(grid_data * grid_data)\n",
    "\n",
    "x_offset = 0\n",
    "y_offset = 0\n",
    "binned_data = np.load('problems/FLI-all/exp_binned_data.npy')[x_offset:x_offset + grid_data, y_offset:y_offset + grid_data]\n",
    "binned_data = binned_data.reshape(1, grid_data * grid_data, 256, 1)\n",
    "\n",
    "data = np.load('problems/FLI-all/final_Data.npy')[:, x_offset:x_offset + grid_data, y_offset:y_offset + grid_data]\n",
    "data = data.reshape(1, grid_data * grid_data, 256, 1)\n",
    "cut_off = 17\n",
    "binary_mask = (np.sum(data, axis=2, keepdims=True) > cut_off)\n",
    "binary_mask = binary_mask.flatten() & (mle_estimates[:, :, 0] != 0).flatten() & (mle_estimates[:, :, 1] != 0).flatten()  # binary mask sets estimates to 0\n",
    "binary_mask = binary_mask.reshape(1, binned_data.shape[1], 1, 1)\n",
    "\n",
    "real_data = binned_data\n",
    "norm = np.max(real_data, axis=2, keepdims=True)\n",
    "norm[~binary_mask] = 1\n",
    "real_data = real_data / norm\n",
    "\n",
    "plt.imshow(np.sum(data[0], axis=(1,2)).reshape(grid_data, grid_data), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ],
   "id": "4b540fbd292a1b1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_samples = 100\n",
    "chunk_size = 100\n",
    "\n",
    "posterior_samples_real = {'log_tau_L': [], 'log_delta_tau_L': [], 'a_l': []}\n",
    "for start_idx in tqdm(range(0, grid_data**2, chunk_size)):\n",
    "    end_idx = min(start_idx + chunk_size, grid_data**2)\n",
    "    posterior_samples_chunk = workflow.sample(conditions={'sim': real_data[0, start_idx:end_idx, :, 0]}, num_samples=num_samples)\n",
    "\n",
    "    for k in posterior_samples_real.keys():\n",
    "        posterior_samples_real[k].append(posterior_samples_chunk[k])\n",
    "\n",
    "for k in posterior_samples_real.keys():\n",
    "    posterior_samples_real[k] = np.concatenate(posterior_samples_real[k])"
   ],
   "id": "a5535a36eff010f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tau, tau_2, A = prior.transform_raw_params(\n",
    "    log_tau=posterior_samples_real['log_tau_L'].T[0].reshape(num_samples, grid_data, grid_data),\n",
    "    log_delta_tau=posterior_samples_real['log_delta_tau_L'].T[0].reshape(num_samples, grid_data, grid_data),\n",
    "    a=posterior_samples_real['a_l'].T[0].reshape(num_samples, grid_data, grid_data),\n",
    ")\n",
    "ps = np.concatenate([tau[:, :, :, np.newaxis], tau_2[:, :, :, np.newaxis], A[:, :, :, np.newaxis]], axis=-1)\n",
    "transf_local_param_names = [r'$\\tau_1^L$', r'$\\tau_2^L$', r'$A^L$']\n",
    "\n",
    "med = np.median(ps, axis=0)\n",
    "posterior_mad = mad(ps, axis=0)\n",
    "visualize_simulation_output(med,\n",
    "                            mask=binary_mask.reshape(grid_data, grid_data),\n",
    "                            title_prefix=['Posterior Median ' + p for p in transf_local_param_names],\n",
    "                            cmap='turbo', scales=[(0,1), (0, 2), (0,1)])\n",
    "visualize_simulation_output(posterior_mad,\n",
    "                            mask=binary_mask.reshape(grid_data, grid_data),\n",
    "                            title_prefix=['Posterior MAD ' + p for p in transf_local_param_names],\n",
    "                            cmap='turbo', scales=[(0,1), (0, 2), (0,1)])"
   ],
   "id": "7e0f6b142afd354c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axis = plt.subplots(1, 5, figsize=(10, 3), tight_layout=True, sharex=True, sharey=True)\n",
    "axis = axis.flatten()\n",
    "for ax in axis:\n",
    "    while True:\n",
    "        pixel_ids = [np.random.randint(0, grid_data), np.random.randint(0, grid_data)]\n",
    "        if binary_mask.reshape(grid_data, grid_data)[pixel_ids[0], pixel_ids[1]]:\n",
    "            break  # only plot meaningful data\n",
    "    plot_index = np.random.randint(0, tau.shape[0])\n",
    "\n",
    "    simulations = np.array([\n",
    "        prior.simulator.decay_gen_single(\n",
    "            tau_L=tau[post_index, pixel_ids[0], pixel_ids[1]],\n",
    "            tau_L_2=tau_2[post_index, pixel_ids[0], pixel_ids[1]],\n",
    "            A_L=A[post_index, pixel_ids[0], pixel_ids[1]]\n",
    "        ) for post_index in range(tau.shape[0])\n",
    "    ])\n",
    "\n",
    "    ax.plot(real_data.reshape(grid_data, grid_data, 256)[pixel_ids[0], pixel_ids[1]], label='data')\n",
    "    ax.plot(np.median(simulations, axis=0), label='posterior median', alpha=0.8, color='orange')\n",
    "    ax.fill_between(\n",
    "        np.arange(simulations.shape[1]),\n",
    "        np.quantile(simulations, 0.025, axis=0),\n",
    "        np.quantile(simulations, 0.975, axis=0),\n",
    "        alpha=0.4,\n",
    "        color='orange',\n",
    "        label='posterior 95% CI'\n",
    "    )\n",
    "    ax.set_xlabel('Time')\n",
    "axis[0].set_ylabel('Normalized Photon Count')\n",
    "fig.legend(labels=['data', 'posterior median', 'posterior 95% CI'], bbox_to_anchor=(0.5, -0.07),\n",
    "           ncol=3, loc='lower center')\n",
    "plt.show()"
   ],
   "id": "d24c9d62e15241ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_post_samples_sim = 20\n",
    "tau_mle = mle_estimates[:, :, 0]\n",
    "tau_2_mle = mle_estimates[:, :, 1]\n",
    "A_mle = mle_estimates[:, :, 2]\n",
    "\n",
    "@delayed\n",
    "def wrapper(pixel_i):\n",
    "    _simulations = np.ones((n_post_samples_sim, 1, grid_data, 256)) * np.nan\n",
    "    _simulations_mle = np.ones((1, 1, grid_data, 256)) * np.nan\n",
    "    for pixel_j in range(grid_data):\n",
    "        if not binary_mask.reshape(grid_data, grid_data)[pixel_i, pixel_j]:\n",
    "            continue  # not a valid pixel\n",
    "        _simulations[:, 0, pixel_j, :] = [\n",
    "            prior.simulator.decay_gen_single(\n",
    "                tau_L=tau[post_index, pixel_i, pixel_j],\n",
    "                tau_L_2=tau_2[post_index, pixel_i, pixel_j],\n",
    "                A_L=A[post_index, pixel_i, pixel_j]\n",
    "            ) for post_index in range(n_post_samples_sim)\n",
    "        ]\n",
    "        _simulations_mle[:, 0, pixel_j, :] = [\n",
    "            prior.simulator.decay_gen_single(\n",
    "                tau_L=tau_mle[pixel_i, pixel_j],\n",
    "                tau_L_2=tau_2_mle[pixel_i, pixel_j],\n",
    "                A_L=A_mle[pixel_i, pixel_j]\n",
    "            )\n",
    "        ]\n",
    "    return _simulations, _simulations_mle\n",
    "\n",
    "simulations = Parallel(n_jobs=10, verbose=1)(wrapper(pixel_i) for pixel_i in range(grid_data))\n",
    "simulations_mle = np.concatenate([s[1] for s in simulations], axis=1)  # shape: (1, grid_data, grid_data, 256)\n",
    "simulations = np.concatenate([s[0] for s in simulations], axis=1)  # shape: (n_post_samples, grid_data, grid_data, 256)"
   ],
   "id": "a5fe58a3a808c17a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def ppc_chi2(real_flat, simulations):\n",
    "    \"\"\"\n",
    "    real_flat: array of shape (G, T) — observed values per group/pixel\n",
    "    simulations: array of shape (S, G, T) — posterior‐predictive draws\n",
    "    Returns:\n",
    "      D_obs:  array of shape (G,)    — observed discrepancies\n",
    "      p_ppc:  array of shape (G,)    — Bayesian p-values\n",
    "    \"\"\"\n",
    "    # 1. Compute summary f_t and empirical variance Var_t\n",
    "    f = np.median(simulations, axis=0)                # shape (G, T)\n",
    "    var_t = np.var(simulations, axis=0, ddof=1)       # shape (G, T)\n",
    "\n",
    "    # in tails simulations can be very small\n",
    "    var_t[var_t < 1e-10] = 1e-10                       # avoid division by zero\n",
    "\n",
    "    # 2. Discrepancy for observed data\n",
    "    D_obs = np.sum((real_flat - f)**2 / var_t, axis=1)  # shape (G,)\n",
    "\n",
    "    # 3. Discrepancies for replicated data\n",
    "    #    D_rep[s, g] = sum_t (simulations[s,g,t] - f[g,t])**2 / var_t[g,t]\n",
    "    diffs = (simulations - f[None, :, :])**2 / var_t[None, :, :]\n",
    "    D_rep = np.sum(diffs, axis=2)                      # shape (S, G)\n",
    "\n",
    "    # 4. Bayesian p-values\n",
    "    p_ppc = np.mean(D_rep >= D_obs[None, :], axis=0)   # shape (G,)\n",
    "\n",
    "    return D_obs, p_ppc"
   ],
   "id": "8c87df6999b4be5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_pixel_metrics_ppc(real_flat, simulations, alphas=(0.01, 0.05, 0.1, 0.2)):\n",
    "    \"\"\"\n",
    "    real_flat: shape (G, T)  — observed values per group/pixel\n",
    "    simulations: shape (S, G, T) — S posterior‐predictive draws per group/pixel\n",
    "    alphas: list of coverage levels for bootstrap CIs\n",
    "    Returns:\n",
    "      - coverages: dict[alpha] → array of length G\n",
    "      - r2s:    array of length G\n",
    "      - p_ppc:  array of length G   (Bayesian p-value from PPC)\n",
    "    \"\"\"\n",
    "    S, G, T = simulations.shape\n",
    "\n",
    "    # 1) R² using sklearn (unchanged)\n",
    "    print('Computing R²')\n",
    "    median_pred = np.median(simulations, axis=0)  # (G, T)\n",
    "    r2s = np.zeros(G)\n",
    "    for g in range(G):\n",
    "        y = real_flat[g]\n",
    "        f = median_pred[g]\n",
    "        r2s[g] = r2_score(y, f)\n",
    "\n",
    "    if S == 1:\n",
    "        # compute only R2, everything else is not defined\n",
    "        return r2s\n",
    "\n",
    "    # 2) Coverage with bootstrap confidence intervals (unchanged)\n",
    "    print('Computing coverages')\n",
    "    coverages = {}\n",
    "    for alpha in alphas:\n",
    "        lo = np.percentile(simulations, 100*(alpha/2), axis=0)        # shape (G, T)\n",
    "        hi = np.percentile(simulations, 100*(1-alpha/2), axis=0)\n",
    "        covered = (real_flat >= lo) & (real_flat <= hi)\n",
    "        coverages[alpha] = covered.mean()                     # mean over time and data points\n",
    "\n",
    "    # 3) Posterior‐predictive\n",
    "    print('Computing posterior‐predictive check')\n",
    "    D_obs, ppc = ppc_chi2(real_flat, simulations)\n",
    "    return r2s, coverages, ppc"
   ],
   "id": "88e8deeb76a12c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reshape real data and simulations\n",
    "real_flat = real_data[0].reshape(grid_data*grid_data, 256)[binary_mask.flatten()]          # (G_active, T)\n",
    "sim_flat  = simulations.reshape(n_post_samples_sim, grid_data*grid_data, 256)[:, binary_mask.flatten(), :]  # (S, G_active, T)\n",
    "sim_flat_mle  = simulations_mle.reshape(1, grid_data*grid_data, 256)[:, binary_mask.flatten(), :]  # (S, G_active, T)"
   ],
   "id": "954f1da226bcdbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute pixel metrics\n",
    "r2s, coverages, ppc = compute_pixel_metrics_ppc(real_flat, sim_flat)\n",
    "r2s_mle = compute_pixel_metrics_ppc(real_flat, sim_flat_mle)\n",
    "# 2/3 of the gates are sufficient to capture the whole decay, if it is in the range of 1 ns\n",
    "r2s_short, coverages_short, ppc_short = compute_pixel_metrics_ppc(real_flat[:, :int(256/3*2)], sim_flat[:, :, :int(256/3*2)])"
   ],
   "id": "64f0b0bd675b71b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print summary:\n",
    "print(f\"Mean R² over valid pixels (MLE):           {r2s_mle.mean():.3f} ({r2s_mle.std():.3f})\")\n",
    "print(f'Min/Max R²  (MLE):                         {np.min(r2s_mle):.3f}, {np.max(r2s_mle):.3f}')\n",
    "print('\\n')\n",
    "\n",
    "for a in coverages.keys():\n",
    "    print(f\"Nominal CI = {(1-a)*100:.1f}% → empirical coverage = {coverages[a]:.3f}\")\n",
    "c_keys = (1-np.array(list(coverages.keys())))*100\n",
    "c_vals = np.array(list(coverages.values()))*100\n",
    "print(f\"Mean R² over valid pixels:           {r2s.mean():.3f} ({r2s.std():.3f})\")\n",
    "print(f'Min/Max R²:                         {np.min(r2s):.3f}, {np.max(r2s):.3f}')\n",
    "print(f\"Mean PPC over valid pixels:   {ppc.mean():.3f} ({ppc.std():.3f})\")\n",
    "print('\\n')\n",
    "\n",
    "print('Using only first 2/3 of the gates:')\n",
    "for a in coverages_short.keys():\n",
    "    print(f\"Nominal CI = {(1-a)*100:.1f}% → empirical coverage = {coverages_short[a]:.3f}\")\n",
    "c_keys = (1-np.array(list(coverages_short.keys())))*100\n",
    "c_vals = np.array(list(coverages_short.values()))*100\n",
    "print(f\"Mean R² over valid pixels:           {r2s_short.mean():.3f} ({r2s_short.std():.3f})\")\n",
    "print(f'Min/Max R²:                         {np.min(r2s_short):.3f}, {np.max(r2s_short):.3f}')\n",
    "print(f\"Mean PPC over valid pixels:   {ppc_short.mean():.3f} ({ppc_short.std():.3f})\")"
   ],
   "id": "181010f2dba7ddf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "r2_map = np.ones(grid_data*grid_data) * np.nan\n",
    "r2_map[binary_mask.flatten()] = r2s\n",
    "r2_map = r2_map.reshape(grid_data, grid_data)\n",
    "\n",
    "r2_map_mle = np.ones(grid_data*grid_data) * np.nan\n",
    "r2_map_mle[binary_mask.flatten()] = r2s_mle\n",
    "r2_map_mle = r2_map_mle.reshape(grid_data, grid_data)\n",
    "\n",
    "p_ppc_map = np.ones(grid_data*grid_data) * np.nan\n",
    "p_ppc_map[binary_mask.flatten()] = ppc\n",
    "p_ppc_map = p_ppc_map.reshape(grid_data, grid_data)\n",
    "\n",
    "p_ppc_map_short = np.ones(grid_data*grid_data) * np.nan\n",
    "p_ppc_map_short[binary_mask.flatten()] = ppc_short\n",
    "p_ppc_map_short = p_ppc_map_short.reshape(grid_data, grid_data)\n",
    "\n",
    "cmap = plt.get_cmap('jet').copy()\n",
    "cmap.set_bad(color=\"black\")\n",
    "cmap.set_under(color='black')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, layout='constrained', figsize=(6, 2))\n",
    "im0 = ax[0].imshow(r2_map, cmap=cmap, vmin=0.8, vmax=1)\n",
    "c0 = fig.colorbar(im0, ax=ax[0])\n",
    "c0.set_label(r'$R^2$')\n",
    "ax[0].set_title(r'Flat Bayesian')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "\n",
    "im1 = ax[1].imshow(r2_map_mle, cmap=cmap, vmin=0.8, vmax=1)\n",
    "c1 = fig.colorbar(im1, ax=ax[1])\n",
    "c1.set_label(r'$R^2$')\n",
    "ax[1].set_title(r'MLE')\n",
    "ax[1].sharex(ax[0])\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([])\n",
    "\n",
    "# Define the pixel-to-micron ratio\n",
    "microns_per_pixel = 135./512\n",
    "scale_bar_length_um = 10\n",
    "scale_bar_length_px = int(scale_bar_length_um / microns_per_pixel)\n",
    "\n",
    "# Position the scale bar in the upper-right corner\n",
    "x0 = r2_map_mle.shape[1] - scale_bar_length_px - 40\n",
    "y0 = 40\n",
    "\n",
    "# Add the scale bar\n",
    "ax[0].hlines(y=y0, xmin=x0, xmax=x0 + scale_bar_length_px, color='white', linewidth=2)\n",
    "ax[1].hlines(y=y0, xmin=x0, xmax=x0 + scale_bar_length_px, color='white', linewidth=2)\n",
    "\n",
    "# # Add label\n",
    "# ax[0].text(x0 + scale_bar_length_px / 2, y0 - 5, f'{scale_bar_length_um} µm',\n",
    "#            color='white', ha='center', va='bottom', fontsize=8)\n",
    "# ax[1].text(x0 + scale_bar_length_px / 2, y0 - 5, f'{scale_bar_length_um} µm',\n",
    "#            color='white', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "#plt.savefig(f'plots/vanilla_real_data_fit_diagnostics.pdf', transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "3a73449511e23d90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff173c73e0b685dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
