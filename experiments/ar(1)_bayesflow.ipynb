{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d206b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "import bayesflow as bf\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fad46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIME_POINTS = 5\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, sigma_noise=0.1):\n",
    "        \"\"\"\n",
    "        Simulator for the hierarchical AR(1) model:\n",
    "            y[t] = alpha + beta y[t-1] + noise[t]\n",
    "        starting from an initial value.\n",
    "\n",
    "        Parameters:\n",
    "            sigma_noise (float): noise standard deviation.\n",
    "        \"\"\"\n",
    "        self.sigma_noise = sigma_noise\n",
    "        self.initial_is_zero = False\n",
    "\n",
    "    def __call__(self, params, n_time_points=N_TIME_POINTS):\n",
    "        eta = np.array(params['eta'])\n",
    "        alpha = np.array(params['alpha'])\n",
    "        N = eta.size\n",
    "        if eta.ndim > 1:\n",
    "            raise ValueError(\"eta must be a 1D array.\")\n",
    "\n",
    "        # Generate noise for the increments: shape (N, n_time_points)\n",
    "        noise = np.random.normal(\n",
    "            loc=0,\n",
    "            scale=self.sigma_noise,\n",
    "            size=(N, n_time_points)\n",
    "        )\n",
    "\n",
    "        # Initialize trajectories with the initial condition\n",
    "        traj = np.zeros((N, n_time_points))\n",
    "\n",
    "        # Simulate the AR(1) process for each trajectory and each batch\n",
    "        if not self.initial_is_zero:\n",
    "            traj[:, 0] = noise[:, 0]\n",
    "        for t in range(1, n_time_points):\n",
    "            traj[:, t] = alpha + traj[:, t - 1] * eta + noise[:, t]\n",
    "\n",
    "        return dict(observable=traj)\n",
    "\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Hierarchical prior for the AR(1) model.\n",
    "        \"\"\"\n",
    "        self.alpha_mean = 0\n",
    "        self.alpha_std = 1\n",
    "        self.beta_mean = 0\n",
    "        self.beta_std = 1 #0.1\n",
    "        self.log_sigma_mean = 0 #np.log(0.1)\n",
    "        self.log_sigma_std = 1 #0.5\n",
    "        self.n_params_global = 3\n",
    "        self.n_params_local = 1\n",
    "        self.global_param_names = [r'$\\alpha$', r'$\\beta$', r'$\\log \\sigma$']\n",
    "\n",
    "        # Build prior parameters as tensors.\n",
    "        self.hyper_prior_means = np.array(\n",
    "            [self.alpha_mean,\n",
    "             self.beta_mean,\n",
    "             self.log_sigma_mean],\n",
    "        )\n",
    "        self.hyper_prior_stds = np.array(\n",
    "            [self.alpha_std,\n",
    "             self.beta_std,\n",
    "             self.log_sigma_std],\n",
    "        )\n",
    "\n",
    "        np.random.seed(0)\n",
    "        self.simulator = Simulator()\n",
    "\n",
    "        # Compute normalization constants\n",
    "        test = self.sample(1000)\n",
    "        self.norm_x_mean = np.mean(test['data'])\n",
    "        self.norm_x_std = np.std(test['data'])\n",
    "        self.norm_prior_global_mean = np.mean(test['global_params'], axis=0)\n",
    "        self.norm_prior_global_std = np.std(test['global_params'], axis=0)\n",
    "        self.norm_prior_local_mean = np.mean(test['local_params_raw'], axis=0)\n",
    "        self.norm_prior_local_std = np.std(test['local_params_raw'], axis=0)\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample(batch_size=batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_local_param_names(n_local_samples):\n",
    "        return [r'$\\eta_{' + str(i) + '}$' for i in range(n_local_samples)]\n",
    "\n",
    "    def _sample_global(self):\n",
    "        # Sample global parameters\n",
    "        self.alpha = np.random.normal(loc=self.alpha_mean, scale=self.alpha_std)\n",
    "        self.beta = np.random.normal(loc=self.beta_mean, scale=self.beta_std)\n",
    "        self.log_sigma = np.random.normal(loc=self.log_sigma_mean, scale=self.log_sigma_std)\n",
    "        return dict(alpha=self.alpha, beta=self.beta, log_sigma=self.log_sigma)\n",
    "\n",
    "    def _sample_local(self, n_local_samples=1):\n",
    "        # Sample local parameters\n",
    "        eta_raw = np.random.normal(loc=0, scale=np.exp(self.log_sigma), size=n_local_samples)\n",
    "        eta = self.transform_local_params(beta=self.beta, eta_raw=eta_raw)\n",
    "        return dict(eta=eta, eta_raw=eta_raw)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_local_params(beta, eta_raw):\n",
    "        # transform raw local parameters\n",
    "        return 2*expit(beta + eta_raw)-1\n",
    "\n",
    "    @staticmethod\n",
    "    def back_transform_local_params(local_params):\n",
    "        local_params_raw = logit((local_params + 1) / 2)\n",
    "        local_params_raw[local_params_raw < -100] = -100\n",
    "        local_params_raw[local_params_raw > 100] = 100\n",
    "        return local_params_raw\n",
    "\n",
    "    def sample(self, batch_size, n_local_samples=1, n_time_points=N_TIME_POINTS, get_grid=False):\n",
    "        # Sample global and local parameters and simulate data\n",
    "        global_params = np.zeros((batch_size, self.n_params_global))\n",
    "        local_params_raw = np.zeros((batch_size, n_local_samples))\n",
    "        local_params = np.zeros((batch_size, n_local_samples))\n",
    "        data = np.zeros((batch_size, n_local_samples, n_time_points))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            global_sample = self._sample_global()\n",
    "            local_sample = self._sample_local(n_local_samples=n_local_samples)\n",
    "            sim_dict = {'alpha': global_sample['alpha'], 'eta': local_sample['eta']}\n",
    "            sim = self.simulator(sim_dict, n_time_points=n_time_points)\n",
    "\n",
    "            global_params[i] = [global_sample['alpha'], global_sample['beta'], global_sample['log_sigma']]\n",
    "            local_params_raw[i] = local_sample['eta_raw']\n",
    "            local_params[i] = local_sample['eta']\n",
    "            data[i] = sim['observable']\n",
    "\n",
    "        # Convert to tensors\n",
    "        global_params = np.array(global_params)\n",
    "        local_params = np.array(local_params)\n",
    "        local_params_raw = np.array(local_params_raw)\n",
    "\n",
    "        data = np.array(data)\n",
    "        if get_grid:\n",
    "            grid_size = int(np.sqrt(n_local_samples))\n",
    "            data = data[:, :grid_size ** 2]\n",
    "            data = data.reshape(batch_size, n_time_points, grid_size, grid_size)\n",
    "            local_params = local_params[:, :grid_size ** 2]\n",
    "            local_params_raw = local_params_raw[:, :grid_size ** 2]\n",
    "            local_params = local_params.reshape(batch_size, grid_size, grid_size)\n",
    "            local_params_raw = local_params_raw.reshape(batch_size, grid_size, grid_size)\n",
    "        return dict(global_params=global_params, local_params=local_params,\n",
    "                    local_params_raw=local_params_raw, data=data)\n",
    "\n",
    "    def normalize_theta(self, theta, global_params):\n",
    "        if global_params:\n",
    "            return (theta - self.norm_prior_global_mean) / self.norm_prior_global_std\n",
    "        return (theta - self.norm_prior_local_mean) / self.norm_prior_local_std\n",
    "\n",
    "    def denormalize_theta(self, theta, global_params):\n",
    "        if global_params:\n",
    "            return theta * self.norm_prior_global_std + self.norm_prior_global_mean\n",
    "        return theta * self.norm_prior_local_std + self.norm_prior_local_mean\n",
    "\n",
    "    def normalize_data(self, x):\n",
    "        return (x - self.norm_x_mean) / self.norm_x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ad5aa",
   "metadata": {},
   "source": [
    "## Experiment 1: 4 x 4 Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2affb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior()\n",
    "sim = Simulator()\n",
    "\n",
    "# 10000 data points (same as compositional)\n",
    "\n",
    "train_dict = prior.sample(\n",
    "    batch_size=625,\n",
    "    n_local_samples=16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "train_dict['global_params'] = prior.normalize_theta(train_dict['global_params'], global_params=True)\n",
    "train_dict['data'] = prior.normalize_data(train_dict['data'])\n",
    "\n",
    "val_dict = prior.sample(\n",
    "    batch_size=300,\n",
    "    n_local_samples=16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "val_dict['global_params'] = prior.normalize_theta(val_dict['global_params'], global_params=True)\n",
    "val_dict['data'] = prior.normalize_data(val_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e3af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_post_samples = 100\n",
    "\n",
    "test_dict = prior.sample(\n",
    "    batch_size=100,\n",
    "    n_local_samples=16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "test_dict['global_params'] = prior.normalize_theta(test_dict['global_params'], global_params=True)\n",
    "test_dict['data'] = prior.normalize_data(test_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a74276",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .drop([\"local_params\", \"local_params_raw\"])\n",
    "    .rename(\"global_params\", \"inference_variables\")\n",
    "    .rename(\"data\", \"summary_variables\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18cc5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"fm\": (bf.networks.FlowMatching, {}),\n",
    "    \"coupling\": (bf.networks.CouplingFlow, {\n",
    "        \"transform\": \"spline\",\n",
    "        \"depth\": 2\n",
    "    }),\n",
    "    \"dm_cosine_F\": (bf.networks.DiffusionModel, {\n",
    "        \"noise_schedule\": \"cosine\", \n",
    "        \"prediction_type\": \"F\", }),\n",
    "    \"dm_cosine_v\": (bf.networks.DiffusionModel, {\n",
    "        \"noise_schedule\": \"cosine\", \n",
    "        \"prediction_type\": \"velocity\"}),   \n",
    "    \"dm_cosine_noise\": (bf.networks.DiffusionModel, {\n",
    "        \"noise_schedule\": \"cosine\", \n",
    "        \"prediction_type\": \"noise\"}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model_name, model_packet in models.items():\n",
    "\n",
    "    results[model_name] = None\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "        adapter=adapter,\n",
    "        summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),\n",
    "        inference_network=model_packet[0](**model_packet[1]),\n",
    "        checkpoint_filepath=f\"bf_checkpoints/{model_name}_16\"\n",
    "    )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=32, epochs=500 if model_name != \"coupling\" else 100, validation_data=test_dict)\n",
    "\n",
    "    test_global_samples = workflow_global.sample(conditions=test_dict, num_samples=n_post_samples)\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"RMSE\": bf.diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict)['values'].mean(),\n",
    "        \"ECE\": bf.diagnostics.metrics.calibration_error(test_global_samples, test_dict)['values'].mean(),\n",
    "        \"PC\": bf.diagnostics.metrics.posterior_contraction(test_global_samples, test_dict)['values'].mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25cb81",
   "metadata": {},
   "source": [
    "## Experiment on 16 x 16 Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed07c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 data points (same as compositional)\n",
    "\n",
    "train_dict = prior.sample(\n",
    "    batch_size=40,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "train_dict['global_params'] = prior.normalize_theta(train_dict['global_params'], global_params=True)\n",
    "train_dict['data'] = prior.normalize_data(train_dict['data'])\n",
    "\n",
    "val_dict = prior.sample(\n",
    "    batch_size=300,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "val_dict['global_params'] = prior.normalize_theta(val_dict['global_params'], global_params=True)\n",
    "val_dict['data'] = prior.normalize_data(val_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_results = {}\n",
    "\n",
    "for model_name, model_packet in models.items():\n",
    "\n",
    "    larger_results[model_name] = None\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "        adapter=adapter,\n",
    "        summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),\n",
    "        inference_network=model_packet[0](**model_packet[1]),\n",
    "        checkpoint_filepath=f\"bf_checkpoints/{model_name}_256\"\n",
    "    )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=16, epochs=200 if model_name != \"coupling\" else 100, validation_data=test_dict)\n",
    "\n",
    "    test_global_samples = workflow_global.sample(conditions=test_dict, num_samples=n_post_samples)\n",
    "\n",
    "    larger_results[model_name] = {\n",
    "        \"RMSE\": bf.diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict)['values'].mean(),\n",
    "        \"ECE\": bf.diagnostics.metrics.calibration_error(test_global_samples, test_dict)['values'].mean(),\n",
    "        \"PC\": bf.diagnostics.metrics.posterior_contraction(test_global_samples, test_dict)['values'].mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1326915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our results (from paper)\n",
    "\n",
    "results[\"Ours\"] = {\n",
    "    \"RMSE\": 0.09,\n",
    "    \"PC\": 0.97\n",
    "}\n",
    "\n",
    "larger_results[\"Ours\"] = {\n",
    "    \"RMSE\": 0.08,\n",
    "    \"PC\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646abf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    \"4x4\": results,\n",
    "    \"16x16\": larger_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d139739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- build a compact table: size Ã— method ----\n",
    "rows = {}\n",
    "\n",
    "for size, methods in all_results.items():\n",
    "    row = {}\n",
    "    for method, metrics in methods.items():\n",
    "        rmse = metrics['RMSE']\n",
    "        pc = metrics['PC']\n",
    "        row[method] = f\"{rmse:.4f} | {pc:.3f}\"\n",
    "    rows[size] = row\n",
    "\n",
    "df = pd.DataFrame.from_dict(rows, orient='index')\n",
    "df.index.name = \"size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43d05ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fm</th>\n",
       "      <th>coupling</th>\n",
       "      <th>dm_cosine_F</th>\n",
       "      <th>dm_cosine_v</th>\n",
       "      <th>dm_cosine_noise</th>\n",
       "      <th>Ours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4x4</th>\n",
       "      <td>0.0900 | 0.929</td>\n",
       "      <td>0.1082 | 0.865</td>\n",
       "      <td>0.0981 | 0.905</td>\n",
       "      <td>0.0952 | 0.925</td>\n",
       "      <td>0.1017 | 0.899</td>\n",
       "      <td>0.0900 | 0.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16x16</th>\n",
       "      <td>0.1966 | 0.497</td>\n",
       "      <td>0.1954 | 0.541</td>\n",
       "      <td>0.2347 | 0.309</td>\n",
       "      <td>0.2184 | 0.458</td>\n",
       "      <td>0.6158 | 0.000</td>\n",
       "      <td>0.0800 | 1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   fm        coupling     dm_cosine_F     dm_cosine_v  \\\n",
       "size                                                                    \n",
       "4x4    0.0900 | 0.929  0.1082 | 0.865  0.0981 | 0.905  0.0952 | 0.925   \n",
       "16x16  0.1966 | 0.497  0.1954 | 0.541  0.2347 | 0.309  0.2184 | 0.458   \n",
       "\n",
       "      dm_cosine_noise            Ours  \n",
       "size                                   \n",
       "4x4    0.1017 | 0.899  0.0900 | 0.970  \n",
       "16x16  0.6158 | 0.000  0.0800 | 1.000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c4d74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"bf_results/metrics.csv\", sep=\";\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee20403",
   "metadata": {},
   "source": [
    "### Larger budget (64 x 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "316de892",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = prior.sample(\n",
    "    batch_size=400,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "train_dict['global_params'] = prior.normalize_theta(train_dict['global_params'], global_params=True)\n",
    "train_dict['data'] = prior.normalize_data(train_dict['data'])\n",
    "\n",
    "val_dict = prior.sample(\n",
    "    batch_size=300,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "val_dict['global_params'] = prior.normalize_theta(val_dict['global_params'], global_params=True)\n",
    "val_dict['data'] = prior.normalize_data(val_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_results2 = {}\n",
    "\n",
    "for model_name, model_packet in models.items():\n",
    "\n",
    "    larger_results2[model_name] = None\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "        adapter=adapter,\n",
    "        summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),\n",
    "        inference_network=model_packet[0](**model_packet[1]),\n",
    "        checkpoint_filepath=f\"bf_checkpoints/{model_name}_256_bigS\"\n",
    "    )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=16, epochs=200 if model_name != \"coupling\" else 100, validation_data=test_dict)\n",
    "\n",
    "    test_global_samples = workflow_global.sample(conditions=test_dict, num_samples=n_post_samples)\n",
    "\n",
    "    larger_results2[model_name] = {\n",
    "        \"RMSE\": bf.diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict)['values'].mean(),\n",
    "        \"ECE\": bf.diagnostics.metrics.calibration_error(test_global_samples, test_dict)['values'].mean(),\n",
    "        \"PC\": bf.diagnostics.metrics.posterior_contraction(test_global_samples, test_dict)['values'].mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1814579",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_results2[\"Ours\"] = {\n",
    "    \"RMSE\": 0.08,\n",
    "    \"PC\": 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec026cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results2 = {\n",
    "    \"4x4\": results,\n",
    "    \"16x16\": larger_results,\n",
    "    \"16x16 (x10)\": larger_results2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "354b18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_sig(x, sig=2):\n",
    "    return float(f\"{x:.2g}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for size, methods in all_results2.items():\n",
    "    row = {\"size\": size}\n",
    "    for method, metrics in methods.items():\n",
    "        rmse = round_sig(float(metrics[\"RMSE\"]))\n",
    "        pc   = round_sig(float(metrics[\"PC\"]))\n",
    "        row[(method, \"RMSE\")] = rmse\n",
    "        row[(method, \"PC\")]   = pc\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows).set_index(\"size\")\n",
    "df.columns = pd.MultiIndex.from_tuples(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71162f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"bf_results/metrics.csv\", sep=\";\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aade3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
