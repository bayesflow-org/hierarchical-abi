{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hierarchical AR(1) inference with ABI\n",
    "\n",
    "We evaluate the coupling flow version (Heinrich et al., 2024, Habermann et al., 2024) and the v-prediction DM used for our compositional backbones."
   ],
   "id": "b260d46b78535dea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation as mad\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "from bayesflow import diagnostics\n",
    "\n",
    "from problems import AR1GridPrior, AR1GridSimulator"
   ],
   "id": "28734558c6dd25fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grid_size = [4*4, 16*16][0]\n",
    "grid_size"
   ],
   "id": "e2b2444b067cd702",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = AR1GridPrior()\n",
    "sim = AR1GridSimulator()\n",
    "\n",
    "# Match budget of compositional\n",
    "train_dict = prior.sample(\n",
    "    batch_size=10000 // grid_size,\n",
    "    n_local_samples=grid_size,\n",
    ")\n",
    "train_dict['global_params'] = keras.ops.convert_to_numpy(prior.normalize_theta(train_dict['global_params'], global_params=True))\n",
    "train_dict['data'] = keras.ops.convert_to_numpy(prior.normalize_data(train_dict['data']))\n",
    "train_dict['global_params'].shape"
   ],
   "id": "18309bb331bb36f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2affb875",
   "metadata": {},
   "source": [
    "test_dict = prior.sample(\n",
    "    batch_size=100,\n",
    "    n_local_samples=grid_size,\n",
    ")\n",
    "\n",
    "test_dict['global_params'] = keras.ops.convert_to_numpy(prior.normalize_theta(test_dict['global_params'], global_params=True))\n",
    "test_dict['data'] = keras.ops.convert_to_numpy(prior.normalize_data(test_dict['data']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9a74276",
   "metadata": {},
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .rename(\"global_params\", \"inference_variables\")\n",
    "    .rename(\"data\", \"summary_variables\")\n",
    "    .keep([\"inference_variables\", \"summary_variables\"])\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18cc5ea2",
   "metadata": {},
   "source": [
    "models = {\n",
    "    \"coupling\": (bf.networks.CouplingFlow, {\n",
    "        \"transform\": \"spline\",\n",
    "        \"depth\": 2\n",
    "    }),\n",
    "    \"dm_cosine_v\": (bf.networks.DiffusionModel, {\n",
    "        \"subnet_kwargs\": {'widths': (256, 256, 256, 256, 256), 'dropout': 0.1},\n",
    "        \"noise_schedule\": \"cosine\",\n",
    "        \"schedule_kwargs\": {\"weighting\": \"likelihood_weighting\"},\n",
    "        \"prediction_type\": \"velocity\",\n",
    "        \"integration_kwargs\": {\"method\": \"euler_maruyama\", \"steps\": 300},\n",
    "    })\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "356e2c04",
   "metadata": {},
   "source": [
    "for model_name, model_packet in models.items():\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "            adapter=adapter,\n",
    "            summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),  # shallow summary net\n",
    "            inference_network=model_packet[0](**model_packet[1]),\n",
    "            #checkpoint_filepath=f\"bf_checkpoints/ar1_{model_name}_{grid_size}\",\n",
    "            standardize=None\n",
    "        )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=32, epochs=1000 if model_name != \"coupling\" else 100, verbose=2\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    test_global_samples = workflow_global.sample(conditions=test_dict, num_samples=1000)\n",
    "    global_rmse = diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict,\n",
    "                                                              aggregation=np.median)['values'].mean().round(2)\n",
    "    global_rmse_mad = diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict,\n",
    "                                                                  aggregation=mad)['values'].mean().round(2)\n",
    "    print('Global RMSE:', global_rmse, global_rmse_mad)\n",
    "\n",
    "    global_rmse = diagnostics.posterior_contraction(test_global_samples, test_dict,\n",
    "                                                    aggregation=np.median)['values'].mean().round(2)\n",
    "    global_rmse_mad = diagnostics.posterior_contraction(test_global_samples, test_dict,\n",
    "                                                        aggregation=mad)['values'].mean().round(2)\n",
    "    print('Global Contraction:', global_rmse, global_rmse_mad)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bee20403",
   "metadata": {},
   "source": "## 10x Simulation Budget"
  },
  {
   "cell_type": "code",
   "id": "316de892",
   "metadata": {},
   "source": [
    "# Match budget of compositional\n",
    "train_dict = prior.sample(\n",
    "    batch_size=10000 // grid_size * 10,\n",
    "    n_local_samples=grid_size,\n",
    ")\n",
    "\n",
    "train_dict['global_params'] = keras.ops.convert_to_numpy(prior.normalize_theta(train_dict['global_params'], global_params=True))\n",
    "train_dict['data'] = keras.ops.convert_to_numpy(prior.normalize_data(train_dict['data']))\n",
    "train_dict['global_params'].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9efcd57",
   "metadata": {},
   "source": [
    "for model_name, model_packet in models.items():\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "            adapter=adapter,\n",
    "            summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),  # shallow summary net\n",
    "            inference_network=model_packet[0](**model_packet[1]),\n",
    "            #checkpoint_filepath=f\"bf_checkpoints/ar1_{model_name}_{grid_size}_x10\",\n",
    "            standardize=None\n",
    "        )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=32, epochs=1000 if model_name != \"coupling\" else 100, verbose=2\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    test_global_samples = workflow_global.sample(conditions=test_dict, num_samples=1000)\n",
    "    global_rmse = diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict,\n",
    "                                                              aggregation=np.median)['values'].mean().round(2)\n",
    "    global_rmse_mad = diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict,\n",
    "                                                                  aggregation=mad)['values'].mean().round(2)\n",
    "    print('Global RMSE:', global_rmse, global_rmse_mad)\n",
    "\n",
    "    global_rmse = diagnostics.posterior_contraction(test_global_samples, test_dict,\n",
    "                                                    aggregation=np.median)['values'].mean().round(2)\n",
    "    global_rmse_mad = diagnostics.posterior_contraction(test_global_samples, test_dict,\n",
    "                                                        aggregation=mad)['values'].mean().round(2)\n",
    "    print('Global Contraction:', global_rmse, global_rmse_mad)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5afea47be9d56f1f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
