{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d206b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bayesflow:Using backend 'torch'\n",
      "WARNING:bayesflow:\n",
      "When using torch backend, we need to disable autograd by default to avoid excessive memory usage. Use\n",
      "\n",
      "with torch.enable_grad():\n",
      "    ...\n",
      "\n",
      "in contexts where you need gradients (e.g. custom training loops).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "import bayesflow as bf\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb4c85",
   "metadata": {},
   "source": [
    "## Simulator (self-contained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fad46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIME_POINTS = 5\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self, sigma_noise=0.1):\n",
    "        \"\"\"\n",
    "        Simulator for the hierarchical AR(1) model:\n",
    "            y[t] = alpha + beta y[t-1] + noise[t]\n",
    "        starting from an initial value.\n",
    "\n",
    "        Parameters:\n",
    "            sigma_noise (float): noise standard deviation.\n",
    "        \"\"\"\n",
    "        self.sigma_noise = sigma_noise\n",
    "        self.initial_is_zero = False\n",
    "\n",
    "    def __call__(self, params, n_time_points=N_TIME_POINTS):\n",
    "        eta = np.array(params['eta'])\n",
    "        alpha = np.array(params['alpha'])\n",
    "        N = eta.size\n",
    "        if eta.ndim > 1:\n",
    "            raise ValueError(\"eta must be a 1D array.\")\n",
    "\n",
    "        # Generate noise for the increments: shape (N, n_time_points)\n",
    "        noise = np.random.normal(\n",
    "            loc=0,\n",
    "            scale=self.sigma_noise,\n",
    "            size=(N, n_time_points)\n",
    "        )\n",
    "\n",
    "        # Initialize trajectories with the initial condition\n",
    "        traj = np.zeros((N, n_time_points))\n",
    "\n",
    "        # Simulate the AR(1) process for each trajectory and each batch\n",
    "        if not self.initial_is_zero:\n",
    "            traj[:, 0] = noise[:, 0]\n",
    "        for t in range(1, n_time_points):\n",
    "            traj[:, t] = alpha + traj[:, t - 1] * eta + noise[:, t]\n",
    "\n",
    "        return dict(observable=traj)\n",
    "\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Hierarchical prior for the AR(1) model.\n",
    "        \"\"\"\n",
    "        self.alpha_mean = 0\n",
    "        self.alpha_std = 1\n",
    "        self.beta_mean = 0\n",
    "        self.beta_std = 1 #0.1\n",
    "        self.log_sigma_mean = 0 #np.log(0.1)\n",
    "        self.log_sigma_std = 1 #0.5\n",
    "        self.n_params_global = 3\n",
    "        self.n_params_local = 1\n",
    "        self.global_param_names = [r'$\\alpha$', r'$\\beta$', r'$\\log \\sigma$']\n",
    "\n",
    "        # Build prior parameters as tensors.\n",
    "        self.hyper_prior_means = np.array(\n",
    "            [self.alpha_mean,\n",
    "             self.beta_mean,\n",
    "             self.log_sigma_mean],\n",
    "        )\n",
    "        self.hyper_prior_stds = np.array(\n",
    "            [self.alpha_std,\n",
    "             self.beta_std,\n",
    "             self.log_sigma_std],\n",
    "        )\n",
    "\n",
    "        np.random.seed(0)\n",
    "        self.simulator = Simulator()\n",
    "\n",
    "        # Compute normalization constants\n",
    "        test = self.sample(1000)\n",
    "        self.norm_x_mean = np.mean(test['data'])\n",
    "        self.norm_x_std = np.std(test['data'])\n",
    "        self.norm_prior_global_mean = np.mean(test['global_params'], axis=0)\n",
    "        self.norm_prior_global_std = np.std(test['global_params'], axis=0)\n",
    "        self.norm_prior_local_mean = np.mean(test['local_params_raw'], axis=0)\n",
    "        self.norm_prior_local_std = np.std(test['local_params_raw'], axis=0)\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample(batch_size=batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_local_param_names(n_local_samples):\n",
    "        return [r'$\\eta_{' + str(i) + '}$' for i in range(n_local_samples)]\n",
    "\n",
    "    def _sample_global(self):\n",
    "        # Sample global parameters\n",
    "        self.alpha = np.random.normal(loc=self.alpha_mean, scale=self.alpha_std)\n",
    "        self.beta = np.random.normal(loc=self.beta_mean, scale=self.beta_std)\n",
    "        self.log_sigma = np.random.normal(loc=self.log_sigma_mean, scale=self.log_sigma_std)\n",
    "        return dict(alpha=self.alpha, beta=self.beta, log_sigma=self.log_sigma)\n",
    "\n",
    "    def _sample_local(self, n_local_samples=1):\n",
    "        # Sample local parameters\n",
    "        eta_raw = np.random.normal(loc=0, scale=np.exp(self.log_sigma), size=n_local_samples)\n",
    "        eta = self.transform_local_params(beta=self.beta, eta_raw=eta_raw)\n",
    "        return dict(eta=eta, eta_raw=eta_raw)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_local_params(beta, eta_raw):\n",
    "        # transform raw local parameters\n",
    "        return 2*expit(beta + eta_raw)-1\n",
    "\n",
    "    @staticmethod\n",
    "    def back_transform_local_params(local_params):\n",
    "        local_params_raw = logit((local_params + 1) / 2)\n",
    "        local_params_raw[local_params_raw < -100] = -100\n",
    "        local_params_raw[local_params_raw > 100] = 100\n",
    "        return local_params_raw\n",
    "\n",
    "    def sample(self, batch_size, n_local_samples=1, n_time_points=N_TIME_POINTS, get_grid=False):\n",
    "        # Sample global and local parameters and simulate data\n",
    "        global_params = np.zeros((batch_size, self.n_params_global))\n",
    "        local_params_raw = np.zeros((batch_size, n_local_samples))\n",
    "        local_params = np.zeros((batch_size, n_local_samples))\n",
    "        data = np.zeros((batch_size, n_local_samples, n_time_points))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            global_sample = self._sample_global()\n",
    "            local_sample = self._sample_local(n_local_samples=n_local_samples)\n",
    "            sim_dict = {'alpha': global_sample['alpha'], 'eta': local_sample['eta']}\n",
    "            sim = self.simulator(sim_dict, n_time_points=n_time_points)\n",
    "\n",
    "            global_params[i] = [global_sample['alpha'], global_sample['beta'], global_sample['log_sigma']]\n",
    "            local_params_raw[i] = local_sample['eta_raw']\n",
    "            local_params[i] = local_sample['eta']\n",
    "            data[i] = sim['observable']\n",
    "\n",
    "        # Convert to tensors\n",
    "        global_params = np.array(global_params)\n",
    "        local_params = np.array(local_params)\n",
    "        local_params_raw = np.array(local_params_raw)\n",
    "\n",
    "        data = np.array(data)\n",
    "        if get_grid:\n",
    "            grid_size = int(np.sqrt(n_local_samples))\n",
    "            data = data[:, :grid_size ** 2]\n",
    "            data = data.reshape(batch_size, n_time_points, grid_size, grid_size)\n",
    "            local_params = local_params[:, :grid_size ** 2]\n",
    "            local_params_raw = local_params_raw[:, :grid_size ** 2]\n",
    "            local_params = local_params.reshape(batch_size, grid_size, grid_size)\n",
    "            local_params_raw = local_params_raw.reshape(batch_size, grid_size, grid_size)\n",
    "        return dict(global_params=global_params, local_params=local_params,\n",
    "                    local_params_raw=local_params_raw, data=data)\n",
    "\n",
    "    def normalize_theta(self, theta, global_params):\n",
    "        if global_params:\n",
    "            return (theta - self.norm_prior_global_mean) / self.norm_prior_global_std\n",
    "        return (theta - self.norm_prior_local_mean) / self.norm_prior_local_std\n",
    "\n",
    "    def denormalize_theta(self, theta, global_params):\n",
    "        if global_params:\n",
    "            return theta * self.norm_prior_global_std + self.norm_prior_global_mean\n",
    "        return theta * self.norm_prior_local_std + self.norm_prior_local_mean\n",
    "\n",
    "    def normalize_data(self, x):\n",
    "        return (x - self.norm_x_mean) / self.norm_x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ad5aa",
   "metadata": {},
   "source": [
    "## Experiment 2a: Small 4 x 4 Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2affb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = Prior()\n",
    "sim = Simulator()\n",
    "\n",
    "# Match budget of compositional\n",
    "train_dict = prior.sample(\n",
    "    batch_size=625,\n",
    "    n_local_samples=16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "train_dict['global_params'] = prior.normalize_theta(train_dict['global_params'], global_params=True)\n",
    "train_dict['data'] = prior.normalize_data(train_dict['data'])\n",
    "\n",
    "val_dict = prior.sample(\n",
    "    batch_size=300,\n",
    "    n_local_samples=16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "val_dict['global_params'] = prior.normalize_theta(val_dict['global_params'], global_params=True)\n",
    "val_dict['data'] = prior.normalize_data(val_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a74276",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .drop([\"local_params\", \"local_params_raw\"])\n",
    "    .rename(\"global_params\", \"inference_variables\")\n",
    "    .rename(\"data\", \"summary_variables\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18cc5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"fm\": (bf.networks.FlowMatching, {}),\n",
    "    \"coupling\": (bf.networks.CouplingFlow, {\n",
    "        \"transform\": \"spline\",\n",
    "        \"depth\": 2\n",
    "    }),\n",
    "    \"dm_cosine_F\": (bf.networks.DiffusionModel, {\n",
    "        \"noise_schedule\": \"cosine\", \n",
    "        \"prediction_type\": \"F\", }),\n",
    "    \"dm_cosine_v\": (bf.networks.DiffusionModel, {\n",
    "        \"noise_schedule\": \"cosine\", \n",
    "        \"prediction_type\": \"velocity\"}),   \n",
    "    \"dm_cosine_noise\": (bf.networks.DiffusionModel, {\n",
    "        \"noise_schedule\": \"cosine\", \n",
    "        \"prediction_type\": \"noise\"}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_packet in models.items():\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "        adapter=adapter,\n",
    "        summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),\n",
    "        inference_network=model_packet[0](**model_packet[1]),\n",
    "        checkpoint_filepath=f\"bf_checkpoints/{model_name}_16\"\n",
    "    )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=32, epochs=500 if model_name != \"coupling\" else 100, validation_data=val_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25cb81",
   "metadata": {},
   "source": [
    "## Experiment 2b: Larger 16 x 16 Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed07c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 data points (same as compositional)\n",
    "train_dict = prior.sample(\n",
    "    batch_size=40,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "train_dict['global_params'] = prior.normalize_theta(train_dict['global_params'], global_params=True)\n",
    "train_dict['data'] = prior.normalize_data(train_dict['data'])\n",
    "\n",
    "val_dict = prior.sample(\n",
    "    batch_size=300,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "val_dict['global_params'] = prior.normalize_theta(val_dict['global_params'], global_params=True)\n",
    "val_dict['data'] = prior.normalize_data(val_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_packet in models.items():\n",
    "\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "        adapter=adapter,\n",
    "        summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),\n",
    "        inference_network=model_packet[0](**model_packet[1]),\n",
    "        checkpoint_filepath=f\"bf_checkpoints/{model_name}_256\"\n",
    "    )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=16, epochs=200 if model_name != \"coupling\" else 100, validation_data=val_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee20403",
   "metadata": {},
   "source": [
    "## Experiment 2c: Larger 16 x 16 Grids with 10x Simulation Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "316de892",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = prior.sample(\n",
    "    batch_size=400,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "\n",
    "train_dict['global_params'] = prior.normalize_theta(train_dict['global_params'], global_params=True)\n",
    "train_dict['data'] = prior.normalize_data(train_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_packet in models.items():\n",
    "\n",
    "    workflow_global = bf.BasicWorkflow(\n",
    "        adapter=adapter,\n",
    "        summary_network=bf.networks.DeepSet(summary_dim=5, dropout=0.1, depth=1),\n",
    "        inference_network=model_packet[0](**model_packet[1]),\n",
    "        checkpoint_filepath=f\"bf_checkpoints/{model_name}_256_bigS\"\n",
    "    )\n",
    "\n",
    "    history = workflow_global.fit_offline(\n",
    "        train_dict, batch_size=16, epochs=200 if model_name != \"coupling\" else 100, validation_data=val_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aade3f9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e180c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only evaluate the coupling flow version (Heinrich et al., 2024, Habermann et al., 2024) and the v-prediction DM\n",
    "# used for our compositional backbones\n",
    "eval_models = {\n",
    "    \"coupling\": (bf.networks.CouplingFlow, {\n",
    "        \"transform\": \"spline\",\n",
    "        \"depth\": 2\n",
    "    }),\n",
    "    \"dm_cosine_v\": (bf.networks.DiffusionModel, {\n",
    "        \"noise_schedule\": \"cosine\", \n",
    "        \"prediction_type\": \"velocity\"}),   \n",
    "}\n",
    "\n",
    "n_post_samples = 100\n",
    "\n",
    "# Eval data (small and large)\n",
    "test_dict_16 = prior.sample(\n",
    "    batch_size=100,\n",
    "    n_local_samples=4 * 4,\n",
    "    n_time_points=5\n",
    ")\n",
    "test_dict_16['global_params'] = prior.normalize_theta(test_dict_16['global_params'], global_params=True)\n",
    "test_dict_16['data'] = prior.normalize_data(test_dict_16['data'])\n",
    "\n",
    "\n",
    "test_dict_256 = prior.sample(\n",
    "    batch_size=100,\n",
    "    n_local_samples=16 * 16,\n",
    "    n_time_points=5\n",
    ")\n",
    "test_dict_256['global_params'] = prior.normalize_theta(test_dict_256['global_params'], global_params=True)\n",
    "test_dict_256['data'] = prior.normalize_data(test_dict_256['data'])\n",
    "\n",
    "\n",
    "# Eval on small grids (4 x 4, matched budget)\n",
    "small_results = {}\n",
    "for model_name, model_packet in eval_models.items():\n",
    "\n",
    "    approximator = keras.saving.load_model(f\"bf_checkpoints/{model_name}_16/model.keras\")\n",
    "\n",
    "    test_global_samples = approximator.sample(conditions=test_dict_16, num_samples=n_post_samples)\n",
    "\n",
    "    rmse = bf.diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict_16)['values']\n",
    "    ece = bf.diagnostics.metrics.calibration_error(test_global_samples, test_dict_16)['values']\n",
    "    pc = bf.diagnostics.metrics.posterior_contraction(test_global_samples, test_dict_16)['values']\n",
    "\n",
    "    small_results[model_name] = {\n",
    "        \"RMSE_mean\": rmse.mean(),\n",
    "        \"ECE_mean\": ece.mean(),\n",
    "        \"PC_mean\": pc.mean(),\n",
    "        \"RMSE_std\": rmse.std(),\n",
    "        \"ECE_std\": ece.std(),\n",
    "        \"PC_std\": pc.std()\n",
    "    }\n",
    "\n",
    "\n",
    "# Eval on larger grid (16 x 16, matched budget)\n",
    "larger_results = {}\n",
    "for model_name, model_packet in eval_models.items():\n",
    "\n",
    "    approximator = keras.saving.load_model(f\"bf_checkpoints/{model_name}_256/model.keras\")\n",
    "\n",
    "    test_global_samples = approximator.sample(conditions=test_dict_256, num_samples=n_post_samples)\n",
    "\n",
    "    rmse = bf.diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict_256)['values']\n",
    "    ece = bf.diagnostics.metrics.calibration_error(test_global_samples, test_dict_256)['values']\n",
    "    pc = bf.diagnostics.metrics.posterior_contraction(test_global_samples, test_dict_256)['values']\n",
    "\n",
    "    larger_results[model_name] = {\n",
    "        \"RMSE_mean\": rmse.mean(),\n",
    "        \"ECE_mean\": ece.mean(),\n",
    "        \"PC_mean\": pc.mean(),\n",
    "        \"RMSE_std\": rmse.std(),\n",
    "        \"ECE_std\": ece.std(),\n",
    "        \"PC_std\": pc.std()\n",
    "    }\n",
    "\n",
    "\n",
    "# Eval on larger grid (16 x 16, 10x budget)\n",
    "larger_results_10x = {}\n",
    "for model_name, model_packet in eval_models.items():\n",
    "\n",
    "    approximator = keras.saving.load_model(f\"bf_checkpoints/{model_name}_256_bigS/model.keras\")\n",
    "\n",
    "    test_global_samples = approximator.sample(conditions=test_dict_256, num_samples=n_post_samples)\n",
    "\n",
    "    rmse = bf.diagnostics.metrics.root_mean_squared_error(test_global_samples, test_dict_256)['values']\n",
    "    ece = bf.diagnostics.metrics.calibration_error(test_global_samples, test_dict_256)['values']\n",
    "    pc = bf.diagnostics.metrics.posterior_contraction(test_global_samples, test_dict_256)['values']\n",
    "\n",
    "    larger_results_10x[model_name] = {\n",
    "        \"RMSE_mean\": rmse.mean(),\n",
    "        \"ECE_mean\": ece.mean(),\n",
    "        \"PC_mean\": pc.mean(),\n",
    "        \"RMSE_std\": rmse.std(),\n",
    "        \"ECE_std\": ece.std(),\n",
    "        \"PC_std\": pc.std()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4233d",
   "metadata": {},
   "source": [
    "### Present results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "227949be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>PC</th>\n",
       "      <th>ECE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Problem size</th>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Small (4x4)</th>\n",
       "      <th>coupling</th>\n",
       "      <td>0.10 (0.05)</td>\n",
       "      <td>0.87 (0.09)</td>\n",
       "      <td>0.02 (0.00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dm_cosine_v</th>\n",
       "      <td>0.09 (0.04)</td>\n",
       "      <td>0.94 (0.04)</td>\n",
       "      <td>0.07 (0.03)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Large (16x16)</th>\n",
       "      <th>coupling</th>\n",
       "      <td>0.17 (0.06)</td>\n",
       "      <td>0.66 (0.20)</td>\n",
       "      <td>0.07 (0.03)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dm_cosine_v</th>\n",
       "      <td>0.19 (0.07)</td>\n",
       "      <td>0.53 (0.38)</td>\n",
       "      <td>0.03 (0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Large x10 (16x16)</th>\n",
       "      <th>coupling</th>\n",
       "      <td>0.07 (0.04)</td>\n",
       "      <td>0.92 (0.07)</td>\n",
       "      <td>0.06 (0.02)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dm_cosine_v</th>\n",
       "      <td>0.07 (0.03)</td>\n",
       "      <td>0.96 (0.03)</td>\n",
       "      <td>0.09 (0.08)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      RMSE           PC          ECE\n",
       "Problem size      Method                                            \n",
       "Small (4x4)       coupling     0.10 (0.05)  0.87 (0.09)  0.02 (0.00)\n",
       "                  dm_cosine_v  0.09 (0.04)  0.94 (0.04)  0.07 (0.03)\n",
       "Large (16x16)     coupling     0.17 (0.06)  0.66 (0.20)  0.07 (0.03)\n",
       "                  dm_cosine_v  0.19 (0.07)  0.53 (0.38)  0.03 (0.01)\n",
       "Large x10 (16x16) coupling     0.07 (0.04)  0.92 (0.07)  0.06 (0.02)\n",
       "                  dm_cosine_v  0.07 (0.03)  0.96 (0.03)  0.09 (0.08)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = {\n",
    "    \"Small (4x4)\": small_results,\n",
    "    \"Large (16x16)\": larger_results,\n",
    "    \"Large x10 (16x16)\": larger_results_10x,\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for size, res in all_results.items():\n",
    "    for method, metrics in res.items():\n",
    "        row = {}\n",
    "        for k, v in metrics.items():\n",
    "            row[k] = float(v)  # unwrap VariableArray\n",
    "        rows.append((size, method, row))\n",
    "\n",
    "# build DataFrame\n",
    "index = pd.MultiIndex.from_tuples(\n",
    "    [(size, method) for size, method, _ in rows],\n",
    "    names=[\"Problem size\", \"Method\"],\n",
    ")\n",
    "data = [r for _, _, r in rows]\n",
    "df = pd.DataFrame(data, index=index)\n",
    "\n",
    "# combine mean and std into \"mean (std)\" columns\n",
    "out = {}\n",
    "for metric in [\"RMSE\", \"PC\", \"ECE\"]:\n",
    "    mean_col = f\"{metric}_mean\"\n",
    "    std_col = f\"{metric}_std\"\n",
    "    out[metric] = [\n",
    "        f\"{m:.2f} ({s:.2f})\"\n",
    "        for m, s in zip(df[mean_col], df[std_col])\n",
    "    ]\n",
    "\n",
    "final_df = pd.DataFrame(out, index=df.index)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48684245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
