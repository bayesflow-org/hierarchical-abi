{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hierarchical Ar(1) on a Grid Test with compositional score matching\n",
    "\n",
    "In this notebook, we will test the compositional score matching on a hierarchical problem defined on a grid.\n",
    "- The observations are on grid with `n_grid` x `n_grid` points.\n",
    "- The global parameters are the same for all grid points with hyper-priors:\n",
    "$$ \\alpha \\sim \\mathcal{N}(0, 1) \\quad\n",
    "  \\mu_\\beta \\sim \\mathcal{N}(0, 1) \\quad\n",
    "  \\log\\text{std}_\\beta \\sim \\mathcal{N}(-1, 1);$$\n",
    "\n",
    "- The local parameters are different for each grid point\n",
    "$$ \\beta_{i,j}^\\text{raw} \\sim \\mathcal{N}(\\mu_\\beta, \\text{std}_\\beta^2), \\qquad \\beta_{i,j} = 2\\operatorname{sigmoid}(\\beta_{i,j}^\\text{raw})-1$$\n",
    "\n",
    "-  In each grid point, we have a time series of `T` observations. For the time beeing, we fix $\\sigma=1$.\n",
    "$$ y_{i,j} \\sim \\mathcal{N}(\\alpha + \\beta_{i,j}y_{i,j-1}, \\sigma^2), y_{i,0} \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "- We observe $T=5$ time points for each grid point. We can also amortize over the time dimension."
   ],
   "id": "5c47077e28d8827f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from bayesflow import diagnostics\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, euler_maruyama_sampling, adaptive_sampling, train_score_model\n",
    "from diffusion_model.helper_networks import GaussianFourierProjection, ShallowSet\n",
    "from problems.ar1_grid import AR1GridProblem, Prior\n",
    "from problems import plot_shrinkage, visualize_simulation_output"
   ],
   "id": "54483797f862ea66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cpu\")",
   "id": "1f23673a8dba58c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior()\n",
    "\n",
    "# test the simulator\n",
    "sim_test = prior.sample(1, n_local_samples=16, get_grid=True)['data'][0]\n",
    "visualize_simulation_output(sim_test)"
   ],
   "id": "69a8dc61f8e7e0c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "number_of_obs = 1 #[1, 4, 8, 16, 64, 128]  # or a list\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "dataset = AR1GridProblem(\n",
    "    n_data=10000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    online_learning=True,\n",
    "    number_of_obs=number_of_obs,\n",
    "    amortize_time=False,\n",
    "    as_set=True\n",
    ")\n",
    "\n",
    "dataset_valid = AR1GridProblem(\n",
    "    n_data=1000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    number_of_obs=number_of_obs,\n",
    "    as_set=True,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for test in dataloader:\n",
    "    print(test[4].shape)\n",
    "    break"
   ],
   "id": "3125aa477854d777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "global_summary_dim = 5\n",
    "obs_n_time_steps = 0\n",
    "#summary_net = LSTM(input_size=1, hidden_dim=global_summary_dim, max_batch_size=1024)\n",
    "global_summary_net = ShallowSet(dim_input=5, dim_output=global_summary_dim, dim_hidden=16)\n",
    "\n",
    "time_dim = 8\n",
    "time_embedding_local = nn.Sequential(\n",
    "    GaussianFourierProjection(time_dim),\n",
    "    nn.Linear(time_dim, time_dim),\n",
    "    nn.Mish()\n",
    ")\n",
    "time_embedding_global = nn.Sequential(\n",
    "    GaussianFourierProjection(time_dim),\n",
    "    nn.Linear(time_dim, time_dim),\n",
    "    nn.Mish()\n",
    ")\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x_global=global_summary_dim,\n",
    "    input_dim_x_local=global_summary_dim,\n",
    "    #summary_net=summary_net,\n",
    "    global_summary_net=global_summary_net if isinstance(number_of_obs, list) else None,\n",
    "    time_embedding_local=time_embedding_local,\n",
    "    time_embedding_global=time_embedding_global,\n",
    "    hidden_dim=256,\n",
    "    n_blocks=5,\n",
    "    max_number_of_obs=number_of_obs if isinstance(number_of_obs, int) else max(number_of_obs),\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior,\n",
    "    name_prefix=f'ar1_3_{number_of_obs if isinstance(number_of_obs, int) else max(number_of_obs)}',\n",
    ")\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "96000faed8041fdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid, hierarchical=True,\n",
    "                                              epochs=2000, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Training', color=\"#132a70\", lw=2.0, alpha=0.9)\n",
    "plt.plot(loss_history[:, 1], label='Validation', linestyle=\"--\", marker=\"o\", color='black')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('Training epoch #')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')"
   ],
   "id": "7a547137b6c88170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\",\n",
    "                                       map_location=torch_device, weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "74fbd0646ed4b2bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "57c74f266a1b73a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 2\n",
    "#np.random.seed(0)\n",
    "prior_dict = prior.sample(batch_size=100, n_local_samples=n_grid*n_grid)\n",
    "\n",
    "valid_prior_global, valid_prior_local, valid_data = prior_dict['global_params'], prior_dict['local_params'], prior_dict['data']\n",
    "n_post_samples = 100\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(n_grid*n_grid)\n",
    "#score_model.current_number_of_obs = 1 # we can choose here, how many observations are passed together through the score\n",
    "\n",
    "#score_model.current_number_of_obs = 4\n",
    "print(valid_data.shape, score_model.current_number_of_obs)"
   ],
   "id": "38502b19cf973a3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(prior.normalize_data(valid_data[0]).reshape(5, n_grid, n_grid))",
   "id": "6f34c52c97365219",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(valid_data[0].reshape(5, n_grid, n_grid))",
   "id": "98f6856b4bc78fcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = euler_maruyama_sampling(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                         n_post_samples=n_post_samples, #mini_batch_arg=mini_batch_arg,\n",
    "                                                         diffusion_steps=300, device=torch_device, verbose=True)"
   ],
   "id": "827c505971c104a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "print('RMSE:', diagnostics.root_mean_squared_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "print('ECDF:', diagnostics.calibration_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sampler.png')"
   ],
   "id": "b70165384ef04878",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 8\n",
    "t1_value = 0.5# /( (n_grid*n_grid) //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    'size': mini_batch_size,\n",
    "    #'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'noisy_condition': {\n",
    "    #    'apply': False,\n",
    "    #    'noise_scale': 1.,\n",
    "    #    'tau_1': 0.6,\n",
    "    #    'tau_2': 0.8,\n",
    "    #    'mixing_factor': 1.\n",
    "    #}\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "score_model.sde.s_shift_cosine = 0\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "9229b9adbf619eab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid, step_list = adaptive_sampling(score_model, valid_data,\n",
    "                                                              obs_n_time_steps=obs_n_time_steps,\n",
    "                                                         n_post_samples=100, #max_evals=2000,\n",
    "                                                         mini_batch_arg=mini_batch_arg, run_sampling_in_parallel=False,\n",
    "                                                         device=torch_device, verbose=True, return_steps=True)"
   ],
   "id": "996c64cb32e5aa66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(step_list[0])\n",
    "plt.show()"
   ],
   "id": "be1b5828042df35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "print('RMSE:', diagnostics.root_mean_squared_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "print('ECDF:', diagnostics.calibration_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler.png')"
   ],
   "id": "e897ccde2e6f9be2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                   n_post_samples=n_post_samples,\n",
    "                                                   #mini_batch_arg=mini_batch_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "2e0e68a78a1593ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler.png')"
   ],
   "id": "28b3c9c67f92be6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conditions_global = (np.median(posterior_global_samples_valid, axis=0), posterior_global_samples_valid)[1]\n",
    "posterior_local_samples_valid = euler_maruyama_sampling(score_model, valid_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                        n_post_samples=n_post_samples, conditions=conditions_global,\n",
    "                                                        diffusion_steps=100, device=torch_device, verbose=True)\n",
    "\n",
    "posterior_local_samples_valid = score_model.prior.transform_local_params(posterior_local_samples_valid)"
   ],
   "id": "54255d4bbbc8bd1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1).shape",
   "id": "501acff164f908c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          variable_names=local_param_names);"
   ],
   "id": "947470d71cedfe0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 7\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('log sigma:', np.median(posterior_global_samples_valid[valid_id, :, 2]), np.std(posterior_global_samples_valid[valid_id, :, 2]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][1].item())\n",
    "print('log sigma:', valid_prior_global[valid_id][2].item())"
   ],
   "id": "f63e9e6f4dba8b14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "local_p_posterior = posterior_local_samples_valid[valid_id]\n",
    "local_p_true = valid_prior_local[valid_id]\n",
    "\n",
    "med = np.median(local_p_posterior, axis=0).flatten()\n",
    "std = np.std(local_p_posterior, axis=0).flatten()\n",
    "error = (med-local_p_true.numpy())**2\n",
    "print(error.sum())\n",
    "visualize_simulation_output(np.stack((med, local_p_true)).T,\n",
    "                            title_prefix=['Posterior Median', 'True'])\n",
    "\n",
    "visualize_simulation_output(np.stack((std, error)).T, title_prefix=['Uncertainty', 'Error'], same_scale=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4), tight_layout=True)\n",
    "plt.errorbar(x=local_p_true.flatten(), y=med.flatten(), yerr=1.96*std.flatten(), fmt='o')\n",
    "plt.plot([np.min(med), np.max(med)], [np.min(med), np.max(med)], 'k--')\n",
    "#plt.axhline(np.median(posterior_global_samples_valid[valid_id], axis=0)[1], color='red', linestyle='--',\n",
    "#            label='Global posterior mean', alpha=0.75)\n",
    "plt.ylabel('Prediction')\n",
    "plt.xlabel('True')\n",
    "#plt.legend()\n",
    "plt.show()"
   ],
   "id": "dad2b7c0c92b5679",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare to STAN",
   "id": "454c871b333f246d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N = 32*32\n",
    "global_posterior_stan = np.load(f'problems/ar1/global_posterior_{N}.npy')#[:, -100:]\n",
    "local_posterior_stan = np.load(f'problems/ar1/local_posterior_{N}.npy')#[:, -100:]\n",
    "true_global = np.load(f'problems/ar1/true_global_{N}.npy')\n",
    "true_local = np.load(f'problems/ar1/true_local_{N}.npy')\n",
    "\n",
    "n_grid_stan = int(np.sqrt(true_local.shape[1]))\n",
    "\n",
    "test_data = []\n",
    "for g, l in zip(true_global, true_local):\n",
    "    sim_dict = {'alpha': g[0],\n",
    "                'beta': l}\n",
    "    td = prior.simulator(sim_dict)['observable']\n",
    "    test_data.append(td.reshape(1, n_grid_stan*n_grid_stan, 5))\n",
    "test_data = np.concatenate(test_data)\n",
    "\n",
    "n_obs = n_grid_stan*n_grid_stan\n",
    "batch_size = test_data.shape[0]\n",
    "n_post_samples = 100\n",
    "\n",
    "print(n_grid_stan*n_grid_stan, test_data.shape)"
   ],
   "id": "581f128580f57fd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    t1_value = trial.suggest_float('t1_value', 1e-7, 1)\n",
    "    s_shift_cosine = trial.suggest_float('s_shift_cosine', 0, 10)\n",
    "    tau1 = trial.suggest_float('tau_1', 0.4, 0.9)\n",
    "    tau2 = min(tau1 + trial.suggest_float('delta_tau_2', 0, 0.4), 1)\n",
    "\n",
    "    t0_value = 1\n",
    "    mini_batch_arg = {\n",
    "        'size': 16,\n",
    "        'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2 * t),\n",
    "        'noisy_condition': {\n",
    "            'apply': True,\n",
    "            'noise_scale': 1.,\n",
    "            'tau_1': tau1,\n",
    "            'tau_2': tau2,\n",
    "            'mixing_factor': 1.\n",
    "        }\n",
    "    }\n",
    "    score_model.sde.s_shift_cosine = s_shift_cosine\n",
    "\n",
    "    test_global_samples = adaptive_sampling(score_model, test_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                   n_post_samples=n_post_samples,\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=False)\n",
    "\n",
    "    c_error = diagnostics.calibration_error(test_global_samples, true_global)['values'].mean()\n",
    "    rmse = diagnostics.root_mean_squared_error(test_global_samples, true_global)['values'].mean()\n",
    "    return rmse + c_error\n",
    "\n",
    "study = optuna.create_study()\n",
    "#study.optimize(objective, n_trials=20)\n",
    "#print(study.best_params)\n",
    "\n",
    "study_best_params = {'t1_value': 0.3994890029993266, 's_shift_cosine': 4.190800555472762,\n",
    "                     'tau_1': 0.7639228586194012, 'delta_tau_2': 0.3010046756155764}"
   ],
   "id": "69e77c6be4c572b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t0_value, t1_value = 1, study_best_params['t1_value']\n",
    "mini_batch_arg = {\n",
    "    'size': 4,\n",
    "    'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    'noisy_condition': {\n",
    "        'apply': False,\n",
    "        'noise_scale': 1.,\n",
    "        'tau_1': study_best_params['tau_1'],\n",
    "        'tau_2': min(study_best_params['tau_1'] + study_best_params['delta_tau_2'], 1),\n",
    "        'mixing_factor': 1.\n",
    "    }\n",
    "}\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(n_grid_stan*n_grid_stan)\n",
    "param_names_stan = ['STAN '+ p for p in global_param_names]\n",
    "score_model.sde.s_shift_cosine = study_best_params['s_shift_cosine']\n",
    "\n",
    "print(mini_batch_arg)"
   ],
   "id": "331b32a82f23f60b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.current_number_of_obs = 64\n",
    "# posterior_global_samples_test = adaptive_sampling(score_model, test_data, obs_n_time_steps=obs_n_time_steps,\n",
    "#                                                     n_post_samples=n_post_samples,\n",
    "#                                                     mini_batch_arg=mini_batch_arg,\n",
    "#                                                     run_sampling_in_parallel=False,\n",
    "#                                                     device=torch_device, verbose=True)\n",
    "\n",
    "posterior_global_samples_test = euler_maruyama_sampling(score_model, test_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                   n_post_samples=n_post_samples,\n",
    "                                                   mini_batch_arg=mini_batch_arg,\n",
    "                                                   diffusion_steps=300,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "8bd578faee7c394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_test, true_global, variable_names=global_param_names)\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_ours.png')\n",
    "print('RMSE:', diagnostics.root_mean_squared_error(posterior_global_samples_test, true_global)['values'].mean())\n",
    "#fig = diagnostics.recovery(posterior_global_samples_test, np.median(global_posterior_stan, axis=1),\n",
    "#                     variable_names=global_param_names, xlabel='STAN Median Estimate')\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_ours_vs_STAN.png')\n",
    "fig = diagnostics.recovery(global_posterior_stan, true_global, variable_names=param_names_stan)\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_STAN.png')\n",
    "print('RMSE STAN:', diagnostics.root_mean_squared_error(global_posterior_stan, true_global)['values'].mean())"
   ],
   "id": "e8f8151c350c38a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_test, true_global, difference=True,\n",
    "                             variable_names=global_param_names)\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global_ours.png')\n",
    "print('ECDF:', diagnostics.calibration_error(posterior_global_samples_test, true_global)['values'].mean())\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(global_posterior_stan, true_global, difference=True, variable_names=param_names_stan)\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global_STAN.png')\n",
    "print('ECDF STAN:', diagnostics.calibration_error(global_posterior_stan, true_global)['values'].mean())"
   ],
   "id": "277ce7c4c4edb333",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.sde.s_shift_cosine = 0\n",
    "score_model.current_number_of_obs = 1\n",
    "posterior_local_samples_test = euler_maruyama_sampling(score_model, test_data[:, :12], obs_n_time_steps=obs_n_time_steps,\n",
    "                                                       n_post_samples=n_post_samples,\n",
    "                                                       conditions=posterior_global_samples_test,\n",
    "                                                       #conditions=global_posterior_stan[:, :n_post_samples],\n",
    "                                                       #run_sampling_in_parallel=False,\n",
    "                                                       mini_batch_arg={'local_chunk_size': 1000},\n",
    "                                                       diffusion_steps=200,\n",
    "                                                       device=torch_device, verbose=True)\n",
    "\n",
    "posterior_local_samples_test = score_model.prior.transform_local_params(posterior_local_samples_test)"
   ],
   "id": "bf57087b90c578f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_var = np.exp(np.median(posterior_global_samples_test, axis=1)[:, 1])[:, np.newaxis] ** 2\n",
    "shrinkage = 1-np.var(np.median(posterior_local_samples_test, axis=1), axis=1)/global_var\n",
    "\n",
    "global_var_stan = np.exp(np.median(global_posterior_stan, axis=1)[:, 1])**2\n",
    "shrinkage_stan = 1-np.var(np.median(local_posterior_stan, axis=1), axis=1)/global_var_stan\n",
    "\n",
    "true_var = np.exp(true_global)[:, 1]**2\n",
    "shrinkage_true = 1-np.var(true_local, axis=1)/true_var\n",
    "\n",
    "s_order = np.argsort(shrinkage_true)\n",
    "shrinkage = shrinkage.flatten()[s_order]\n",
    "shrinkage_stan = shrinkage_stan.flatten()[s_order]\n",
    "shrinkage_true = shrinkage_true[s_order]\n",
    "\n",
    "min_s = -10\n",
    "shrinkage[shrinkage < min_s] = min_s\n",
    "shrinkage_stan[shrinkage_stan < min_s] = min_s\n",
    "shrinkage_true[shrinkage_true < min_s] = min_s\n",
    "\n",
    "plt.title('Shrinkage')\n",
    "plt.plot(shrinkage, label='score', alpha=0.75)\n",
    "plt.plot(shrinkage_stan, label='STAN', alpha=0.75)\n",
    "plt.plot(shrinkage_true, label='true', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Correlation shrinkage score and STAN:', np.corrcoef(shrinkage, shrinkage_stan)[0, 1])\n",
    "print('Correlation shrinkage true and score:', np.corrcoef(shrinkage_true, shrinkage)[0, 1])\n",
    "print('Correlation shrinkage true and STAN:', np.corrcoef(shrinkage_true, shrinkage_stan)[0, 1])\n",
    "\n",
    "print(f\"Score shrinkage < STAN shrinkage: {(shrinkage < shrinkage_stan).sum() / shrinkage.shape[0]*100}%\")"
   ],
   "id": "34fa0480f91f9f36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1)[:, :, :12],\n",
    "                     true_local[:, :12],\n",
    "                     variable_names=local_param_names[:12])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_local_ours.png')\n",
    "diagnostics.recovery(local_posterior_stan[:, :, :12], true_local[:, :12], variable_names=local_param_names[:12]);"
   ],
   "id": "9ee1210cd7e43a79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1)[:, :, :12],\n",
    "                     np.median(local_posterior_stan[:, :, :12], axis=1),\n",
    "                           ylabel='Score Based Estimates', xlabel='STAN Median Estimate', variable_names=local_param_names[:12])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_local_ours_vs_STAN.png')"
   ],
   "id": "66ddb1260bd2b0df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1)\n",
    "\n",
    "plot_shrinkage(posterior_global_samples_test[:12, :, 1:], posterior_local_samples_test[..., np.newaxis][:12],\n",
    "               min_max=(-10,10))"
   ],
   "id": "7bfc6285e8c7963b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(global_posterior_stan[:12, :, 1:], local_posterior_stan[..., np.newaxis][:12], min_max=(-10,10))",
   "id": "85ac95e3f5f922f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check different sampling schemes",
   "id": "7262a8aec635e1ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure we generate enough synthetic data samples.\n",
    "n_samples_data = 100\n",
    "n_post_samples = 100\n",
    "score_model.current_number_of_obs = 1\n",
    "max_steps = 10000\n",
    "variables_of_interest = ['mini_batch', 'cosine_shift', 'damping_factor_t']\n",
    "\n",
    "variables_of_interest.append('n_conditions')\n",
    "variable_of_interest = variables_of_interest[1]\n",
    "print(variable_of_interest)\n",
    "\n",
    "mini_batch = ['10%']\n",
    "n_conditions = [1]\n",
    "cosine_shifts = [0]\n",
    "d_factors = [1]  # using the d factor depending on the mini batch size\n",
    "data_sizes = np.array([4*4, 16*16, 64*64, 512*512])\n",
    "\n",
    "if variable_of_interest == 'mini_batch':\n",
    "    # Set up your data sizes and mini-batch parameters.\n",
    "    mini_batch = [1, 10, 100, 1000, 10000, None]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest == 'n_conditions':\n",
    "    n_conditions = [1, 4, 8, 16, 64, 128]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest == 'cosine_shift':\n",
    "    cosine_shifts = [0, -1, 1, 2, 5, 10][::-1]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "\n",
    "elif variable_of_interest == 'damping_factor_t':\n",
    "    d_factors = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 0.75, 1]\n",
    "    second_variable_of_interest = 'data_size'\n",
    "else:\n",
    "    raise ValueError('Unknown variable_of_interest')\n",
    "\n",
    "df_path = f'_plots/{score_model.name}/df_results_{variable_of_interest}.csv'\n",
    "if os.path.exists(df_path):\n",
    "    # Load CSV\n",
    "    df_results = pd.read_csv(df_path, index_col=0)\n",
    "else:\n",
    "    df_results = None"
   ],
   "id": "13c7b199d1cb89c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List to store results.\n",
    "results = []\n",
    "reached_max_evals = []\n",
    "\n",
    "# Iterate over data sizes.\n",
    "for n in data_sizes:\n",
    "    # Generate synthetic data with enough samples\n",
    "    prior_dict = prior.sample(batch_size=n_samples_data, n_local_samples=n)\n",
    "    true_params_global, true_params_local, test_data = prior_dict['global_params'], prior_dict['local_params'], prior_dict['data']\n",
    "    print(test_data.shape)\n",
    "\n",
    "    true_params_global = true_params_global.numpy()\n",
    "    true_params_local = true_params_local.numpy()\n",
    "    # Iterate over experimental setting\n",
    "    for mb, nc, cs, d_factor in itertools.product(mini_batch, n_conditions, cosine_shifts, d_factors):\n",
    "        # Skip mini-batch settings that are larger than or equal to the data size.\n",
    "        if mb == '10%':\n",
    "            mb = max(int(n * 0.1), 1)\n",
    "        if mb is not None and mb >= n:\n",
    "            continue\n",
    "        if nc > n:\n",
    "            continue\n",
    "\n",
    "        skip = False\n",
    "        for max_reached in reached_max_evals:\n",
    "            if max_reached[0] <= n:  # check if for a smaller data size we already failed\n",
    "                if max_reached[2] == nc and max_reached[3] == cs and max_reached[4] == d_factor:\n",
    "                    # all conditions are the same, only mini batch size is different\n",
    "                    if max_reached[1] is None:\n",
    "                        pass\n",
    "                    elif mb is None or max_reached[1] < mb:\n",
    "                        print(f'smaller mini batch size already failed, skipping {nc}, {cs}')\n",
    "                        skip = True\n",
    "                        break\n",
    "                #elif max_reached[2] == nc and max_reached[3] == cs and max_reached[4] < d_factor:\n",
    "                #    # all conditions are the same (assuming mini-batching does not change)\n",
    "                #    # check if smaller damping factor already failed\n",
    "                #    print(f'smaller damping factor already failed, skipping {nc}, {cs}')\n",
    "                #    skip = True\n",
    "                #    break\n",
    "        if skip:\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": -1,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"damping_factor\": d_factor,\n",
    "                'n_conditions': nc,\n",
    "                'cosine_shift': cs,\n",
    "                \"n_steps\": max_steps,\n",
    "                \"median\": np.nan,\n",
    "                \"rmse_global\":  np.nan,\n",
    "                \"c_error_global\":  np.nan,\n",
    "                \"contractions_global\":  np.nan,\n",
    "                \"rmse_local\":  np.nan,\n",
    "                \"c_error_local\":  np.nan,\n",
    "                \"contractions_local\":  np.nan,\n",
    "            })\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_csv(df_path)\n",
    "            continue\n",
    "\n",
    "        print(f\"Data Size: {n}, Mini Batch: {mb}, Conditions: {nc}, Cosine shift: {cs}, Damping Factor: {d_factor}\")\n",
    "        # Set current number of conditions\n",
    "        score_model.current_number_of_obs = nc\n",
    "\n",
    "        # Set cosine shit\n",
    "        score_model.sde.s_shift_cosine = cs\n",
    "\n",
    "        # Damping factor\n",
    "        if variable_of_interest == 'damping_factor_t':\n",
    "            t0_value = 1\n",
    "            t1_value = d_factor\n",
    "            damping_factor = lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t)\n",
    "            if mb is None:\n",
    "                mini_batch_arg = {'damping_factor': damping_factor}\n",
    "            else:\n",
    "                mini_batch_arg = {'size': mb, 'damping_factor': damping_factor}\n",
    "        else:\n",
    "            damping_factor = lambda t: torch.ones_like(t) * d_factor\n",
    "            if mb is None:\n",
    "                mini_batch_arg = {'damping_factor': damping_factor}\n",
    "            else:\n",
    "                mini_batch_arg = {'size': mb, 'damping_factor': damping_factor}\n",
    "\n",
    "        # Run adaptive sampling.\n",
    "        try:\n",
    "            print('global sampling')\n",
    "            test_global_samples, list_steps = adaptive_sampling(score_model, test_data, conditions=None,\n",
    "                                                         obs_n_time_steps=obs_n_time_steps,\n",
    "                                                         n_post_samples=n_post_samples,\n",
    "                                                         mini_batch_arg=mini_batch_arg,\n",
    "                                                         max_evals=max_steps*2,\n",
    "                                                         t_end=0, random_seed=0, device=torch_device,\n",
    "                                                         run_sampling_in_parallel=False,  # can actually be faster\n",
    "                                                         return_steps=True)\n",
    "\n",
    "            score_model.current_number_of_obs = 1\n",
    "            score_model.sde.s_shift_cosine = 0\n",
    "            print('local sampling')\n",
    "            test_local_samples = euler_maruyama_sampling(score_model, test_data, obs_n_time_steps=obs_n_time_steps,\n",
    "                                                       n_post_samples=test_global_samples.shape[1],  # in the case some samples failed\n",
    "                                                       conditions=test_global_samples,\n",
    "                                                       diffusion_steps=200,\n",
    "                                                       device=torch_device, verbose=False)\n",
    "            test_local_samples = score_model.prior.transform_local_params(test_local_samples)[..., 0]\n",
    "\n",
    "        except torch.OutOfMemoryError as e:\n",
    "            print(e)\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": -1,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"damping_factor\": d_factor,\n",
    "                'n_conditions': nc,\n",
    "                'cosine_shift': cs,\n",
    "                \"n_steps\": max_steps,\n",
    "                \"median\": np.nan,\n",
    "                \"rmse_global\":  np.nan,\n",
    "                \"c_error_global\":  np.nan,\n",
    "                \"contractions_global\":  np.nan,\n",
    "                \"rmse_local\":  np.nan,\n",
    "                \"c_error_local\":  np.nan,\n",
    "                \"contractions_local\":  np.nan,\n",
    "            })\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_csv(df_path)\n",
    "            continue\n",
    "\n",
    "        # Number of steps\n",
    "        if np.isnan(test_global_samples).any():\n",
    "            n_steps = np.inf\n",
    "            reached_max_evals.append((n, mb, nc, cs, d_factor))\n",
    "            print('nan in global samples')\n",
    "        else:\n",
    "            n_steps = np.mean([len(ls) for ls in list_steps])\n",
    "            if n_steps >= max_steps:\n",
    "                # others will also fail to converge\n",
    "                reached_max_evals.append((n, mb, nc, cs, d_factor))\n",
    "                print('max steps reached')\n",
    "\n",
    "        print('global shapes', test_global_samples.shape, true_params_global.shape)\n",
    "        rmse_global = diagnostics.root_mean_squared_error(test_global_samples, true_params_global)['values'].mean()\n",
    "        c_error_global = diagnostics.calibration_error(test_global_samples, true_params_global)['values'].mean()\n",
    "        contractions_global = diagnostics.posterior_contraction(test_global_samples, true_params_global)['values'].mean()\n",
    "\n",
    "        print('local shapes', test_local_samples.shape, true_params_local.shape)\n",
    "        rmse_local = diagnostics.root_mean_squared_error(test_local_samples, true_params_local)['values'].mean()\n",
    "        c_error_local = diagnostics.calibration_error(test_local_samples, true_params_local)['values'].mean()\n",
    "        contractions_local = diagnostics.posterior_contraction(test_local_samples, true_params_local)['values'].mean()\n",
    "\n",
    "        # Save results into a dictionary.\n",
    "        for i in range(n_samples_data):  # might be less than the actual data points because inference failed\n",
    "            results.append({\n",
    "                \"data_size\": n,\n",
    "                \"data_id\": i,\n",
    "                \"mini_batch\": mb if mb is not None else n,\n",
    "                \"damping_factor\": d_factor,\n",
    "                'n_conditions': nc,\n",
    "                'cosine_shift': cs,\n",
    "                \"n_steps\": n_steps,\n",
    "                \"median_global\": np.median(test_global_samples, axis=1)[i],\n",
    "                \"median_local\": np.median(test_local_samples, axis=1)[i],\n",
    "                \"rmse_global\": rmse_global,\n",
    "                \"c_error_global\": c_error_global,\n",
    "                \"contractions_global\": contractions_global,\n",
    "                \"rmse_local\": rmse_local,\n",
    "                \"c_error_local\": c_error_local,\n",
    "                \"contractions_local\": contractions_local,\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the results list. Save intermediate results\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.to_csv(df_path)"
   ],
   "id": "6cf3d77e80646c31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8037a5538e462e73",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical-abi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
