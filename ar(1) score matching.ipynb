{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hierarchical Ar(1) on a Grid Test with compositional score matching\n",
    "\n",
    "In this notebook, we will test the compositional score matching on a hierarchical problem defined on a grid.\n",
    "- The observations are on grid with `n_grid` x `n_grid` points.\n",
    "- The global parameters are the same for all grid points with hyper-priors:\n",
    "$$ \\alpha \\sim \\mathcal{N}(0, 1) \\quad\n",
    "  \\mu_\\beta \\sim \\mathcal{N}(0, 1) \\quad\n",
    "  \\log\\text{std}_\\beta \\sim \\mathcal{N}(0, 1);$$\n",
    "\n",
    "- The local parameters are different for each grid point\n",
    "$$ \\eta_{i,j}^\\text{raw} \\sim \\mathcal{N}(0, I), \\qquad \\eta_{i,j} = 2\\operatorname{sigmoid}(\\beta + \\sigma\\cdot\\eta_{i,j}^\\text{raw})-1$$\n",
    "\n",
    "-  In each grid point, we have a time series of `T` observations.\n",
    "$$ y_{i,j} \\sim \\mathcal{N}(\\alpha + \\eta_{i,j}y_{i,j-1}, 0.1 I), y_{i,0} \\sim \\mathcal{N}(0, 0.1 I)$$\n",
    "- We observe $T=5$ time points for each grid point. We can also amortize over the time dimension."
   ],
   "id": "44ed9ffe72cea83d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from bayesflow import diagnostics\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, euler_maruyama_sampling, adaptive_sampling, train_score_model\n",
    "from diffusion_model.helper_networks import GaussianFourierProjection, ShallowSet, GRUEncoder\n",
    "from problems.ar1_grid import AR1GridProblem, Prior\n",
    "from problems import plot_shrinkage, visualize_simulation_output"
   ],
   "id": "88cd303247bbaa07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"mps\")",
   "id": "83f1b6f60d3e7440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior = Prior()\n",
    "\n",
    "# test the simulator\n",
    "sim_test = prior.sample(1, n_local_samples=16, get_grid=True)['data'][0]\n",
    "visualize_simulation_output(sim_test)"
   ],
   "id": "d1dcd72a51917c1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "number_of_obs = [1, 4, 8, 16, 64, 128]  # or a list\n",
    "max_number_of_obs = number_of_obs if isinstance(number_of_obs, int) else max(number_of_obs)\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "dataset = AR1GridProblem(\n",
    "    n_data=10000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    online_learning=True,\n",
    "    number_of_obs=number_of_obs,\n",
    "    amortize_time=False,\n",
    "    as_set=True\n",
    ")\n",
    "\n",
    "dataset_valid = AR1GridProblem(\n",
    "    n_data=1000,\n",
    "    prior=prior,\n",
    "    sde=current_sde,\n",
    "    number_of_obs=number_of_obs,\n",
    "    as_set=True,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for test in dataloader:\n",
    "    print(test[4].shape)\n",
    "    break"
   ],
   "id": "8273bdcba78b7512",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define diffusion model\n",
    "global_summary_dim = 5\n",
    "global_summary_net = ShallowSet(dim_input=5, dim_output=global_summary_dim, dim_hidden=128)\n",
    "#local_summary_net = GRUEncoder(input_size=1, summary_dim=5, num_layers=1, hidden_size=16, dropout=0)\n",
    "\n",
    "time_dim = 8\n",
    "time_embedding_local = nn.Sequential(\n",
    "    GaussianFourierProjection(time_dim),\n",
    "    nn.Linear(time_dim, time_dim),\n",
    "    nn.Mish()\n",
    ")\n",
    "time_embedding_global = nn.Sequential(\n",
    "    GaussianFourierProjection(time_dim),\n",
    "    nn.Linear(time_dim, time_dim),\n",
    "    nn.Mish()\n",
    ")\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x_global=global_summary_dim,\n",
    "    input_dim_x_local=global_summary_dim,\n",
    "    #summary_net=local_summary_net,\n",
    "    global_summary_net=global_summary_net if isinstance(number_of_obs, list) else None,\n",
    "    time_embedding_local=time_embedding_local,\n",
    "    time_embedding_global=time_embedding_global,\n",
    "    hidden_dim=[256, 512][0],\n",
    "    n_blocks=[5,6][0],\n",
    "    dropout_rate=0.1,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior,\n",
    "    name_prefix=f'ar1_archiv/ar1_3_{max_number_of_obs}',\n",
    "    #name_prefix=f'ar1_GRU_',\n",
    ")\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "id": "4fa0d8343d6e2c8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid, hierarchical=True,\n",
    "                                              epochs=3000, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(16, 4), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Training', color=\"#132a70\", lw=2.0, alpha=0.9)\n",
    "plt.plot(loss_history[:, 1], label='Validation', linestyle=\"--\", marker=\"o\", color='black')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('Training epoch #')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')"
   ],
   "id": "4df53e0ad4cf18f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\",\n",
    "                                       map_location=torch_device, weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "4248a6241f7f81b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "f7646967710fbda0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 2#512\n",
    "#np.random.seed(0)\n",
    "prior_dict = prior.sample(batch_size=100, n_local_samples=n_grid*n_grid)\n",
    "\n",
    "valid_prior_global, valid_prior_local, valid_data = prior_dict['global_params'], prior_dict['local_params'], prior_dict['data']\n",
    "n_post_samples = 300\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(n_grid*n_grid)\n",
    "#score_model.current_number_of_obs = 1 # we can choose here, how many observations are passed together through the score\n",
    "\n",
    "#score_model.current_number_of_obs = 4\n",
    "#valid_data = valid_data.reshape(100, n_grid*n_grid, 5, 1)\n",
    "print(valid_data.shape, score_model.current_number_of_obs)"
   ],
   "id": "3d4b333722955506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(prior.normalize_data(valid_data[0]).reshape(5, n_grid, n_grid))",
   "id": "2edde98394a72333",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_simulation_output(valid_data[0].reshape(5, n_grid, n_grid))",
   "id": "4d9f0d4d023fedce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#t1_value = 1e-8 #0.13694009343501426\n",
    "#t0_value = 1e-2 #e-2\n",
    "t1_value = 0.044240666535491316 #0.13694009343501426\n",
    "t0_value = 1\n",
    "\n",
    "#{'t1_value': 0.044240666535491316, 's_shift_cosine': 4.960568548054871, 'tau_1': 0.9958869789363028, 'delta_tau_2': 0.12817031370179582, 'noise_scale': 1.3480710136773473, 'mixing_factor': 0.3577859411856741, 'diffusion_steps': 851}\n",
    "\n",
    "\n",
    "def damping_factor(t):\n",
    "    #if t.ndim == 0:\n",
    "    #    #print(t)\n",
    "    #    val = -np.log(t0_value / (t1_value * np.array([1., 0.1, 0.1], dtype=np.float32)) )\n",
    "    #    out = t0_value * torch.exp(torch.tensor(val).to(t.device) * 3*t)\n",
    "    #    print(1/torch.sqrt(out))\n",
    "    #else:\n",
    "    val = -np.log(t0_value / (t1_value * np.array([1., 1., 1.], dtype=np.float32)) )\n",
    "    out = t0_value * torch.exp(torch.tensor(val).to(t.device) * 3*t)\n",
    "    return out\n",
    "\n",
    "sampling_arg = {\n",
    "    'size': 4,\n",
    "    #'damping_factor': lambda t: (1-torch.ones_like(t)) * 1e-7 + 2e-4,\n",
    "    #'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'damping_factor': damping_factor,\n",
    "    'noisy_condition': {\n",
    "        'apply': False,\n",
    "        'noise_scale': 1.,\n",
    "        'tau_1': 1.,\n",
    "        'tau_2': 1.,\n",
    "        'mixing_factor': 1.\n",
    "    },\n",
    "    'MC-dropout': False\n",
    "}\n",
    "score_model.sde.s_shift_cosine = 0#3.71213313557092-2\n",
    "#score_model.sde.s_shift_cosine = 4.960568548054871"
   ],
   "id": "ca7899d7e3a1c3f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = euler_maruyama_sampling(score_model, valid_data,\n",
    "                                                         n_post_samples=n_post_samples, #ampling_arg=sampling_arg,\n",
    "                                                         sampling_arg={'size': 1},\n",
    "                                                         diffusion_steps=500, device=torch_device, verbose=True)\n",
    "\n",
    "# posterior_global_samples_valid = adaptive_sampling(score_model, valid_data,\n",
    "#                                                    n_post_samples=n_post_samples, sampling_arg=sampling_arg,\n",
    "#                                                    run_sampling_in_parallel=False, device=torch_device, verbose=True)"
   ],
   "id": "c5b402db69ad1bbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "print('RMSE:', diagnostics.root_mean_squared_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_n_grid_{n_grid}.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                                   difference=True, variable_names=global_param_names)\n",
    "print('ECDF:', diagnostics.calibration_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_n_grid_{n_grid}.png')"
   ],
   "id": "49a6a553a0fcd8d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 8\n",
    "t1_value = 1e-7# /( (n_grid*n_grid) //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "sampling_arg = {\n",
    "    'size': mini_batch_size,\n",
    "    'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'noisy_condition': {\n",
    "    #    'apply': False,\n",
    "    #    'noise_scale': 1.,\n",
    "    #    'tau_1': 0.6,\n",
    "    #    'tau_2': 0.8,\n",
    "    #    'mixing_factor': 1.\n",
    "    #}\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), sampling_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "score_model.sde.s_shift_cosine = 10\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "e3893c06f9eae08a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# posterior_global_samples_valid, step_list = adaptive_sampling(score_model, valid_data,\n",
    "#                                                               obs_n_time_steps=obs_n_time_steps,\n",
    "#                                                          n_post_samples=100, #max_evals=2000,\n",
    "#                                                          sampling_arg=sampling_arg, run_sampling_in_parallel=False,\n",
    "#                                                          device=torch_device, verbose=True, return_steps=True)\n",
    "\n",
    "posterior_global_samples_valid = euler_maruyama_sampling(score_model, valid_data,\n",
    "                                                         n_post_samples=n_post_samples,\n",
    "                                                         sampling_arg=sampling_arg,\n",
    "                                                         diffusion_steps=10, device=torch_device, verbose=True)"
   ],
   "id": "25a21e1a383dbd5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "print('RMSE:', diagnostics.root_mean_squared_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "print('ECDF:', diagnostics.calibration_error(posterior_global_samples_valid, np.array(valid_prior_global))['values'])\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler.png')"
   ],
   "id": "47ed1c57caa28438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data,\n",
    "                                                   n_post_samples=n_post_samples,\n",
    "                                                   #sampling_arg=sampling_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "96e6929bbd511c2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)"
   ],
   "id": "2a8df45fff6320f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conditions_global = (np.median(posterior_global_samples_valid, axis=0), posterior_global_samples_valid)[1]\n",
    "posterior_local_samples_valid = euler_maruyama_sampling(score_model, valid_data[:, :12],\n",
    "                                                        conditions=conditions_global,\n",
    "                                                        n_post_samples=n_post_samples,\n",
    "                                                        diffusion_steps=100, device=torch_device, verbose=True)\n",
    "\n",
    "posterior_local_samples_valid = score_model.prior.transform_local_params(posterior_local_samples_valid)"
   ],
   "id": "2bbba1b2239d3ffc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local)[:, :12],\n",
    "                          variable_names=local_param_names[:12]);"
   ],
   "id": "901a10110d946904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 7\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('log sigma:', np.median(posterior_global_samples_valid[valid_id, :, 2]), np.std(posterior_global_samples_valid[valid_id, :, 2]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][1].item())\n",
    "print('log sigma:', valid_prior_global[valid_id][2].item())"
   ],
   "id": "89c7b889d40548f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "local_p_posterior = posterior_local_samples_valid[valid_id]\n",
    "local_p_true = valid_prior_local[valid_id]\n",
    "\n",
    "med = np.median(local_p_posterior, axis=0).flatten()\n",
    "std = np.std(local_p_posterior, axis=0).flatten()\n",
    "error = (med-local_p_true.numpy())**2\n",
    "print(error.sum())\n",
    "visualize_simulation_output(np.stack((med, local_p_true)).T,\n",
    "                            title_prefix=['Posterior Median', 'True'])\n",
    "\n",
    "visualize_simulation_output(np.stack((std, error)).T, title_prefix=['Uncertainty', 'Error'], same_scale=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4), tight_layout=True)\n",
    "plt.errorbar(x=local_p_true.flatten(), y=med.flatten(), yerr=1.96*std.flatten(), fmt='o')\n",
    "plt.plot([np.min(med), np.max(med)], [np.min(med), np.max(med)], 'k--')\n",
    "#plt.axhline(np.median(posterior_global_samples_valid[valid_id], axis=0)[1], color='red', linestyle='--',\n",
    "#            label='Global posterior mean', alpha=0.75)\n",
    "plt.ylabel('Prediction')\n",
    "plt.xlabel('True')\n",
    "#plt.legend()\n",
    "plt.show()"
   ],
   "id": "fe75087564122925",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare to STAN",
   "id": "f740a50172dd327a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N = 32*32\n",
    "global_posterior_stan = np.load(f'problems/ar1/global_posterior_{N}.npy')#[:, -100:]\n",
    "local_posterior_stan = np.load(f'problems/ar1/local_posterior_{N}.npy')#[:, -100:]\n",
    "true_global = np.load(f'problems/ar1/true_global_{N}.npy')\n",
    "true_local = np.load(f'problems/ar1/true_local_{N}.npy')\n",
    "\n",
    "n_grid_stan = int(np.sqrt(true_local.shape[1]))\n",
    "\n",
    "test_data = []\n",
    "for g, l in zip(true_global, true_local):\n",
    "    sim_dict = {'alpha': g[0],\n",
    "                'beta': l}\n",
    "    td = prior.simulator(sim_dict)['observable']\n",
    "    test_data.append(td.reshape(1, n_grid_stan*n_grid_stan, 5))\n",
    "test_data = np.concatenate(test_data)\n",
    "\n",
    "n_obs = n_grid_stan*n_grid_stan\n",
    "batch_size = test_data.shape[0]\n",
    "n_post_samples = 300\n",
    "\n",
    "print(n_grid_stan*n_grid_stan, test_data.shape)"
   ],
   "id": "5dedb4944efc474d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    print(trial.number)\n",
    "    t1_value = trial.suggest_float('t1_value', 0.01, 0.2)\n",
    "    s_shift_cosine = trial.suggest_float('s_shift_cosine', 0, 5)\n",
    "    tau1 = trial.suggest_float('tau_1', 0.5, 1.)\n",
    "    tau2 = min(tau1 + trial.suggest_float('delta_tau_2', 0, 0.5), 1.)\n",
    "    noise_scale = trial.suggest_float('noise_scale', 0.1, 2.)\n",
    "    mixing_factor = trial.suggest_float('mixing_factor', 0, 1.)\n",
    "    diffusion_steps = trial.suggest_int('diffusion_steps', 500, 1000)\n",
    "\n",
    "    t0_value = 1\n",
    "    sampling_arg = {\n",
    "        'size': 1,\n",
    "        'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2 * t),\n",
    "        'noisy_condition': {\n",
    "            'apply': True,\n",
    "            'noise_scale': noise_scale,\n",
    "            'tau_1': tau1,\n",
    "            'tau_2': tau2,\n",
    "            'mixing_factor': mixing_factor\n",
    "        },\n",
    "    }\n",
    "    score_model.sde.s_shift_cosine = s_shift_cosine\n",
    "\n",
    "    test_global_samples = euler_maruyama_sampling(score_model, test_data,\n",
    "                                                       n_post_samples=n_post_samples,\n",
    "                                                       sampling_arg=sampling_arg,\n",
    "                                                       diffusion_steps=diffusion_steps,\n",
    "                                                       device=torch_device, verbose=False)\n",
    "\n",
    "    c_error = diagnostics.calibration_error(test_global_samples, true_global)['values'].mean()\n",
    "    rmse = diagnostics.root_mean_squared_error(test_global_samples, true_global)['values'].mean()\n",
    "    return rmse + c_error\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=2)\n",
    "#print(study.best_params)\n",
    "\n",
    "study_best_params = {'t1_value': 0.13694009343501426, #0.5*0.3994890029993266,\n",
    "                     's_shift_cosine': 3.712133135570926-0.5, #4.190800555472762-2,\n",
    "                     'tau_1': 0.6, 'delta_tau_2': 0.2}"
   ],
   "id": "4d5405b8a7c530e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def damping_factor(t):\n",
    "    val = -np.log(t0_value / (t1_value * np.array([1, 1.5, 1.5], dtype=np.float32)) )\n",
    "    return t0_value * torch.exp(torch.tensor(val).to(t.device) * 2*t)"
   ],
   "id": "996cc079fe1a6689",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t0_value, t1_value = 1, study_best_params['t1_value']\n",
    "sampling_arg = {\n",
    "    'size': 1,\n",
    "    'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'damping_factor': damping_factor,\n",
    "    'noisy_condition': {\n",
    "        'apply': True,\n",
    "        'noise_scale': 2.,\n",
    "        'tau_1': 0.8, #study_best_params['tau_1'],\n",
    "        'tau_2': 0.9, #min(study_best_params['tau_1'] + study_best_params['delta_tau_2'], 1),\n",
    "        'mixing_factor': 0.4\n",
    "    },\n",
    "    'MC-dropout': False\n",
    "}\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(n_grid_stan*n_grid_stan)\n",
    "param_names_stan = ['STAN '+ p for p in global_param_names]\n",
    "score_model.sde.s_shift_cosine = study_best_params['s_shift_cosine']\n",
    "\n",
    "print(sampling_arg)"
   ],
   "id": "8d2fa15dc7ae0cad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#score_model.current_number_of_obs = 64\n",
    "# posterior_global_samples_test = adaptive_sampling(score_model, test_data,\n",
    "#                                                     n_post_samples=n_post_samples,\n",
    "#                                                     sampling_arg=sampling_arg,\n",
    "#                                                     run_sampling_in_parallel=False,\n",
    "#                                                     device=torch_device, verbose=True)\n",
    "\n",
    "posterior_global_samples_test = euler_maruyama_sampling(score_model, test_data,\n",
    "                                                   n_post_samples=n_post_samples,\n",
    "                                                   sampling_arg=sampling_arg,\n",
    "                                                   diffusion_steps=600,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "id": "6a9e3c207095deb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_test, true_global, variable_names=global_param_names)\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_ours.png')\n",
    "print('RMSE:', diagnostics.root_mean_squared_error(posterior_global_samples_test, true_global)['values'].mean())\n",
    "#fig = diagnostics.recovery(posterior_global_samples_test, np.median(global_posterior_stan, axis=1),\n",
    "#                     variable_names=global_param_names, xlabel='STAN Median Estimate')\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_ours_vs_STAN.png')\n",
    "fig = diagnostics.recovery(global_posterior_stan, true_global, variable_names=param_names_stan)\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_global_STAN.png')\n",
    "print('RMSE STAN:', diagnostics.root_mean_squared_error(global_posterior_stan, true_global)['values'].mean())"
   ],
   "id": "721f18fd4b4c36e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_test, true_global, difference=True,\n",
    "                             variable_names=global_param_names)\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global_ours.png')\n",
    "print('ECDF:', diagnostics.calibration_error(posterior_global_samples_test, true_global)['values'].mean())\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(global_posterior_stan, true_global, difference=True, variable_names=param_names_stan)\n",
    "#fig.savefig(f'plots/{score_model.name}/ecdf_global_STAN.png')\n",
    "print('ECDF STAN:', diagnostics.calibration_error(global_posterior_stan, true_global)['values'].mean())"
   ],
   "id": "4a6fdc5543875d1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.sde.s_shift_cosine = 0\n",
    "score_model.current_number_of_obs = 1\n",
    "posterior_local_samples_test = euler_maruyama_sampling(score_model, test_data[:, :12],\n",
    "                                                       n_post_samples=n_post_samples,\n",
    "                                                       conditions=posterior_global_samples_test,\n",
    "                                                       #conditions=global_posterior_stan[:, :n_post_samples],\n",
    "                                                       #run_sampling_in_parallel=False,\n",
    "                                                       diffusion_steps=200,\n",
    "                                                       device=torch_device, verbose=True)\n",
    "\n",
    "posterior_local_samples_test = score_model.prior.transform_local_params(posterior_local_samples_test)"
   ],
   "id": "d670f38935fa71a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_var = np.exp(np.median(posterior_global_samples_test, axis=1)[:, 1])[:, np.newaxis] ** 2\n",
    "shrinkage = 1-np.var(np.median(posterior_local_samples_test, axis=1), axis=1)/global_var\n",
    "\n",
    "global_var_stan = np.exp(np.median(global_posterior_stan, axis=1)[:, 1])**2\n",
    "shrinkage_stan = 1-np.var(np.median(local_posterior_stan, axis=1), axis=1)/global_var_stan\n",
    "\n",
    "true_var = np.exp(true_global)[:, 1]**2\n",
    "shrinkage_true = 1-np.var(true_local, axis=1)/true_var\n",
    "\n",
    "s_order = np.argsort(shrinkage_true)\n",
    "shrinkage = shrinkage.flatten()[s_order]\n",
    "shrinkage_stan = shrinkage_stan.flatten()[s_order]\n",
    "shrinkage_true = shrinkage_true[s_order]\n",
    "\n",
    "min_s = -10\n",
    "shrinkage[shrinkage < min_s] = min_s\n",
    "shrinkage_stan[shrinkage_stan < min_s] = min_s\n",
    "shrinkage_true[shrinkage_true < min_s] = min_s\n",
    "\n",
    "plt.title('Shrinkage')\n",
    "plt.plot(shrinkage, label='score', alpha=0.75)\n",
    "plt.plot(shrinkage_stan, label='STAN', alpha=0.75)\n",
    "plt.plot(shrinkage_true, label='true', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Correlation shrinkage score and STAN:', np.corrcoef(shrinkage, shrinkage_stan)[0, 1])\n",
    "print('Correlation shrinkage true and score:', np.corrcoef(shrinkage_true, shrinkage)[0, 1])\n",
    "print('Correlation shrinkage true and STAN:', np.corrcoef(shrinkage_true, shrinkage_stan)[0, 1])\n",
    "\n",
    "print(f\"Score shrinkage < STAN shrinkage: {(shrinkage < shrinkage_stan).sum() / shrinkage.shape[0]*100}%\")"
   ],
   "id": "a92f717b1cdf4473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1)[:, :, :12],\n",
    "                     true_local[:, :12],\n",
    "                     variable_names=local_param_names[:12])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_local_ours.png')\n",
    "diagnostics.recovery(local_posterior_stan[:, :, :12], true_local[:, :12], variable_names=local_param_names[:12]);"
   ],
   "id": "41a189b82084b780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1)[:, :, :12],\n",
    "                     np.median(local_posterior_stan[:, :, :12], axis=1),\n",
    "                           xlabel='STAN Median Estimate', variable_names=local_param_names[:12])\n",
    "#fig.savefig(f'plots/{score_model.name}/recovery_local_ours_vs_STAN.png')"
   ],
   "id": "7d44c353c1bd96d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1)\n",
    "\n",
    "plot_shrinkage(posterior_global_samples_test[:12, :, 1:], posterior_local_samples_test[..., np.newaxis][:12],\n",
    "               min_max=(-10,10))"
   ],
   "id": "6e416570155eb0c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(global_posterior_stan[:12, :100, 1:], local_posterior_stan[..., np.newaxis][:12], min_max=(-10,10))",
   "id": "35de45a544d2e55e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "62a252431c1dd7b3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical-abi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
