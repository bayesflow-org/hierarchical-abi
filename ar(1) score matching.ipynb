{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd4b6bd87124bc5",
   "metadata": {},
   "source": [
    "# Hierarchical Ar(1) on a Grid Test with compositional score matching\n",
    "\n",
    "In this notebook, we will test the compositional score matching on a hierarchical problem defined on a grid.\n",
    "- The observations are on grid with `n_grid` x `n_grid` points.\n",
    "- The global parameters are the same for all grid points with hyper-priors:\n",
    "$$ \\alpha \\sim \\mathcal{N}((0, 1) \\quad\n",
    "  \\mu_\\beta \\sim \\mathcal{N}(0, 1) \\quad\n",
    "  \\log\\text{std}_\\beta \\sim \\mathcal{N}(-1, 1);$$\n",
    "\n",
    "- The local parameters are different for each grid point\n",
    "$$ \\beta_{i,j} \\sim \\mathcal{N}(\\mu_\\beta, \\text{std}_\\beta^2)$$\n",
    "\n",
    "-  In each grid point, we have a time series of `T` observations. For the time beeing, we fix $\\sigma=1$.\n",
    "$$ y_{i,j} \\sim \\mathcal{N}(\\alpha + \\beta_{i,j}y_{i,j-1}, \\sigma^2), y_{i,0} \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "- We observe $T=10$ time points for each grid point. We can also amortize over the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f9cb2cab39cb3f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "from bayesflow import diagnostics\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_model import HierarchicalScoreModel, SDE, weighting_function, euler_maruyama_sampling, adaptive_sampling, \\\n",
    "    probability_ode_solving, langevin_sampling, \\\n",
    "    generate_diffusion_time, count_parameters, train_hierarchical_score_model\n",
    "from problems.ar1_grid import AR1GridProblem, Prior, visualize_simulation_output, plot_shrinkage"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70c931844e56c698",
   "metadata": {},
   "source": "torch_device = torch.device(\"cpu\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1ac50b53a7ed912",
   "metadata": {},
   "source": [
    "prior = Prior()\n",
    "\n",
    "# test the simulator\n",
    "sim_test = prior.sample(1, n_local_samples=16)['data'][0]\n",
    "visualize_simulation_output(sim_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69e6aacbe018a801",
   "metadata": {},
   "source": [
    "batch_size = 128\n",
    "max_number_of_obs = 1  # larger than one means we condition the score on multiple observations\n",
    "\n",
    "dataset = AR1GridProblem(\n",
    "    n_data=10000,\n",
    "    prior=prior,\n",
    "    online_learning=True,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    "    amortize_time=False\n",
    ")\n",
    "\n",
    "dataset_valid = AR1GridProblem(\n",
    "    n_data=batch_size*2,\n",
    "    prior=prior,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ded414838e34143f",
   "metadata": {},
   "source": [
    "# Define diffusion model\n",
    "current_sde = SDE(\n",
    "    kernel_type=['variance_preserving', 'sub_variance_preserving'][0],\n",
    "    noise_schedule=['linear', 'cosine', 'flow_matching'][1]\n",
    ")\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=prior.n_params_global,\n",
    "    input_dim_theta_local=prior.n_params_local,\n",
    "    input_dim_x=1,\n",
    "    hidden_dim=64,\n",
    "    n_blocks=3,\n",
    "    max_number_of_obs=max_number_of_obs,\n",
    "    prediction_type=['score', 'e', 'x', 'v'][3],\n",
    "    sde=current_sde,\n",
    "    time_embed_dim=16,\n",
    "    use_film=True,\n",
    "    weighting_type=[None, 'likelihood_weighting', 'flow_matching', 'sigmoid'][1],\n",
    "    prior=prior,\n",
    "    name_prefix='AR1_'\n",
    ")\n",
    "print(score_model.name)\n",
    "count_parameters(score_model)\n",
    "\n",
    "# make dir for plots\n",
    "if not os.path.exists(f\"plots/{score_model.name}\"):\n",
    "    os.makedirs(f\"plots/{score_model.name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab5eef7cdb5ec27e",
   "metadata": {},
   "source": [
    "# train model\n",
    "loss_history = train_hierarchical_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                              epochs=500, device=torch_device)\n",
    "score_model.eval()\n",
    "torch.save(score_model.state_dict(), f\"models/{score_model.name}.pt\")\n",
    "\n",
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Mean Train')\n",
    "plt.plot(loss_history[:, 1], label='Mean Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/loss_training.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75a70bd9458b2655",
   "metadata": {},
   "source": [
    "score_model.load_state_dict(torch.load(f\"models/{score_model.name}.pt\", weights_only=True))\n",
    "score_model.eval();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e3644f3daaa5484",
   "metadata": {},
   "source": [
    "# check the error prediction: is it close to the noise?\n",
    "loss_list_target = {}\n",
    "loss_list_score = {}\n",
    "loss_list_error_w_global = {}\n",
    "loss_list_error_w_local = {}\n",
    "loss_list_error = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time = generate_diffusion_time(size=100, device=torch_device)\n",
    "    for t in diffusion_time:\n",
    "        loss_list_target[t.item()] = 0\n",
    "        loss_list_score[t.item()] = 0\n",
    "        loss_list_error_w_global[t.item()] = 0\n",
    "        loss_list_error_w_local[t.item()] = 0\n",
    "        loss_list_error[t.item()] = 0\n",
    "\n",
    "        for theta_global_batch, _, theta_local_batch, _, x_batch in dataloader_valid:\n",
    "            theta_global_batch = theta_global_batch.to(torch_device)\n",
    "            theta_local_batch = theta_local_batch.to(torch_device)\n",
    "            x_batch = x_batch.to(torch_device)\n",
    "\n",
    "            # sample from the Gaussian kernel, just learn the noise\n",
    "            epsilon_global = torch.randn_like(theta_global_batch, dtype=torch.float32, device=torch_device)\n",
    "            epsilon_local = torch.randn_like(theta_local_batch, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "            # perturb the theta batch\n",
    "            t_tensor = torch.full((theta_global_batch.shape[0], 1), t,\n",
    "                                  dtype=torch.float32, device=torch_device)\n",
    "            # perturb the theta batch\n",
    "            snr = score_model.sde.get_snr(t=t_tensor)\n",
    "            alpha, sigma = score_model.sde.kernel(log_snr=snr)\n",
    "            z_global = alpha * theta_global_batch + sigma * epsilon_global\n",
    "            if score_model.max_number_of_obs > 1:\n",
    "                # global params are not factorized to the same level as local params\n",
    "                alpha_local = alpha.unsqueeze(1)\n",
    "                sigma_local = sigma.unsqueeze(1)\n",
    "            else:\n",
    "                alpha_local = alpha\n",
    "                sigma_local = sigma\n",
    "            z_local = alpha_local * theta_local_batch + sigma_local * epsilon_local\n",
    "\n",
    "            # predict from perturbed theta\n",
    "            pred_epsilon_global, pred_epsilon_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                       time=t_tensor, x=x_batch, pred_score=False)\n",
    "            pred_score_global, pred_score_local = score_model(theta_global=z_global, theta_local=z_local,\n",
    "                                     time=t_tensor, x=x_batch, pred_score=True)\n",
    "            true_score_global = score_model.sde.grad_log_kernel(x=z_global,\n",
    "                                                                x0=theta_global_batch, t=t_tensor)\n",
    "            if score_model.max_number_of_obs == 1:\n",
    "                true_score_local = score_model.sde.grad_log_kernel(x=z_local,\n",
    "                                                                   x0=theta_local_batch,\n",
    "                                                                   t=t_tensor)\n",
    "            else:\n",
    "                true_score_local = []\n",
    "                for i in range(score_model.max_number_of_obs):\n",
    "                    score_local = score_model.sde.grad_log_kernel(x=z_local[:, i],\n",
    "                                                                   x0=theta_local_batch[:, i],\n",
    "                                                                   t=t_tensor)\n",
    "                    true_score_local.append(score_local.unsqueeze(1))\n",
    "                true_score_local = torch.concatenate(true_score_local, dim=1)\n",
    "\n",
    "            if score_model.prediction_type == 'score':\n",
    "                target_global = -epsilon_global / sigma\n",
    "                pred_target_global = -pred_epsilon_global / sigma\n",
    "                target_local = -epsilon_local / sigma_local\n",
    "                pred_target_local = -pred_epsilon_local / sigma_local\n",
    "            elif score_model.prediction_type == 'e':\n",
    "                target_global = epsilon_global\n",
    "                pred_target_global = pred_epsilon_global\n",
    "                target_local = epsilon_local\n",
    "                pred_target_local = pred_epsilon_local\n",
    "            elif score_model.prediction_type == 'v':\n",
    "                target_global = alpha*epsilon_global - sigma * theta_global_batch\n",
    "                pred_target_global = alpha*pred_epsilon_global - sigma * theta_global_batch\n",
    "                target_local = alpha_local*epsilon_local - sigma_local * theta_local_batch\n",
    "                pred_target_local = alpha_local*pred_epsilon_local - sigma_local * theta_local_batch\n",
    "            elif score_model.prediction_type == 'x':\n",
    "                target_global = theta_global_batch\n",
    "                pred_target_global = (z_global - pred_epsilon_global * sigma) / alpha\n",
    "                target_local = theta_local_batch\n",
    "                pred_target_local = (z_local - pred_epsilon_local * sigma_local) / alpha_local\n",
    "            else:\n",
    "                raise ValueError(\"Invalid prediction type.\")\n",
    "\n",
    "            # calculate the loss (sum over the last dimension, mean over the batch)\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_target_global - target_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_target_local - target_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_target[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the error of the true score\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_score_global - true_score_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_score_local - true_score_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_score[t.item()] += loss.item()\n",
    "\n",
    "            # calculate the weighted loss\n",
    "            w = weighting_function(t_tensor, sde=score_model.sde,\n",
    "                                   weighting_type=score_model.weighting_type, prediction_type=score_model.prediction_type)\n",
    "            loss_global = torch.mean(w * torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(w * torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            #loss = loss_global + loss_local\n",
    "            loss_list_error_w_global[t.item()] += loss_global.item()\n",
    "            loss_list_error_w_local[t.item()] += loss_local.item()\n",
    "\n",
    "            # check if the weighting function is correct\n",
    "            loss_global = torch.mean(torch.sum(torch.square(pred_epsilon_global - epsilon_global), dim=-1))\n",
    "            loss_local = torch.mean(torch.sum(torch.square(pred_epsilon_local - epsilon_local), dim=-1))\n",
    "            loss = loss_global + loss_local\n",
    "            loss_list_error[t.item()] += loss.item()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75adebcae950de41",
   "metadata": {},
   "source": [
    "df_target = pd.DataFrame(loss_list_target.items(), columns=['Time', 'Loss'])\n",
    "df_score = pd.DataFrame(loss_list_score.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_local = pd.DataFrame(loss_list_error_w_local.items(), columns=['Time', 'Loss'])\n",
    "df_error_w_global = pd.DataFrame(loss_list_error_w_global.items(), columns=['Time', 'Loss'])\n",
    "df_error = pd.DataFrame(loss_list_error.items(), columns=['Time', 'Loss'])\n",
    "\n",
    "# compute snr\n",
    "snr = score_model.sde.get_snr(diffusion_time)\n",
    "#upper_bound_loss = (np.sqrt(2) + 1) / (std.numpy()**2)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, sharex=True, figsize=(16, 3), tight_layout=True)\n",
    "ax[0].plot(df_target['Time'], np.log(df_target['Loss']), label=f'Unscaled {score_model.prediction_type} Loss')\n",
    "ax[1].plot(df_score['Time'], np.log(df_score['Loss']), label='Score Loss')\n",
    "#ax[1].plot(df_score['Time'], df_score['Loss'] / upper_bound_loss, label='Score Loss')\n",
    "ax[1].plot(diffusion_time, snr, label='log snr', alpha=0.5)\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']), label='Local Weighted Loss')\n",
    "ax[2].plot(df_error_w_global['Time'], np.log(df_error_w_global['Loss']), label='Global Weighted Loss')\n",
    "ax[2].plot(df_error_w_local['Time'], np.log(df_error_w_local['Loss']+df_error_w_global['Loss']), label='Weighted Loss (as in Optimization)')\n",
    "ax[3].plot(df_error['Time'], np.log(df_error['Loss']), label='Loss on Error')\n",
    "for a in ax:\n",
    "    a.set_xlabel('Diffusion Time')\n",
    "    a.set_ylabel('Log Loss')\n",
    "    a.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/losses_diffusion_time.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(diffusion_time.cpu(),\n",
    "         weighting_function(diffusion_time, sde=score_model.sde, weighting_type=score_model.weighting_type,\n",
    "                            prediction_type=score_model.prediction_type).cpu(),\n",
    "         label='weighting')\n",
    "plt.xlabel('Diffusion Time')\n",
    "plt.ylabel('Weight')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "157900e315686835",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "28d63d6aa09d9245",
   "metadata": {},
   "source": [
    "n_grid = 8\n",
    "np.random.seed(0)\n",
    "prior_dict = prior.sample(batch_size=10, n_local_samples=n_grid*n_grid)\n",
    "\n",
    "valid_prior_global, valid_prior_local, valid_data = prior_dict['global_params'], prior_dict['local_params'], prior_dict['data']\n",
    "n_post_samples = 10\n",
    "global_param_names = prior.global_param_names\n",
    "local_param_names = prior.get_local_param_names(n_grid*n_grid)\n",
    "#score_model.current_number_of_obs = 4  # we can choose here, how many observations are passed together through the score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7da728796fa5bde",
   "metadata": {},
   "source": "visualize_simulation_output(valid_data[0])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "id": "6c68dd16ce84d821",
   "metadata": {},
   "source": [
    "from importlib import reload\n",
    "import diffusion_sampling\n",
    "reload(diffusion_sampling)\n",
    "from diffusion_sampling import langevin_sampling, adaptive_sampling, euler_maruyama_sampling, probability_ode_solving"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd3d0805fc460d85",
   "metadata": {},
   "source": [
    "posterior_global_samples_valid = langevin_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                   diffusion_steps=300, langevin_steps=5, step_size_factor=0.05,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "118c0de3d3ab0b1c",
   "metadata": {},
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_langevin_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_langevin_sampler.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e906aed050d0714b",
   "metadata": {},
   "source": [
    "posterior_global_samples_valid = euler_maruyama_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                        diffusion_steps=1000, device=torch_device, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "874d1359427acaa8",
   "metadata": {},
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sampler.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_size = 10\n",
    "t1_value = mini_batch_size /( (n_grid*n_grid) //score_model.current_number_of_obs)\n",
    "t0_value = 1\n",
    "mini_batch_arg = {\n",
    "    'size': mini_batch_size,\n",
    "    #'damping_factor': lambda t: t1_value + (t0_value - t1_value) * 0.5 * (1 + torch.cos(torch.pi * t)),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * score_model.sde.kernel(log_snr=score_model.sde.get_snr(t))[1],\n",
    "    'damping_factor': lambda t: 0.1, #t1_value,\n",
    "    #'damping_factor': lambda t: t0_value * torch.exp(-np.log(t0_value / t1_value) * 2*t),\n",
    "    #'damping_factor': lambda t: t0_value + (t1_value - t0_value) * torch.sigmoid(20*(t-0.3))\n",
    "}\n",
    "#plt.plot(torch.linspace(0, 1, 100), mini_batch_arg['damping_factor'](torch.linspace(0, 1, 100)))\n",
    "#plt.show()\n",
    "\n",
    "t0_value, t1_value"
   ],
   "id": "1d00411267d3e96b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a691225901a6b91",
   "metadata": {},
   "source": [
    "posterior_global_samples_valid = euler_maruyama_sampling(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                         mini_batch_arg=mini_batch_arg,\n",
    "                                                         diffusion_steps=200, device=torch_device, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc9389a3bdf1a30d",
   "metadata": {},
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_sub_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_sub_sampler.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "id": "74e331f6f6c60522",
   "metadata": {},
   "source": [
    "posterior_global_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   #n_scores_update=10,\n",
    "                                                                   pareto_smooth_fraction=0.3,\n",
    "                                                                   diffusion_steps=100, device=torch_device)\n",
    "                                        for vd in valid_data])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef0ee96d1ec7fec2",
   "metadata": {},
   "source": [
    "fig = diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\sigma$'])\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_euler_pareto_sampler.png')\n",
    "\n",
    "fig = diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\sigma$'])\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_euler_pareto_sampler.png')"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab55fadb21762acc",
   "metadata": {},
   "source": [
    "posterior_global_samples_valid = adaptive_sampling(score_model, valid_data, n_post_samples,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "509691ae9e76856a",
   "metadata": {},
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_adaptive_sampler.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_adaptive_sampler.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d700cbffbf4d35c5",
   "metadata": {},
   "source": [
    "posterior_global_samples_valid = probability_ode_solving(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                         run_sampling_in_parallel=False,\n",
    "                                                         device=torch_device, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afe9df5c1ca137da",
   "metadata": {},
   "source": [
    "fig = diagnostics.recovery(posterior_global_samples_valid, np.array(valid_prior_global), variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/recovery_global_ode.png')\n",
    "\n",
    "fig = diagnostics.calibration_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, variable_names=global_param_names)\n",
    "fig.savefig(f'plots/{score_model.name}/ecdf_global_ode.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9de5da14663e13c4",
   "metadata": {},
   "source": [
    "conditions_global = (np.median(posterior_global_samples_valid, axis=0), posterior_global_samples_valid)[1]\n",
    "posterior_local_samples_valid = euler_maruyama_sampling(score_model, valid_data,\n",
    "                                                        n_post_samples=n_post_samples, conditions=conditions_global,\n",
    "                                                        diffusion_steps=50, device=torch_device, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c954e8ad82c5e89",
   "metadata": {},
   "source": [
    "diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          variable_names=local_param_names);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a45ca1277eefae1",
   "metadata": {},
   "source": [
    "#conditions_global = np.median(posterior_global_samples_valid, axis=1)\n",
    "posterior_local_samples_valid = probability_ode_solving(score_model, valid_data, n_post_samples=n_post_samples,\n",
    "                                                        run_sampling_in_parallel=False,\n",
    "                                                        conditions=posterior_global_samples_valid, device=torch_device, verbose=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44320e5b15ea6349",
   "metadata": {},
   "source": [
    "diagnostics.recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          variable_names=local_param_names);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ccad359370327978",
   "metadata": {},
   "source": "plot_shrinkage(posterior_global_samples_valid, posterior_local_samples_valid)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4bb5620888ab007",
   "metadata": {},
   "source": [
    "valid_id = 2\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 0]), np.std(posterior_global_samples_valid[valid_id, :, 0]))\n",
    "print('log sigma:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][0].item())\n",
    "print('log sigma:', valid_prior_global[valid_id][1].item())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9daf84cada1b0a1",
   "metadata": {},
   "source": [
    "med = np.median(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "std = np.std(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "error = (med-valid_prior_local[valid_id].numpy())**2\n",
    "visualize_simulation_output(np.stack((med, valid_prior_local[valid_id], )),\n",
    "                            title_prefix=['Posterior Median', 'True'])\n",
    "\n",
    "visualize_simulation_output(np.stack((std, error)), title_prefix=['Uncertainty', 'Error'], same_scale=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4), tight_layout=True)\n",
    "plt.errorbar(x=valid_prior_local[valid_id].flatten(), y=med.flatten(), yerr=1.96*std.flatten(), fmt='o')\n",
    "plt.plot([np.min(med), np.max(med)], [np.min(med), np.max(med)], 'k--')\n",
    "plt.axhline(np.median(posterior_global_samples_valid[valid_id, :], axis=0)[0], color='red', linestyle='--',\n",
    "            label='Global posterior mean', alpha=0.75)\n",
    "plt.ylabel('Prediction')\n",
    "plt.xlabel('True')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare to STAN",
   "id": "ad84d5cfb735c636"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_posterior_stan = np.load('problems/grid/global_posterior.npy')#[:, -100:]\n",
    "local_posterior_stan = np.load('problems/grid/local_posterior.npy')#[:, -100:]\n",
    "true_global = np.load('problems/grid/true_global.npy')\n",
    "true_local = np.load('problems/grid/true_local.npy')\n",
    "\n",
    "n_grid_stan = int(np.sqrt(true_local.shape[1]))\n",
    "\n",
    "test_data = []\n",
    "for g, l in zip(true_global, true_local):\n",
    "    sim_dict = {\n",
    "        'mu': g[0].reshape(1,1),\n",
    "        'log_sigma': g[1].reshape(1,1),\n",
    "        'theta': l.reshape(1, n_grid_stan, n_grid_stan)\n",
    "    }\n",
    "    td = simulator_test(sim_dict)['observable']\n",
    "    test_data.append(td)\n",
    "test_data = np.concatenate(test_data)\n",
    "test_data.shape"
   ],
   "id": "db32500769fea91d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mini_batch_arg = {\n",
    "    'size': 10,\n",
    "    #'damping_factor': lambda t: 0.1,\n",
    "}\n",
    "#score_model.sde.s_shift_cosine = 6\n",
    "n_post_samples = 100\n",
    "param_names = [r'$\\mu$', r'$\\log \\sigma$']\n",
    "local_param_names = [r'$\\theta_{'+str(i)+'}$' for i in range(n_grid_stan**2)]\n",
    "param_names_stan = ['STAN '+ p for p in param_names]"
   ],
   "id": "1779892d454043d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.sde.s_shift_cosine = 6\n",
    "posterior_global_samples_test = adaptive_sampling(score_model, test_data, n_post_samples,\n",
    "                                                  mini_batch_arg=mini_batch_arg,\n",
    "                                                   run_sampling_in_parallel=False,\n",
    "                                                   device=torch_device, verbose=False)"
   ],
   "id": "2e14723034b7829b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score_model.sde.s_shift_cosine = 0\n",
    "posterior_local_samples_test = euler_maruyama_sampling(score_model, test_data, n_post_samples,\n",
    "                                                 conditions=posterior_global_samples_test,\n",
    "                                                 #run_sampling_in_parallel=False,\n",
    "                                                 diffusion_steps=50,\n",
    "                                                 device=torch_device, verbose=True)"
   ],
   "id": "4dc009703a317f88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_global_samples_test, true_global, variable_names=param_names)\n",
    "diagnostics.recovery(posterior_global_samples_test, np.median(global_posterior_stan, axis=1),\n",
    "                     variable_names=param_names, xlabel='STAN Median Estimate')\n",
    "diagnostics.recovery(global_posterior_stan, true_global, variable_names=param_names_stan);"
   ],
   "id": "5e282000a6f2e1c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.calibration_ecdf(posterior_global_samples_test, true_global, difference=True, variable_names=param_names)\n",
    "diagnostics.calibration_ecdf(global_posterior_stan, true_global, difference=True, variable_names=param_names_stan);"
   ],
   "id": "ad2eb5e423de91bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "global_var = np.exp(np.median(posterior_global_samples_test, axis=1)[:, 1])[:, np.newaxis] ** 2\n",
    "shrinkage = 1-np.var(np.median(posterior_local_samples_test, axis=1), axis=1)/global_var\n",
    "\n",
    "global_var_stan = np.exp(np.median(global_posterior_stan, axis=1)[:, 1])**2\n",
    "shrinkage_stan = 1-np.var(np.median(local_posterior_stan, axis=1), axis=1)/global_var_stan\n",
    "\n",
    "true_var = np.exp(true_global)[:, 1]**2\n",
    "shrinkage_true = 1-np.var(true_local, axis=1)/true_var\n",
    "\n",
    "s_order = np.argsort(shrinkage_true)\n",
    "shrinkage = shrinkage.flatten()[s_order]\n",
    "shrinkage_stan = shrinkage_stan.flatten()[s_order]\n",
    "shrinkage_true = shrinkage_true[s_order]\n",
    "\n",
    "min_s = -10\n",
    "shrinkage[shrinkage < min_s] = min_s\n",
    "shrinkage_stan[shrinkage_stan < min_s] = min_s\n",
    "shrinkage_true[shrinkage_true < min_s] = min_s\n",
    "\n",
    "plt.title('Shrinkage')\n",
    "plt.plot(shrinkage, label='score', alpha=0.75)\n",
    "plt.plot(shrinkage_stan, label='STAN', alpha=0.75)\n",
    "plt.plot(shrinkage_true, label='true', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Correlation shrinkage score and STAN:', np.corrcoef(shrinkage, shrinkage_stan)[0, 1])\n",
    "print('Correlation shrinkage true and score:', np.corrcoef(shrinkage_true, shrinkage)[0, 1])\n",
    "print('Correlation shrinkage true and STAN:', np.corrcoef(shrinkage_true, shrinkage_stan)[0, 1])\n",
    "\n",
    "print(f\"Score shrinkage < STAN shrinkage: {(shrinkage < shrinkage_stan).sum() / shrinkage.shape[0]*100}%\")"
   ],
   "id": "47354cc8420450dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1), true_local,\n",
    "                     variable_names=local_param_names)\n",
    "diagnostics.recovery(local_posterior_stan, true_local, variable_names=local_param_names);"
   ],
   "id": "342e107e56e2c01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.recovery(posterior_local_samples_test.reshape(test_data.shape[0], n_post_samples, -1),\n",
    "                     np.median(local_posterior_stan, axis=1), ylabel='Score Based Estimates', xlabel='STAN Median Estimate');"
   ],
   "id": "342b5fe3b80d651b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(posterior_global_samples_test[:10], posterior_local_samples_test[:, :, :, np.newaxis][:10], min_max=(-10,10))",
   "id": "74ee3c08723d81d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_shrinkage(global_posterior_stan[:10], local_posterior_stan[:, :, :, np.newaxis][:10], min_max=(-10,10))",
   "id": "3a3d612f4307de53",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc9c59ae04231887",
   "metadata": {},
   "source": [
    "# Visualize the Score"
   ]
  },
  {
   "cell_type": "code",
   "id": "36b9381866daf22e",
   "metadata": {},
   "source": [
    "n_grid = 8\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(prior, n_samples=10, grid_size=n_grid,\n",
    "                                                                            normalize=False, random_seed=0)\n",
    "\n",
    "valid_id = 3\n",
    "\n",
    "diffusion_time = generate_diffusion_time(size=10, device=torch_device)\n",
    "x_valid = valid_data[valid_id].to(torch_device)\n",
    "x_valid_norm = score_model.prior.normalize_data(x_valid)\n",
    "theta_global = score_model.prior.normalize_theta(valid_prior_global[valid_id], global_params=True).cpu().numpy()  # we normalize as the score is normalized space\n",
    "print(valid_id, 'theta global', theta_global)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9dff25c3741f7d4",
   "metadata": {},
   "source": [
    "test_sample = adaptive_sampling(score_model, x_valid[None], conditions=None, n_post_samples=1, #e_abs=0.00078,\n",
    "                                t_end=0, random_seed=0, device=torch_device)\n",
    "test_sample = score_model.prior.normalize_theta(torch.tensor(test_sample), global_params=True).cpu().numpy()\n",
    "print(test_sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9053cd01f24ecf4",
   "metadata": {},
   "source": [
    "posterior_sample_path = np.array([adaptive_sampling(score_model, x_valid[None],\n",
    "                                                    conditions=None, n_post_samples=1, t_end=t,\n",
    "                                                    random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path = score_model.prior.normalize_theta(torch.tensor(posterior_sample_path), global_params=True).cpu().numpy()\n",
    "\n",
    "posterior_sample_path2 = np.array([probability_ode_solving(score_model, x_valid[None],\n",
    "                                                           conditions=None, n_post_samples=1, t_end=t, random_seed=0, device=torch_device)\n",
    "                                  for t in diffusion_time[:-1]])\n",
    "# we normalize as the score is normalized space\n",
    "posterior_sample_path2 = score_model.prior.normalize_theta(torch.tensor(posterior_sample_path2), global_params=True).cpu().numpy()\n",
    "\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "474671e4362d31cb",
   "metadata": {},
   "source": [
    "# Define grid boundaries and resolution for your 2D space.\n",
    "x_min, x_max, y_min, y_max = -1.5, 1.5, -1.5, 1.5\n",
    "grid_res = 10  # Number of points per dimension\n",
    "\n",
    "# Create a meshgrid of points\n",
    "x_vals = np.linspace(x_min, x_max, grid_res)\n",
    "y_vals = np.linspace(y_min, y_max, grid_res)\n",
    "xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "# Stack into (N,2) where N = grid_res*grid_res\n",
    "grid_points = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "# Convert grid to a torch tensor and move to device\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32, device=torch_device)\n",
    "x_valid_norm_e = x_valid_norm.reshape(10, -1).to(torch_device)\n",
    "x_valid_norm_ext = x_valid_norm_e.unsqueeze(0).repeat(grid_tensor.shape[0], 1, 1)\n",
    "\n",
    "# Dictionary to hold score outputs for each time\n",
    "scores = {}\n",
    "scores_smoothed = {}\n",
    "drifts = {}\n",
    "drifts_smoothed = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Evaluate the score model for each time value\n",
    "    for t in diffusion_time:\n",
    "        # Create a tensor of time values for each grid point\n",
    "        t_tensor = torch.full((grid_tensor.shape[0], 1), t.item(), dtype=torch.float32, device=torch_device)\n",
    "        epsilon = torch.randn_like(grid_tensor, dtype=torch.float32, device=torch_device)\n",
    "\n",
    "        # perturb theta\n",
    "        snr = score_model.sde.get_snr(t=t_tensor)\n",
    "        alpha, sigma = score_model.sde.kernel(log_snr=snr)\n",
    "        z = grid_tensor #alpha * grid_tensor + sigma * epsilon\n",
    "\n",
    "        # Evaluate the score model\n",
    "        score_indv = torch.zeros((x_valid_norm_ext.shape[2], grid_tensor.shape[0], 2))\n",
    "\n",
    "        prior_scores = (1 - t) * score_model.prior.score_global_batch(z)\n",
    "        prior_scores_indv = prior_scores.unsqueeze(0)\n",
    "        for i in range(x_valid_norm_ext.shape[2]):\n",
    "            score_indv[i] = score_model.forward_global(theta_global=z, time=t_tensor, x=x_valid_norm_ext[:, :, i].unsqueeze(-1),\n",
    "                                                       pred_score=True, clip_x=False)\n",
    "        score_indv = score_indv - prior_scores_indv\n",
    "\n",
    "        score = score_indv.sum(axis=0)\n",
    "        score = score + prior_scores\n",
    "        scores[t.item()] = score.cpu().numpy()\n",
    "\n",
    "        # score_pareto = torch.zeros_like(score)\n",
    "        # for i in range(score_indv.shape[1]):\n",
    "        #     score_pareto[i] = pareto_smooth_sum(score_indv[:, i].unsqueeze(0),\n",
    "        #                                            tail_fraction=0.3)[0]  # expects dim to be the batch\n",
    "        #\n",
    "        # score_pareto = score_pareto + prior_scores\n",
    "        # scores_smoothed[t.item()] = score_pareto.cpu().numpy()\n",
    "\n",
    "        f, g = score_model.sde.get_f_g(x=z, t=t_tensor)\n",
    "        drift = f - 0.5 * torch.square(g) * scores[t.item()]\n",
    "        drifts[t.item()] = drift.cpu().numpy()\n",
    "\n",
    "        # drift_pareto = f - 0.5 * torch.square(g) * scores_smoothed[t.item()]\n",
    "        # drifts_smoothed[t.item()] = drift_pareto.cpu().numpy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8458bd11effe5a4e",
   "metadata": {},
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(scores.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)  # negative since we are plotting the reverse score\n",
    "\n",
    "    ax = axes[i]\n",
    "\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    if i != 0:\n",
    "        j = i\n",
    "        h2, = ax.plot(posterior_sample_path[:-j, 0, 0, 0], posterior_sample_path[:-j, 0, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "        h4, = ax.plot(posterior_sample_path2[:-j, 0, 0, 0], posterior_sample_path2[:-j, 0, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=5*n_grid*n_grid, alpha=.75, label='Score')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5, h0, h1, h2, h4], loc='lower center', ncols=5, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/score.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "287ac0f1bf8a8ee7",
   "metadata": {},
   "source": [
    "# Plot the vector field (score) for each time step using subplots\n",
    "nrows = 2\n",
    "fig, axes = plt.subplots(nrows, len(diffusion_time) // nrows, sharex=True, sharey=True,\n",
    "                         figsize=(15, 3*nrows), tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (t_val, score_val) in enumerate(sorted(drifts.items(), reverse=True)):\n",
    "    # Reshape score components back to (grid_res, grid_res) for quiver plotting\n",
    "    U = score_val[:, 0].reshape(grid_res, grid_res)\n",
    "    V = score_val[:, 1].reshape(grid_res, grid_res)\n",
    "\n",
    "    ax = axes[i]\n",
    "    h0, = ax.plot(0, 0, 'o', color='black', label='Latent Prior')\n",
    "    h1, = ax.plot(theta_global[0], theta_global[1], 'ro', label='True Parameter')\n",
    "\n",
    "    j = len(diffusion_time)-i-1\n",
    "    h2, = ax.plot(posterior_sample_path[j:, 0, 0, 0], posterior_sample_path[j:, 0, 0, 1], 'o-', label='Posterior Path Sampling', alpha=0.5)\n",
    "    h4, = ax.plot(posterior_sample_path2[j:, 0, 0, 0], posterior_sample_path2[j:, 0, 0, 1], 'o-', label='Posterior Path ODE', alpha=0.5)\n",
    "    h5 = ax.quiver(xx, yy, U, V, color='blue', angles='xy', scale_units='xy', scale=5*n_grid*n_grid, alpha=.75,\n",
    "                   label='Probability Flow')\n",
    "    ax.set_title(f\"Diffusion t = {t_val:.3f}\")\n",
    "    ax.set_xlim(x_min-0.5, x_max+0.5)\n",
    "    ax.set_ylim(y_min-0.5, y_max+0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    #ax.legend()\n",
    "    #print(posterior_sample_path[i, 0, 0], posterior_sample_path[i, 0, 1])\n",
    "fig.legend(handles=[h5, h0, h1, h2, h4], loc='lower center', ncols=5, bbox_to_anchor=(0.5, -0.05))\n",
    "plt.savefig(f'plots/{score_model.name}/drift.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('theta global', theta_global, posterior_sample_path[0].flatten())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be293168e0abdda5",
   "metadata": {},
   "source": [
    "# Step Size for different Grid Sizes"
   ]
  },
  {
   "cell_type": "code",
   "id": "4936602cfa640068",
   "metadata": {},
   "source": [
    "# check number of steps needed for different number of data points\n",
    "n_steps = {}\n",
    "n_steps_per_grid = {}\n",
    "n_steps_rmse = {}\n",
    "n_steps_calibration = {}\n",
    "n_steps_contraction = {}\n",
    "\n",
    "score_model.current_number_of_obs = 1 #max_number_of_obs\n",
    "grid_sizes = [4, 8, 12, 16, 32]\n",
    "#grid_sizes = [10, 20, 30, 40, 100] #[5, 10, 15, 30, 70] # [1, 3, 4, 10, 32]\n",
    "mini_batch_arg = {\n",
    "    'size': 10,\n",
    "    'damping_factor': lambda t: 0.1,\n",
    "}\n",
    "sampling_types = [('normal', None), ('subsample', mini_batch_arg)]\n",
    "\n",
    "for sampling_type in sampling_types:\n",
    "    print(sampling_type)\n",
    "    n_steps[sampling_type[0]] = []\n",
    "    n_steps_per_grid[sampling_type[0]] = []\n",
    "    n_steps_rmse[sampling_type[0]] = []\n",
    "    n_steps_calibration[sampling_type[0]] = []\n",
    "    n_steps_contraction[sampling_type[0]] = []\n",
    "    for n in grid_sizes:\n",
    "        true_params, _, valid_data = generate_synthetic_data(prior, n_samples=10, grid_size=n,\n",
    "                                                   normalize=False, random_seed=0)\n",
    "        test_sample, list_steps = adaptive_sampling(score_model, valid_data, conditions=None, n_post_samples=25,\n",
    "                                                    mini_batch_arg=sampling_type[1],\n",
    "                                                    run_sampling_in_parallel=False,\n",
    "                                                    t_end=0, random_seed=0, device=torch_device,\n",
    "                                                    return_steps=True)\n",
    "\n",
    "        n_steps_per_grid[sampling_type[0]].append(list_steps)\n",
    "        n_steps[sampling_type[0]].append(len(list_steps))\n",
    "\n",
    "        rmse = diagnostics.root_mean_squared_error(test_sample, np.array(true_params))['values']\n",
    "        n_steps_rmse[sampling_type[0]].append(rmse)\n",
    "\n",
    "        cali = diagnostics.calibration_error(test_sample, np.array(true_params))['values']\n",
    "        n_steps_calibration[sampling_type[0]].append(cali)\n",
    "\n",
    "        contract = diagnostics.posterior_contraction(test_sample, np.array(true_params))['values']\n",
    "        n_steps_contraction[sampling_type[0]].append(contract)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12aa82da545123ec",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "for sampling_type in sampling_types:\n",
    "    plt.plot(grid_sizes, n_steps[sampling_type[0]], 'o-', label=sampling_type[0])\n",
    "\n",
    "if score_model.max_number_of_obs > 1:\n",
    "    plt.title(f'Number of Steps for Different Grid Sizes ({score_model.current_number_of_obs} Obs)')\n",
    "else:\n",
    "    plt.title('Number of Steps for Different Grid Sizes')\n",
    "#plt.plot(grid_sizes, n_steps, 'o-', label='#Steps')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Number of Steps')\n",
    "plt.yscale('log')\n",
    "plt.ylim(1, 1e4)\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/number_of_steps_vs_grid_size{score_model.current_number_of_obs}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "for sampling_type in sampling_types:\n",
    "    plt.plot(np.array(grid_sizes)**2/score_model.current_number_of_obs,\n",
    "             n_steps[sampling_type[0]], 'o-', label=sampling_type[0])\n",
    "\n",
    "if score_model.max_number_of_obs > 1:\n",
    "    plt.title(f'Number of Steps for Different Grid Sizes ({score_model.current_number_of_obs} Obs)')\n",
    "else:\n",
    "    plt.title('Number of Steps for Different Grid Sizes')\n",
    "#plt.plot(grid_sizes, n_steps, 'o-', label='#Steps')\n",
    "plt.xlabel('#Observations (divided by number of obs used to predict score)')\n",
    "plt.ylabel('Number of Steps')\n",
    "plt.yscale('log')\n",
    "plt.ylim(1, 1e4)\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/number_of_steps_vs_n_obs{score_model.current_number_of_obs}.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3), tight_layout=True)\n",
    "for sampling_type in sampling_types:\n",
    "    plt.plot(grid_sizes, n_steps_error[sampling_type[0]], 'o-', label=sampling_type[0])\n",
    "plt.title('Error for Different Grid Sizes')\n",
    "#plt.plot(grid_sizes, n_steps, 'o-', label='#Steps')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'plots/{score_model.name}/error_vs_grid_size{score_model.current_number_of_obs}.png')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, len(sampling_types), sharey=True, figsize=(4*len(sampling_types), 4), tight_layout=True)\n",
    "for a, sampling_type in zip(ax, sampling_types):\n",
    "    a.set_title(sampling_type[0])\n",
    "    for i, n in zip(grid_sizes, n_steps_per_grid[sampling_type[0]]):\n",
    "        a.plot(n, label=f'{i}x{i}')\n",
    "    a.set_xlabel('Step')\n",
    "    a.set_ylabel('Step Size')\n",
    "    a.set_yscale('log')\n",
    "    a.legend(loc='lower right')\n",
    "    a.set_ylim(1e-6, 1)\n",
    "plt.savefig(f'plots/{score_model.name}/n_steps_per_grid{score_model.current_number_of_obs}.png')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "319131e047b1445b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hierarchical-abi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
