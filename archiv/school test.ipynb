{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch_device = torch.device(\"cpu\")\n",
    "torch_device"
   ],
   "id": "70c931844e56c698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# eight schools problem\n",
    "J = 8\n",
    "y = np.array([28., 8, -3, 7, -1, 1, 18, 12])[:, np.newaxis]  # our groups, one observation per group\n",
    "#y = np.repeat(y[:, np.newaxis].T, 2, axis=0).flatten()\n",
    "sigma = np.array([15., 10, 16, 11, 9, 11, 10, 18])  # assumed to be known\n",
    "#sigma = np.repeat(sigma[:, np.newaxis].T, 2, axis=0).flatten()  # sigma is known, so part of the observation\n",
    "n_obs_per_group = 2  # sigma is known, so part of the observation"
   ],
   "id": "18833b2ff27d2d9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(range(J), y.flatten(), yerr=sigma)\n",
    "plt.title(\"8 Schools treatment effects\")\n",
    "plt.xlabel(\"School\")\n",
    "plt.ylabel(\"Treatment effect\")\n",
    "plt.show()"
   ],
   "id": "ccc8c41315afeeb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulator example\n",
    "def simulator(params, school_i=None):\n",
    "    batch_size = params['theta_j'].shape[0]\n",
    "    y_j = np.random.normal(loc=params['theta_j'], scale=sigma, size=(batch_size, J))\n",
    "    if school_i is None:\n",
    "        return dict(observable=y_j, sigma=np.tile(sigma, (batch_size, 1)))\n",
    "    return dict(observable=y_j[:, school_i][:, np.newaxis], sigma=(np.ones(batch_size)*sigma[school_i])[:, np.newaxis])\n",
    "\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        self.mu_mean = 0\n",
    "        self.mu_std = 3\n",
    "        self.log_tau_mean = 3\n",
    "        self.log_tau_std = 1\n",
    "\n",
    "        np.random.seed(0)\n",
    "        test_prior = self.sample(1000)\n",
    "        test = simulator(test_prior, school_i=None)\n",
    "        self.x_mean = torch.tensor(np.array([np.mean(test['observable']), np.mean(test['sigma'])]),\n",
    "                                   dtype=torch.float32, device=torch_device)\n",
    "        self.x_std = torch.tensor(np.array([np.std(test['observable']), np.std(test['sigma'])]),\n",
    "                                  dtype=torch.float32, device=torch_device)\n",
    "        self.prior_global_mean = torch.tensor(np.array([np.mean(test_prior['mu']), np.mean(test_prior['log_tau'])]),\n",
    "                                              dtype=torch.float32, device=torch_device)\n",
    "        self.prior_global_std = torch.tensor(np.array([np.std(test_prior['mu']), np.std(test_prior['log_tau'])]),\n",
    "                                             dtype=torch.float32, device=torch_device)\n",
    "        self.prior_local_mean = torch.tensor(np.array([np.mean(test_prior['theta_j'])]),\n",
    "                                             dtype=torch.float32, device=torch_device)\n",
    "        self.prior_local_std = torch.tensor(np.array([np.std(test_prior['theta_j'])]),\n",
    "                                            dtype=torch.float32, device=torch_device)\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample(batch_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        mu = np.random.normal(loc=self.mu_mean, scale=self.mu_std, size=(batch_size,1))\n",
    "        log_tau = np.random.normal(loc=self.log_tau_mean, scale=self.log_tau_std, size=(batch_size,1))\n",
    "        theta_j = np.random.normal(loc=mu, scale=np.exp(log_tau), size=(batch_size, J))\n",
    "        return dict(mu=mu, log_tau=log_tau, theta_j=theta_j)\n",
    "\n",
    "    def score_global_batch(self, theta_batch_norm, condition_norm=None):\n",
    "        \"\"\" Computes the global score for a batch of parameters.\"\"\"\n",
    "        theta_batch = theta_batch_norm * self.prior_global_std + self.prior_global_mean\n",
    "        mu, log_tau = theta_batch[..., 0], theta_batch[..., 1]\n",
    "        grad_logp_mu = -(mu - self.mu_mean) / (self.mu_std**2)\n",
    "        grad_logp_tau = -(log_tau - self.log_tau_mean) / (self.log_tau_std**2)\n",
    "        # correct the score for the normalization\n",
    "        score = torch.stack([grad_logp_mu, grad_logp_tau], dim=-1)\n",
    "        return score / self.prior_global_std\n",
    "\n",
    "    def score_local_batch(self, theta_batch_norm, condition_norm):\n",
    "        \"\"\" Computes the local score for a batch of samples. \"\"\"\n",
    "        theta_j = theta_batch_norm * self.prior_local_std + self.prior_local_mean\n",
    "        condition = condition_norm * self.prior_global_std + self.prior_global_mean\n",
    "        mu, log_tau = condition[..., 0], condition[..., 1]\n",
    "        # Gradient w.r.t theta_j conditioned on mu and log_tau\n",
    "        grad_logp_theta_j = -(theta_j - mu) / torch.exp(log_tau*2)\n",
    "        # correct the score for the normalization\n",
    "        score = grad_logp_theta_j / self.prior_local_std\n",
    "        return score\n",
    "\n",
    "prior = Prior()\n",
    "n_params_global = 2\n",
    "n_params_local = 1"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prior(2)",
   "id": "557005f1016c5ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "simulator(prior(2))",
   "id": "98fbfb04214412b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def positional_encoding(t, d_model, max_t=1000.0):  # todo: before 10000\n",
    "    \"\"\"\n",
    "    Computes the sinusoidal positional encoding for a given time t.\n",
    "\n",
    "    Args:\n",
    "        t (torch.Tensor): The input time tensor of shape (batch_size, 1).\n",
    "        d_model (int): The dimensionality of the embedding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The positional encoding of shape (batch_size, d_model).\n",
    "    \"\"\"\n",
    "    half_dim = d_model // 2\n",
    "    div_term = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=t.device) *\n",
    "                         -(math.log(max_t) / (half_dim - 1)))\n",
    "    t_proj = t * div_term\n",
    "    pos_enc = torch.cat([torch.sin(t_proj), torch.cos(t_proj)], dim=-1)\n",
    "    return pos_enc\n",
    "\n",
    "# Define the Score Model\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout_rate=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.norm = nn.LayerNorm(out_features)\n",
    "        self.activation = nn.SiLU()  # same as swish\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.proj = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "\n",
    "    @staticmethod\n",
    "    def swish(x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.proj(x)\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        return out + identity\n",
    "\n",
    "class ConditionalResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, cond_dim, dropout=0.1):\n",
    "        super(ConditionalResidualBlock, self).__init__()\n",
    "        # First linear layer that takes [hidden state; conditioning]\n",
    "        self.fc1 = nn.Linear(hidden_dim + cond_dim, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        # Second linear layer also takes [hidden state; conditioning]\n",
    "        self.fc2 = nn.Linear(hidden_dim + cond_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.activation = nn.SiLU()  # same as swish\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def swish(x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, h, cond):\n",
    "        # Concatenate the hidden state with the conditioning vector\n",
    "        x = torch.cat([h, cond], dim=-1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        # Inject conditioning again before the second transformation\n",
    "        out = self.fc2(torch.cat([out, cond], dim=-1))\n",
    "        out = self.norm2(out)\n",
    "        # Add the original hidden state (skip connection) and apply activation\n",
    "        return self.activation(out + h)\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network model that computes score estimates.\n",
    "\n",
    "        Args:\n",
    "            input_dim_theta (int): Input dimension for theta.\n",
    "            input_dim_x (int): Input dimension for x.\n",
    "            input_dim_condition (int): Input dimension for the condition. Can be 0 for global score.\n",
    "            hidden_dim (int): Hidden dimension for theta network.\n",
    "            time_embed_dim (int, optional): Dimension of time embedding. Defaults to 4.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim_theta, input_dim_x, input_dim_condition,\n",
    "                 hidden_dim,\n",
    "                 time_embed_dim=16):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "\n",
    "        # Define the dimension of the conditioning vector\n",
    "        cond_dim = input_dim_x + input_dim_condition + time_embed_dim\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        # Project the concatenation of theta and the condition into hidden_dim\n",
    "        self.input_layer = nn.Linear(input_dim_theta + cond_dim, hidden_dim)\n",
    "\n",
    "        # Create a sequence of conditional residual blocks\n",
    "        self.block1 = ConditionalResidualBlock(hidden_dim, cond_dim)\n",
    "        self.block2 = ConditionalResidualBlock(hidden_dim, cond_dim)\n",
    "        self.block3 = ConditionalResidualBlock(hidden_dim, cond_dim)\n",
    "\n",
    "        # Create a sequence of residual blocks\n",
    "        #self.block1 = ResidualBlock(input_dim_theta + cond_dim, hidden_dim)\n",
    "        #self.block2 = ResidualBlock(hidden_dim, hidden_dim)\n",
    "        #self.block3 = ResidualBlock(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Final layer to get back to the theta dimension\n",
    "        self.final_linear = nn.Linear(hidden_dim, input_dim_theta)\n",
    "\n",
    "    def forward(self, theta, t, x, conditions=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the ScoreModel.\n",
    "\n",
    "        Args:\n",
    "            theta (torch.Tensor): Input theta tensor of shape (batch_size, input_dim_theta).\n",
    "            t (torch.Tensor): Input time tensor of shape (batch_size, 1).\n",
    "            x (torch.Tensor): Input x tensor of shape (batch_size, input_dim_x).\n",
    "            conditions (torch.Tensor, optional): Input condition tensor of shape (batch_size, input_dim_condition).\n",
    "                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the score model.\n",
    "        \"\"\"\n",
    "        # Compute a time embedding (shape: [batch, time_embed_dim])\n",
    "        t_emb = positional_encoding(t, self.time_embed_dim)\n",
    "\n",
    "        # Form the conditioning vector. If conditions is None, only x and time are used.\n",
    "        if conditions is not None:\n",
    "            cond = torch.cat([x, conditions, t_emb], dim=-1)\n",
    "        else:\n",
    "            cond = torch.cat([x, t_emb], dim=-1)\n",
    "\n",
    "        # Concatenate theta with the conditioning vector as the initial input\n",
    "        h = torch.cat([theta, cond], dim=-1)\n",
    "        h = self.input_layer(h)\n",
    "\n",
    "        # Pass through each residual block, injecting the same cond at each layer\n",
    "        h = self.block1(h, cond)\n",
    "        h = self.block2(h, cond)\n",
    "        h = self.block3(h, cond)\n",
    "\n",
    "        theta_emb = self.final_linear(h)\n",
    "        return theta_emb\n",
    "\n",
    "\n",
    "class HierarchicalScoreModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim_theta_local, input_dim_theta_global, input_dim_x,\n",
    "                 hidden_dim,\n",
    "                 time_embed_dim=16):\n",
    "        super(HierarchicalScoreModel, self).__init__()\n",
    "        self.n_params_global = input_dim_theta_global\n",
    "        self.global_model = ScoreModel(\n",
    "            input_dim_theta=input_dim_theta_global,\n",
    "            input_dim_x=input_dim_x,\n",
    "            input_dim_condition=0,\n",
    "            hidden_dim=hidden_dim,\n",
    "            time_embed_dim=time_embed_dim\n",
    "        )\n",
    "        self.n_params_local = input_dim_theta_local\n",
    "        self.local_model = ScoreModel(\n",
    "            input_dim_theta=input_dim_theta_local,\n",
    "            input_dim_x=input_dim_x,\n",
    "            input_dim_condition=input_dim_theta_global,\n",
    "            hidden_dim=hidden_dim,\n",
    "            time_embed_dim=time_embed_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, theta, t, x):\n",
    "        theta_global, theta_local = torch.split(theta, [self.n_params_global, self.n_params_local], dim=-1)\n",
    "        global_out = self.global_model.forward(theta=theta_global, t=t, x=x, conditions=None)\n",
    "        local_out = self.local_model.forward(theta=theta_local, t=t, x=x, conditions=theta_global)\n",
    "        return torch.cat([global_out, local_out], dim=-1)\n",
    "\n",
    "    def forward_local(self, theta_local, theta_global, t, x):\n",
    "        local_out = self.local_model.forward(theta=theta_local, t=t, x=x, conditions=theta_global)\n",
    "        return local_out\n",
    "\n",
    "    def forward_global(self, theta_global, t, x):\n",
    "        global_out = self.global_model.forward(theta=theta_global, t=t, x=x, conditions=None)\n",
    "        return global_out"
   ],
   "id": "6d13bcb1279b6f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_synthetic_data(n_samples, schools_joint=False, device=None):\n",
    "    params_global = []\n",
    "    params_local = []\n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        batch_params = prior(1)\n",
    "        if schools_joint:\n",
    "            sim_batch = simulator(batch_params)\n",
    "            param_global = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1),\n",
    "                                         dtype=torch.float32, device=device)\n",
    "            param_local = torch.tensor(batch_params['theta_j'], dtype=torch.float32, device=device)\n",
    "            x = torch.tensor(np.stack((sim_batch['observable'], sim_batch['sigma']), axis=-1),\n",
    "                             dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            sim_batch = simulator(batch_params, school_i=i % J)\n",
    "            param_global = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=-1),\n",
    "                                         dtype=torch.float32, device=device)\n",
    "            param_local = torch.tensor(batch_params['theta_j'][:, i % J][:, np.newaxis], dtype=torch.float32, device=device)\n",
    "            x = torch.tensor(np.concatenate((sim_batch['observable'], sim_batch['sigma']), axis=-1),\n",
    "                             dtype=torch.float32, device=device)\n",
    "\n",
    "        params_global.append(param_global)\n",
    "        params_local.append(param_local)\n",
    "        data.append(x)\n",
    "    param_global = torch.concatenate(params_global)\n",
    "    param_local = torch.concatenate(params_local)\n",
    "    data = torch.concatenate(data)\n",
    "    return param_global, param_local, data"
   ],
   "id": "ed94262e2cbec749",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_schedule_diffusion_time(t, max_t, s=0.008):\n",
    "    return torch.cos(((t/max_t + s) / (1 + s)) * (np.pi / 2)) ** 2\n",
    "\n",
    "def generate_diffusion_time(max_t, steps, random=False, device=None):\n",
    "    \"\"\"variance based on the Improved Denoising Diffusion Probabilistic Models paper\"\"\"\n",
    "    if random:\n",
    "        time = torch.rand(steps+1, dtype=torch.float32, device=device) * max_t\n",
    "        time = torch.sort(time)[0]\n",
    "    else:\n",
    "        time = torch.linspace(0, max_t, steps+1, dtype=torch.float32, device=device)\n",
    "\n",
    "    # gamma called alpha_t in paper\n",
    "    f_0 = 1#cosine_schedule_diffusion_time(torch.tensor(0, dtype=torch.float32, device=device), max_t)\n",
    "    gamma = cosine_schedule_diffusion_time(time, max_t) / f_0\n",
    "    beta_t = 1 - torch.cat((gamma[0:1], gamma[1:] / gamma[:-1]), dim=0)\n",
    "    # clip to avoid numerical instability\n",
    "    beta_t = torch.clamp(beta_t, max=0.999)\n",
    "    return time, gamma, beta_t"
   ],
   "id": "b506683f73d8dd26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time, gamma, delta_t = generate_diffusion_time(400, 400)\n",
    "\n",
    "plt.plot(time / 400, delta_t / (1-gamma), label='weight')\n",
    "plt.plot(time / 400, delta_t, label='delta_t')\n",
    "plt.plot(time / 400, gamma, label='gamma')\n",
    "plt.plot(time / 400, -1 / torch.sqrt(1 - gamma) * delta_t, label='sampling step')\n",
    "#plt.plot(time, torch.linspace(1, 0, 400+1, dtype=torch.float32)[1:-1], label='gamma_lin')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7bb7a3e5508f208a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Loss function for weighted MSE\n",
    "def weighted_mse_loss(inputs, targets, weights):\n",
    "    return torch.mean(weights * (inputs - targets) ** 2)\n",
    "\n",
    "\n",
    "def compute_score_loss(x_batch, theta_prime_batch, model, diffusion_time, gamma, delta_t, device=None):\n",
    "    # sample a diffusion time for each sample in the batch\n",
    "    t_index = torch.randint(0, len(diffusion_time), size=(x_batch.shape[0],), device=device)\n",
    "    t = diffusion_time[t_index]\n",
    "    g = gamma[t_index]\n",
    "    w = delta_t[t_index]\n",
    "    # sample from the Gaussian kernel, just learn the noise\n",
    "    epsilon = torch.randn_like(theta_prime_batch, dtype=torch.float32, device=device)\n",
    "    theta_batch = torch.sqrt(g) * theta_prime_batch + torch.sqrt(1 - g) * epsilon\n",
    "    # calculate the score for the sampled theta\n",
    "    score_pred = model(theta=theta_batch, t=t, x=x_batch)\n",
    "    # calculate the loss\n",
    "    loss = weighted_mse_loss(score_pred, epsilon, weights=w / (1 - g))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, dataloader_valid=None,\n",
    "                      T=400, epochs=100, lr=1e-3, steps_diffusion_time=400, device=None):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Add Cosine Annealing Scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # Generate diffusion time and step size\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(max_t=T, steps=steps_diffusion_time, device=device)\n",
    "\n",
    "    # Add a new dimension so that each tensor has shape (steps, 1)\n",
    "    diffusion_time = diffusion_time.unsqueeze(1)\n",
    "    gamma = gamma.unsqueeze(1)\n",
    "    delta_t = delta_t.unsqueeze(1)\n",
    "\n",
    "    # Training loop\n",
    "    loss_history = np.zeros((epochs, 2))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        # for each sample in the batch, calculate the loss for a random diffusion time\n",
    "        for theta_global_prime_batch, theta_local_prime_batch, x_batch in dataloader:\n",
    "            # initialize the gradients\n",
    "            optimizer.zero_grad()\n",
    "            theta_prime_batch = torch.concat([theta_global_prime_batch, theta_local_prime_batch], dim=-1)\n",
    "            # calculate the loss\n",
    "            loss = compute_score_loss(x_batch=x_batch, theta_prime_batch=theta_prime_batch,\n",
    "                                      model=model, diffusion_time=diffusion_time, gamma=gamma, delta_t=delta_t, device=device)\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # validate the model\n",
    "        valid_loss = []\n",
    "        if dataloader_valid is not None:\n",
    "            for theta_global_prime_batch, theta_local_prime_batch, x_batch in dataloader_valid:\n",
    "                with torch.no_grad():\n",
    "                    theta_prime_batch = torch.concat([theta_global_prime_batch, theta_local_prime_batch], dim=-1)\n",
    "                    loss = compute_score_loss(x_batch, theta_prime_batch=theta_prime_batch,\n",
    "                                              model=model, diffusion_time=diffusion_time, gamma=gamma, delta_t=delta_t, device=device)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "        loss_history[epoch] = [np.median(total_loss), np.median(valid_loss)]\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {np.median(total_loss):.4f}, \"\n",
    "              f\"Valid Loss: {np.median(valid_loss):.4f}\", end='\\r')\n",
    "    return loss_history"
   ],
   "id": "3e4488af88ad4738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "n_samples = 50000\n",
    "batch_size = 256\n",
    "T = 400\n",
    "steps_time = T\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=n_params_global,\n",
    "    input_dim_theta_local=n_params_local,\n",
    "    input_dim_x=n_obs_per_group,\n",
    "    hidden_dim=64\n",
    ")\n",
    "score_model.to(torch_device)\n",
    "\n",
    "\n",
    "# Create model and dataset\n",
    "thetas_global, thetas_local, xs = generate_synthetic_data(n_samples, schools_joint=False, device=torch_device)  # todo: False\n",
    "# Normalize data\n",
    "thetas_global = (thetas_global - prior.prior_global_mean) / prior.prior_global_std\n",
    "thetas_local = (thetas_local - prior.prior_local_mean) / prior.prior_local_std\n",
    "xs = (xs - prior.x_mean) / prior.x_std\n",
    "#xs = xs.reshape(-1, n_obs_per_group*J)  # todo\n",
    "\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(thetas_global, thetas_local, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create validation data\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(1000, schools_joint=False, device=torch_device)  # todo: False\n",
    "valid_data = (valid_data - prior.x_mean) / prior.x_std\n",
    "#valid_data = valid_data.reshape(-1, n_obs_per_group*J)  # todo\n",
    "valid_prior_global = (valid_prior_global - prior.prior_global_mean) / prior.prior_global_std\n",
    "valid_prior_local = (valid_prior_local - prior.prior_local_mean) / prior.prior_local_std\n",
    "dataset_valid = TensorDataset(valid_prior_global, valid_prior_local, valid_data)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "score_model.train()\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 T=T, epochs=100, lr=1e-3, steps_diffusion_time=steps_time,\n",
    "                                 device=torch_device)\n",
    "score_model.eval();"
   ],
   "id": "ab5eef7cdb5ec27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Train')\n",
    "plt.plot(loss_history[:, 1], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "aca578e1fb265da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Annealed Langevin Dynamics for Sampling\n",
    "def langevin_sampling(model, x_obs, n_post_samples, conditions=None, steps=5, device=None):\n",
    "    x_obs_norm = (x_obs - prior.x_mean) / prior.x_std  # assumes x_obs is not standardized\n",
    "    #x_obs_norm = x_obs_norm.reshape(-1, n_obs_per_group*J)  # todo\n",
    "    n_obs = x_obs_norm.shape[0]\n",
    "\n",
    "    # Ensure x_obs_norm is a PyTorch tensor\n",
    "    if not isinstance(x_obs_norm, torch.Tensor):\n",
    "        x_obs_norm = torch.tensor(x_obs_norm, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Initialize parameters\n",
    "    if conditions is None:  # global\n",
    "        n_params = n_params_global\n",
    "        theta = torch.randn(n_post_samples, n_params_global, dtype=torch.float32, device=device) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32, device=device)) # todo:\n",
    "        conditions_exp = None\n",
    "    else:\n",
    "        # Ensure conditions is a PyTorch tensor\n",
    "        if not isinstance(conditions, torch.Tensor):\n",
    "            conditions = torch.tensor(conditions, dtype=torch.float32, device=device)\n",
    "\n",
    "        n_params = n_params_local*n_obs\n",
    "        theta = torch.randn(n_post_samples, n_obs, n_params_local, dtype=torch.float32, device=device)\n",
    "        conditions = (conditions - prior.prior_global_mean) / prior.prior_global_std\n",
    "        conditions_exp = conditions.unsqueeze(0).expand(n_post_samples, n_obs, -1).reshape(-1, n_params_global)\n",
    "\n",
    "    # Generate diffusion time parameters\n",
    "    diffusion_time, gamma, delta_t = generate_diffusion_time(max_t=T, steps=steps_time, device=device)\n",
    "    scaling = -1 / torch.sqrt(1 - gamma)\n",
    "\n",
    "\n",
    "\n",
    "    # Expand x_obs_norm to match the number of posterior samples\n",
    "    x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, -1, -1)  # Shape: (n_post_samples, n_obs, d)\n",
    "    x_expanded = x_exp.reshape(-1, x_obs_norm.shape[-1])\n",
    "\n",
    "    # Reverse iterate over diffusion times and step sizes\n",
    "    for step_size, t, scale in tqdm(zip(delta_t.flip(0), diffusion_time.flip(0), scaling.flip(0)), total=steps_time+1):\n",
    "        # Create tensor for current time step\n",
    "        t_tensor = torch.full((n_post_samples, 1), t, dtype=torch.float32, device=device)\n",
    "        t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Sample Gaussian noise\n",
    "            eps = torch.randn_like(theta, dtype=torch.float32, device=device)\n",
    "\n",
    "            if conditions is None:  # todo: global\n",
    "                # Compute prior score\n",
    "                prior_score = prior.score_global_batch(theta)\n",
    "            else:\n",
    "                prior_score = 0\n",
    "\n",
    "            # Compute model scores\n",
    "            if conditions is None:\n",
    "                theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, n_params_global)\n",
    "                model_scores = model.forward_global(theta_global=theta_exp, t=t_exp, x=x_expanded)\n",
    "                # Sum over observations\n",
    "                model_scores = model_scores.reshape(n_post_samples, n_obs, -1).sum(dim=1)  # todo\n",
    "                #print(model_scores.shape, theta_exp.shape, theta.shape)\n",
    "            else:\n",
    "                theta_exp = theta.reshape(-1, n_params_local)\n",
    "                model_scores = model.forward_local(theta_local=theta_exp, t=t_exp, x=x_expanded, theta_global=conditions_exp)\n",
    "                model_scores = model_scores.reshape(n_post_samples, n_obs, -1)\n",
    "\n",
    "            # Compute updated scores and perform Langevin step\n",
    "            # scaling since we learned the scaled score network (noise)\n",
    "            scores = (1 - n_obs) * (T - t) / T * prior_score + scale * model_scores\n",
    "            theta = theta + (step_size / 2) * scores + torch.sqrt(step_size) * eps\n",
    "        if torch.isnan(theta).any():\n",
    "            print(\"NaNs in theta\")\n",
    "            break\n",
    "    # correct for normalization\n",
    "    if conditions is None:\n",
    "        theta = theta * prior.prior_global_std + prior.prior_global_mean\n",
    "    else:\n",
    "        theta = theta * prior.prior_local_std + prior.prior_local_mean\n",
    "    # convert to numpy\n",
    "    theta = theta.detach().numpy().reshape(n_post_samples, n_params)\n",
    "    return theta"
   ],
   "id": "bd3241835d991fc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(10, schools_joint=True, device=torch_device)",
   "id": "eafc62be726c8506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=100, steps=1,\n",
    "                                                          device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$']);",
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$']);"
   ],
   "id": "c12c415e037a9277",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conditions_global = np.median(posterior_global_samples_valid, axis=0)\n",
    "posterior_local_samples_valid = np.array([langevin_sampling(score_model, vd, n_post_samples=100, conditions=c, steps=1,\n",
    "                                                          device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, conditions_global)])"
   ],
   "id": "fb0c12e449219594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_local_samples_valid, np.array(valid_prior_local), param_names=[f'$\\\\theta_{i}$' for i in range(J)]);",
   "id": "44320e5b15ea6349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_local_samples_valid, np.array(valid_prior_local),\n",
    "                          difference=True, param_names=[f'$\\\\theta_{i}$' for i in range(J)]);"
   ],
   "id": "1708cd7d0c011284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Apply on Data",
   "id": "b4a8277f98f6eae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate posterior samples\n",
    "test_data = torch.tensor(np.concatenate((y, sigma[:, np.newaxis]), axis=-1), dtype=torch.float32, device=torch_device)\n",
    "if train_global:\n",
    "    conditions = None\n",
    "else:\n",
    "    conditions = torch.tensor(np.array([6.78, 1.78]), dtype=torch.float32, device=torch_device)\n",
    "posterior_samples = langevin_sampling(score_model, test_data, n_post_samples=valid_prior.shape[0], conditions=conditions)\n",
    "print(\"Sampled posterior parameters:\", posterior_samples)"
   ],
   "id": "36e95b8ccefc31f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_posterior_2d(posterior_samples, prior_draws=valid_prior, param_names=param_names);",
   "id": "d497c90b97f71a68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not train_global:\n",
    "    school_effects_med = np.median(posterior_samples, axis=0)\n",
    "    school_effects_low = np.percentile(posterior_samples, 2.5, axis=0)\n",
    "    school_effects_hi = np.percentile(posterior_samples, 97.5, axis=0)\n",
    "    avg_effect = conditions[0].item()\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(6, 4))\n",
    "    ax.scatter(np.array(range(J)), school_effects_med, color='red', s=60)\n",
    "    ax.scatter(\n",
    "        np.array(range(J)) + 0.1, y.flatten(), color='blue', s=60)\n",
    "\n",
    "    plt.plot([-0.2, 7.4], [np.mean(avg_effect),\n",
    "                           np.mean(avg_effect)], 'k', linestyle='--')\n",
    "\n",
    "    ax.errorbar(\n",
    "        np.array(range(8)),\n",
    "        school_effects_med,\n",
    "        yerr=[\n",
    "            school_effects_med - school_effects_low,\n",
    "            school_effects_hi - school_effects_med\n",
    "        ],\n",
    "        fmt='none')\n",
    "\n",
    "    ax.legend(('ABI', 'Observed effect', 'avg_effect'), fontsize=14)\n",
    "\n",
    "    plt.xlabel('School')\n",
    "    plt.ylabel('Treatment effect')\n",
    "    plt.title('ABI estimated school treatment effects vs. observed data')\n",
    "    plt.show()"
   ],
   "id": "2aae671ce3fd9221",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## HMC inference Results\n",
    "\n",
    "https://github.com/blei-lab/edward/blob/master/notebooks/eight_schools.ipynb"
   ],
   "id": "9a13cf5a9239b208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "#import seaborn as sns"
   ],
   "id": "17edbe3260ac3ea0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fit_hmc(data_y, data_sigma):\n",
    "    model = tfd.JointDistributionSequential([\n",
    "      tfd.Normal(loc=prior.mu_mean, scale=prior.mu_std, name=\"avg_effect\"),  # `mu` above\n",
    "      tfd.Normal(loc=prior.log_tau_mean, scale=prior.log_tau_std, name=\"avg_stddev\"),  # `log(tau)` above\n",
    "      tfd.Independent(tfd.Normal(loc=tf.zeros(J),\n",
    "                                 scale=tf.ones(J),\n",
    "                                 name=\"school_effects_standard\"),  # `theta_prime`\n",
    "                      reinterpreted_batch_ndims=1),\n",
    "      lambda school_effects_standard, avg_stddev, avg_effect: (\n",
    "          tfd.Independent(tfd.Normal(loc=(avg_effect[..., tf.newaxis] +\n",
    "                                          tf.exp(avg_stddev[..., tf.newaxis]) *\n",
    "                                          school_effects_standard),  # `theta` above\n",
    "                                     scale=data_sigma.astype(np.float32)),\n",
    "                          name=\"treatment_effects\",  # `y` above\n",
    "                          reinterpreted_batch_ndims=1))\n",
    "    ])\n",
    "\n",
    "    def target_log_prob_fn(avg_effect, avg_stddev, school_effects_standard):\n",
    "      \"\"\"Unnormalized target density as a function of states.\"\"\"\n",
    "      return model.log_prob((\n",
    "          avg_effect, avg_stddev, school_effects_standard, data_y.flatten().astype(np.float32)))\n",
    "\n",
    "    num_results = 5000\n",
    "    num_burnin_steps = 3000\n",
    "\n",
    "    # Improve performance by tracing the sampler using `tf.function`\n",
    "    # and compiling it using XLA.\n",
    "    @tf.function(autograph=False, jit_compile=True)\n",
    "    def do_sampling():\n",
    "      return tfp.mcmc.sample_chain(\n",
    "          num_results=num_results,\n",
    "          num_burnin_steps=num_burnin_steps,\n",
    "          current_state=[\n",
    "              tf.zeros([], name='init_avg_effect'),\n",
    "              tf.zeros([], name='init_avg_stddev'),\n",
    "              tf.ones([J], name='init_school_effects_standard'),\n",
    "          ],\n",
    "          kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "              target_log_prob_fn=target_log_prob_fn,\n",
    "              step_size=0.4,\n",
    "              num_leapfrog_steps=3))\n",
    "\n",
    "    states, kernel_results = do_sampling()\n",
    "\n",
    "    avg_effect, avg_stddev, school_effects_standard = states\n",
    "\n",
    "    school_effects_samples = (\n",
    "        avg_effect[:, np.newaxis] +\n",
    "        np.exp(avg_stddev)[:, np.newaxis] * school_effects_standard)\n",
    "\n",
    "    num_accepted = np.sum(kernel_results.is_accepted)\n",
    "    print('Acceptance rate: {}'.format(num_accepted / num_results))\n",
    "\n",
    "    return avg_effect, avg_stddev, school_effects_standard, school_effects_samples"
   ],
   "id": "ad9a9702a87aeba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# repeat inference for validation data\n",
    "posterior_samples_valid_hmc = []\n",
    "for v in valid_data:\n",
    "    v_temp = v.detach().numpy()\n",
    "    avg_effect, avg_stddev, _, school_effects_samples = fit_hmc(v_temp[:, 0], v_temp[:, 1])\n",
    "    posterior_samples_valid_hmc.append(np.concatenate((avg_effect[:, np.newaxis],\n",
    "                                                       avg_stddev[:, np.newaxis],\n",
    "                                                       school_effects_samples), axis=1))\n",
    "posterior_samples_valid_hmc = np.array(posterior_samples_valid_hmc)"
   ],
   "id": "4ee50417a0002306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_samples_valid_hmc[:, :, :2], prior_samples=valid_prior_global,\n",
    "                          param_names=[r'$\\mu$', r'$\\log \\tau$']);"
   ],
   "id": "b9784b3fadd6850f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_samples_valid_hmc[:, :, 2:], prior_samples=valid_prior_local,\n",
    "                              param_names=[f'$\\\\theta_{i}$' for i in range(J)]);"
   ],
   "id": "c0931011b4a9a51f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Apply to Data",
   "id": "2ae35f040a81ed21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "avg_effect, avg_stddev, school_effects_standard, school_effects_samples = fit_hmc(y, sigma)",
   "id": "ad8e777bc6c2fb9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_names = [r'$\\mu$', r'$\\log \\tau$']\n",
    "valid_prior, _, _ = generate_synthetic_data(avg_effect.shape[0], schools_joint=True, device=torch_device)\n",
    "diagnostics.plot_posterior_2d(tf.stack((avg_effect, avg_stddev), axis=1), prior_draws=valid_prior,\n",
    "                              param_names=param_names);"
   ],
   "id": "9777822f9c7e3377",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"E[avg_effect] = {}\".format(np.mean(avg_effect)))\n",
    "print(\"E[avg_stddev] = {}\".format(np.mean(avg_stddev)))\n",
    "print(\"E[school_effects_standard] =\")\n",
    "print(np.mean(school_effects_standard[:, ]))\n",
    "print(\"E[school_effects] =\")\n",
    "print(np.mean(school_effects_samples[:, ], axis=0))"
   ],
   "id": "9478f7c73556ed08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the 95% interval for school_effects\n",
    "school_effects_low = np.array([\n",
    "    np.percentile(school_effects_samples[:, i], 2.5) for i in range(J)\n",
    "])\n",
    "school_effects_med = np.array([\n",
    "    np.percentile(school_effects_samples[:, i], 50) for i in range(J)\n",
    "])\n",
    "school_effects_hi = np.array([\n",
    "    np.percentile(school_effects_samples[:, i], 97.5)\n",
    "    for i in range(J)\n",
    "])"
   ],
   "id": "4cbe27ecf465c471",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(6, 4))\n",
    "ax.scatter(np.array(range(J)), school_effects_med, color='red', s=60)\n",
    "ax.scatter(\n",
    "    np.array(range(J)) + 0.1, y.flatten(), color='blue', s=60)\n",
    "\n",
    "plt.plot([-0.2, 7.4], [np.mean(avg_effect),\n",
    "                       np.mean(avg_effect)], 'k', linestyle='--')\n",
    "\n",
    "ax.errorbar(\n",
    "    np.array(range(8)),\n",
    "    school_effects_med,\n",
    "    yerr=[\n",
    "        school_effects_med - school_effects_low,\n",
    "        school_effects_hi - school_effects_med\n",
    "    ],\n",
    "    fmt='none')\n",
    "\n",
    "ax.legend(('HMC', 'Observed effect', 'avg_effect'), fontsize=14)\n",
    "\n",
    "plt.xlabel('School')\n",
    "plt.ylabel('Treatment effect')\n",
    "plt.title('HMC estimated school treatment effects vs. observed data')\n",
    "plt.show()"
   ],
   "id": "49496aa43fb0204d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c4bb5620888ab007",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
