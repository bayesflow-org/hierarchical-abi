WARNING:bayesflow:
When using torch backend, we need to disable autograd by default to avoid excessive memory usage. Use

with torch.enable_grad():
    ...

in contexts where you need gradients (e.g. custom training loops).
/home/jarruda_hpc/.conda/envs/habi/lib/python3.11/site-packages/scipy/stats/_stats_py.py:3188: RuntimeWarning: invalid value encountered in subtract
  mad = np.median(np.abs(x - med))
/home/jarruda_hpc/hierarchical-abi/problems/plotting_helper.py:97: RuntimeWarning: overflow encountered in exp
  global_ci = [global_mean_est-1.96*np.mean(np.exp(global_samples[i, :, 1])),
/home/jarruda_hpc/hierarchical-abi/problems/plotting_helper.py:97: RuntimeWarning: invalid value encountered in scalar subtract
  global_ci = [global_mean_est-1.96*np.mean(np.exp(global_samples[i, :, 1])),
/home/jarruda_hpc/hierarchical-abi/problems/plotting_helper.py:98: RuntimeWarning: overflow encountered in exp
  global_mean_est+1.96*np.mean(np.exp(global_samples[i, :, 1]))]
/home/jarruda_hpc/.conda/envs/habi/lib/python3.11/site-packages/numpy/core/_methods.py:176: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)
/home/jarruda_hpc/.conda/envs/habi/lib/python3.11/site-packages/numpy/core/_methods.py:187: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)
Kernel type: variance_preserving, noise schedule: cosine
t_min: 0.00035210439818911254, t_max: 0.999647855758667
alpha, sigma: (tensor(1.0000), tensor(0.0006)) (tensor(0.0006), tensor(1.0000))
+--------------------------------------+------------+
|               Modules                | Parameters |
+--------------------------------------+------------+
|   blocks.res_blocks.0.dense.weight   |    4352    |
|    blocks.res_blocks.0.dense.bias    |    256     |
| blocks.res_blocks.0.projector.weight |    4352    |
|   blocks.res_blocks.1.dense.weight   |   65536    |
|    blocks.res_blocks.1.dense.bias    |    256     |
|   blocks.res_blocks.2.dense.weight   |   65536    |
|    blocks.res_blocks.2.dense.bias    |    256     |
|   blocks.res_blocks.3.dense.weight   |   65536    |
|    blocks.res_blocks.3.dense.bias    |    256     |
|   blocks.res_blocks.4.dense.weight   |   65536    |
|    blocks.res_blocks.4.dense.bias    |    256     |
|    final_projection_linear.weight    |    1536    |
|     final_projection_linear.bias     |     6      |
+--------------------------------------+------------+
Total Trainable Params: 273670
FLI_1_10_256_5_TimeSeriesNetwork_global_score_model_v_variance_preserving_cosine_likelihood_weighting
+--------------------------------------+------------+
|               Modules                | Parameters |
+--------------------------------------+------------+
|   blocks.res_blocks.0.dense.weight   |    5120    |
|    blocks.res_blocks.0.dense.bias    |    256     |
| blocks.res_blocks.0.projector.weight |    5120    |
|   blocks.res_blocks.1.dense.weight   |   65536    |
|    blocks.res_blocks.1.dense.bias    |    256     |
|   blocks.res_blocks.2.dense.weight   |   65536    |
|    blocks.res_blocks.2.dense.bias    |    256     |
|   blocks.res_blocks.3.dense.weight   |   65536    |
|    blocks.res_blocks.3.dense.bias    |    256     |
|   blocks.res_blocks.4.dense.weight   |   65536    |
|    blocks.res_blocks.4.dense.bias    |    256     |
|    final_projection_linear.weight    |    768     |
|     final_projection_linear.bias     |     3      |
+--------------------------------------+------------+
Total Trainable Params: 274435
FLI_1_10_256_5_TimeSeriesNetwork_local_score_model_v_variance_preserving_cosine_likelihood_weighting
+-----------------------------------------------------------+------------+
|                          Modules                          | Parameters |
+-----------------------------------------------------------+------------+
|              summary_net.conv_blocks.0.weight             |     96     |
|               summary_net.conv_blocks.0.bias              |     32     |
|           summary_net.recurrent.skip_conv.weight          |   16384    |
|            summary_net.recurrent.skip_conv.bias           |    128     |
|        summary_net.recurrent.recurrent.weight_ih_l0       |   12288    |
|        summary_net.recurrent.recurrent.weight_hh_l0       |   49152    |
|         summary_net.recurrent.recurrent.bias_ih_l0        |    384     |
|         summary_net.recurrent.recurrent.bias_hh_l0        |    384     |
|    summary_net.recurrent.recurrent.weight_ih_l0_reverse   |   12288    |
|    summary_net.recurrent.recurrent.weight_hh_l0_reverse   |   49152    |
|     summary_net.recurrent.recurrent.bias_ih_l0_reverse    |    384     |
|     summary_net.recurrent.recurrent.bias_hh_l0_reverse    |    384     |
|     summary_net.recurrent.skip_recurrent.weight_ih_l0     |   49152    |
|     summary_net.recurrent.skip_recurrent.weight_hh_l0     |   49152    |
|      summary_net.recurrent.skip_recurrent.bias_ih_l0      |    384     |
|      summary_net.recurrent.skip_recurrent.bias_hh_l0      |    384     |
| summary_net.recurrent.skip_recurrent.weight_ih_l0_reverse |   49152    |
| summary_net.recurrent.skip_recurrent.weight_hh_l0_reverse |   49152    |
|  summary_net.recurrent.skip_recurrent.bias_ih_l0_reverse  |    384     |
|  summary_net.recurrent.skip_recurrent.bias_hh_l0_reverse  |    384     |
|            summary_net.output_projector.weight            |    5120    |
|             summary_net.output_projector.bias             |     10     |
|       global_model.blocks.res_blocks.0.dense.weight       |    4352    |
|        global_model.blocks.res_blocks.0.dense.bias        |    256     |
|     global_model.blocks.res_blocks.0.projector.weight     |    4352    |
|       global_model.blocks.res_blocks.1.dense.weight       |   65536    |
|        global_model.blocks.res_blocks.1.dense.bias        |    256     |
|       global_model.blocks.res_blocks.2.dense.weight       |   65536    |
|        global_model.blocks.res_blocks.2.dense.bias        |    256     |
|       global_model.blocks.res_blocks.3.dense.weight       |   65536    |
|        global_model.blocks.res_blocks.3.dense.bias        |    256     |
|       global_model.blocks.res_blocks.4.dense.weight       |   65536    |
|        global_model.blocks.res_blocks.4.dense.bias        |    256     |
|        global_model.final_projection_linear.weight        |    1536    |
|         global_model.final_projection_linear.bias         |     6      |
|        local_model.blocks.res_blocks.0.dense.weight       |    5120    |
|         local_model.blocks.res_blocks.0.dense.bias        |    256     |
|      local_model.blocks.res_blocks.0.projector.weight     |    5120    |
|        local_model.blocks.res_blocks.1.dense.weight       |   65536    |
|         local_model.blocks.res_blocks.1.dense.bias        |    256     |
|        local_model.blocks.res_blocks.2.dense.weight       |   65536    |
|         local_model.blocks.res_blocks.2.dense.bias        |    256     |
|        local_model.blocks.res_blocks.3.dense.weight       |   65536    |
|         local_model.blocks.res_blocks.3.dense.bias        |    256     |
|        local_model.blocks.res_blocks.4.dense.weight       |   65536    |
|         local_model.blocks.res_blocks.4.dense.bias        |    256     |
|         local_model.final_projection_linear.weight        |    768     |
|          local_model.final_projection_linear.bias         |     3      |
+-----------------------------------------------------------+------------+
Total Trainable Params: 892435
FLI_1_10_256_5_TimeSeriesNetwork_hierarchical_score_model_v_variance_preserving_cosine_likelihood_weighting
torch.Size([100, 1024, 256, 1]) 1
NaNs in theta at time 0.07200000435113907 with step size: tensor(0.0010, device='cuda:0')
NaNs in theta at time 1.0 with step size: tensor(0.0050, device='cuda:0')
Global Estimates
mu: -1.2481987 0.040888846
log sigma: -0.039818287 0.025391098
True
mu: -0.9243212938308716
log sigma: 0.1204466000199318
