WARNING:bayesflow:
When using torch backend, we need to disable autograd by default to avoid excessive memory usage. Use

with torch.enable_grad():
    ...

in contexts where you need gradients (e.g. custom training loops).
/home/jarruda_hpc/.conda/envs/habi/lib/python3.11/site-packages/bayesflow/diagnostics/metrics/posterior_contraction.py:59: RuntimeWarning: Degrees of freedom <= 0 for slice
  post_vars = samples["estimates"].var(axis=1, ddof=1)
/home/jarruda_hpc/.conda/envs/habi/lib/python3.11/site-packages/numpy/core/_methods.py:195: RuntimeWarning: invalid value encountered in divide
  ret = um.true_divide(
/var/spool/slurmd/job20606935/slurm_script: line 22: 1577106 Killed                  python3.11 /home/jarruda_hpc/hierarchical-abi/gaussian_flat_score_matching.py 100
slurmstepd: error: Detected 1 oom_kill event in StepId=20606935.batch. Some of the step tasks have been OOM Killed.
