WARNING:bayesflow:
When using torch backend, we need to disable autograd by default to avoid excessive memory usage. Use

with torch.enable_grad():
    ...

in contexts where you need gradients (e.g. custom training loops).
/home/jarruda_hpc/hierarchical-abi/problems/fli.py:288: RuntimeWarning: overflow encountered in exp
  tau = np.exp(log_tau)
/home/jarruda_hpc/hierarchical-abi/problems/fli.py:289: RuntimeWarning: overflow encountered in exp
  tau_2 = tau + np.exp(log_delta_tau)
/var/spool/slurmd/job20606877/slurm_script: line 25: 2414580 Killed                  python3.11 /home/jarruda_hpc/hierarchical-abi/fli\ score\ matching.py
slurmstepd: error: Detected 1 oom_kill event in StepId=20606877.batch. Some of the step tasks have been OOM Killed.
