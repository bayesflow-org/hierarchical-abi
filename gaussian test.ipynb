{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gaussian on a Grid Test for Hierarchical ABI with compositional score matching",
   "id": "dcd4b6bd87124bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayesflow import diagnostics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ],
   "id": "1f9cb2cab39cb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch_device = torch.device(\"cpu\")",
   "id": "70c931844e56c698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Simulator:\n",
    "    def __init__(self, n_grid=8):\n",
    "        self.n_grid = n_grid\n",
    "        self.max_time = 100\n",
    "        self.n_time_points = 10  # number of observation points to return\n",
    "        self.dt = 0.1            # simulation time step\n",
    "        self.n_time_steps = int(self.max_time / self.dt)  # number of simulation steps\n",
    "\n",
    "    def __call__(self, params):\n",
    "        \"\"\"\n",
    "        Simulate Brownian motion with drift.\n",
    "\n",
    "        The SDE is:\n",
    "            dx(t) = mu * dt + tau * sqrt(dt) * dW(t)\n",
    "        starting from 0.\n",
    "\n",
    "        The simulation runs for self.n_time_steps steps (with step dt)\n",
    "        and then returns self.n_time_points evenly spaced observations\n",
    "        between 0 and self.max_time.\n",
    "\n",
    "        The parameter dict 'params' must contain:\n",
    "            - 'mu': drift coefficient\n",
    "            - 'log_tau': log of the diffusion coefficient\n",
    "\n",
    "        These parameters can be provided as:\n",
    "            - A scalar (for a single grid element),\n",
    "            - A 2D array of shape (batch_size, 1) or (batch_size, n_grid)\n",
    "              for batch simulations over a one-dimensional grid,\n",
    "            - A 3D array of shape (batch_size, n_grid, n_grid)\n",
    "              for batch simulations over a two-dimensional grid.\n",
    "        \"\"\"\n",
    "        # Convert parameters to numpy arrays.\n",
    "        theta = np.array(params['theta'])\n",
    "\n",
    "        # Determine simulation mode and grid shape.\n",
    "        if theta.ndim in (0,1):\n",
    "            # Scalar: simulate a single grid element.\n",
    "            grid_shape = (1,1)\n",
    "            theta = np.full(grid_shape, theta)\n",
    "        elif theta.ndim == 2:\n",
    "            # 2D array: shape (batch_size, d) where d==1.\n",
    "            if theta.shape[1] == 1:\n",
    "                grid_shape = (1,1)\n",
    "            else:\n",
    "                raise ValueError(\"For 2D 'theta', the second dimension must be 1.\")\n",
    "        elif theta.ndim == 3:\n",
    "            # 3D array: shape (batch_size, n_grid, n_grid)\n",
    "            if theta.shape[1] != self.n_grid or theta.shape[2] != self.n_grid:\n",
    "                raise ValueError(\"For 3D 'theta', the second and third dimensions must equal n_grid.\")\n",
    "            grid_shape = (self.n_grid, self.n_grid)\n",
    "        else:\n",
    "            raise ValueError(\"Parameter 'theta' must be provided as a scalar, 2D array, or 3D array.\")\n",
    "        batch_size = theta.shape[0]\n",
    "\n",
    "        # Simulate the full trajectory.\n",
    "        # The noise will have shape: (batch_size, n_time_steps, *grid_shape)\n",
    "        noise_shape = (batch_size, self.n_time_steps) + grid_shape\n",
    "        noise = np.random.normal(loc=0, scale=1, size=noise_shape)\n",
    "\n",
    "        # Expand mu and tau to include a time axis.\n",
    "        if theta.ndim in (1, 2):\n",
    "            # mu and tau have shape (batch_size, grid) in the 2D case\n",
    "            # For a scalar, we already set them to shape (1,)\n",
    "            # Expand to (batch_size, 1, grid)\n",
    "            if batch_size == 1:\n",
    "                # Ensure shape is (1, 1, grid)\n",
    "                theta_expanded = theta[np.newaxis, np.newaxis, :]\n",
    "            else:\n",
    "                theta_expanded = theta[:, np.newaxis, np.newaxis, :]\n",
    "        else:\n",
    "            # For 3D parameters, mu and tau have shape (batch_size, n_grid, n_grid)\n",
    "            # Expand to (batch_size, 1, n_grid, n_grid)\n",
    "            theta_expanded = theta[:, np.newaxis, :, :]\n",
    "\n",
    "        # Compute increments:\n",
    "        #   increment = mu * dt + tau * sqrt(dt) * noise\n",
    "        increments = theta_expanded * self.dt + 1 * np.sqrt(self.dt) * noise\n",
    "\n",
    "        # Initial condition: zeros with shape (batch_size, 1, *grid_shape)\n",
    "        x0 = np.zeros((batch_size, 1) + grid_shape)\n",
    "        # Full trajectory: shape (batch_size, n_time_steps+1, *grid_shape)\n",
    "        traj_full = np.concatenate([x0, np.cumsum(increments, axis=1)], axis=1)\n",
    "\n",
    "        # Sample self.n_time_points evenly spaced indices from the full trajectory.\n",
    "        # These indices span from 0 to self.n_time_steps.\n",
    "        indices = np.linspace(self.n_time_points, self.max_time, self.n_time_points, dtype=int)\n",
    "        traj_sampled = traj_full[:, indices, ...]  # shape: (batch_size, n_time_points, *grid_shape)\n",
    "\n",
    "        if theta.ndim == 2:  # just one grid element\n",
    "            traj_sampled = traj_sampled.reshape(batch_size, self.n_time_points, 1)\n",
    "        return dict(observable=traj_sampled)\n",
    "\n",
    "class Prior:\n",
    "    def __init__(self):\n",
    "        self.mu_mean = 0\n",
    "        self.mu_std = 3\n",
    "        self.log_tau_mean = 0\n",
    "        self.log_tau_std = 1\n",
    "\n",
    "        np.random.seed(0)\n",
    "        test_prior = self.sample_single(1000)\n",
    "        self.simulator = Simulator()\n",
    "        test = self.simulator(test_prior,)\n",
    "        self.x_mean = torch.tensor([np.mean(test['observable'])], dtype=torch.float32, device=torch_device)\n",
    "        self.x_std = torch.tensor([np.std(test['observable'])], dtype=torch.float32, device=torch_device)\n",
    "        self.prior_global_mean = torch.tensor(np.array([np.mean(test_prior['mu']), np.mean(test_prior['log_tau'])]),\n",
    "                                              dtype=torch.float32, device=torch_device)\n",
    "        self.prior_global_std = torch.tensor(np.array([np.std(test_prior['mu']), np.std(test_prior['log_tau'])]),\n",
    "                                             dtype=torch.float32, device=torch_device)\n",
    "        self.prior_local_mean = torch.tensor(np.array([np.mean(test_prior['theta'])]),\n",
    "                                             dtype=torch.float32, device=torch_device)\n",
    "        self.prior_local_std = torch.tensor(np.array([np.std(test_prior['theta'])]),\n",
    "                                            dtype=torch.float32, device=torch_device)\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        return self.sample_single(batch_size)\n",
    "\n",
    "    def sample_single(self, batch_size):\n",
    "        mu = np.random.normal(loc=self.mu_mean, scale=self.mu_std, size=(batch_size,1))\n",
    "        log_tau = np.random.normal(loc=self.log_tau_mean, scale=self.log_tau_std, size=(batch_size,1))\n",
    "        theta = np.random.normal(loc=mu, scale=np.exp(log_tau), size=(batch_size, 1))\n",
    "        return dict(mu=mu, log_tau=log_tau, theta=theta)\n",
    "\n",
    "    def sample_full(self, batch_size):\n",
    "        mu = np.random.normal(loc=self.mu_mean, scale=self.mu_std, size=(batch_size, 1))\n",
    "        log_tau = np.random.normal(loc=self.log_tau_mean, scale=self.log_tau_std, size=(batch_size, 1))\n",
    "        theta = np.random.normal(loc=mu[:, np.newaxis], scale=np.exp(log_tau)[:, np.newaxis],\n",
    "                                 size=(batch_size, self.simulator.n_grid, self.simulator.n_grid))\n",
    "        return dict(mu=mu, log_tau=log_tau, theta=theta)\n",
    "\n",
    "    def score_global_batch(self, theta_batch_norm, condition_norm=None):\n",
    "        \"\"\" Computes the global score for a batch of parameters.\"\"\"\n",
    "        theta_batch = theta_batch_norm * self.prior_global_std + self.prior_global_mean\n",
    "        mu, log_tau = theta_batch[..., 0], theta_batch[..., 1]\n",
    "        grad_logp_mu = -(mu - self.mu_mean) / (self.mu_std**2)\n",
    "        grad_logp_tau = -(log_tau - self.log_tau_mean) / (self.log_tau_std**2)\n",
    "        # correct the score for the normalization\n",
    "        score = torch.stack([grad_logp_mu, grad_logp_tau], dim=-1)\n",
    "        return score / self.prior_global_std\n",
    "\n",
    "    def score_local_batch(self, theta_batch_norm, condition_norm):\n",
    "        \"\"\" Computes the local score for a batch of samples. \"\"\"\n",
    "        theta = theta_batch_norm * self.prior_local_std + self.prior_local_mean\n",
    "        condition = condition_norm * self.prior_global_std + self.prior_global_mean\n",
    "        mu, log_tau = condition[..., 0], condition[..., 1]\n",
    "        # Gradient w.r.t theta conditioned on mu and log_tau\n",
    "        grad_logp_theta = -(theta - mu) / torch.exp(log_tau*2)\n",
    "        # correct the score for the normalization\n",
    "        score = grad_logp_theta / self.prior_local_std\n",
    "        return score\n",
    "\n",
    "prior = Prior()\n",
    "n_params_global = 2\n",
    "n_params_local = 1"
   ],
   "id": "e1ac50b53a7ed912",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prior(2)",
   "id": "557005f1016c5ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_synthetic_data(n_samples, n_grid=8, full_grid=False, device=None):\n",
    "    if full_grid:\n",
    "        batch_params = prior.sample_full(n_samples)\n",
    "    else:\n",
    "        batch_params = prior.sample_single(n_samples)\n",
    "    simulator = Simulator(n_grid=n_grid)\n",
    "    sim_batch = simulator(batch_params)\n",
    "\n",
    "    param_global = torch.tensor(np.concatenate([batch_params['mu'], batch_params['log_tau']], axis=1),\n",
    "                                dtype=torch.float32, device=device)\n",
    "    param_local = torch.tensor(batch_params['theta'], dtype=torch.float32, device=device)\n",
    "    data = torch.tensor(sim_batch['observable'], dtype=torch.float32, device=device)\n",
    "    return param_global, param_local, data"
   ],
   "id": "f3fbe1e0f18546ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_simulation_output(sim_output, title_prefix=\"Time\", cmap=\"viridis\"):\n",
    "    \"\"\"\n",
    "    Visualize the full simulation trajectory on a grid of subplots.\n",
    "\n",
    "    Parameters:\n",
    "        sim_output (np.ndarray): Simulation trajectory output.\n",
    "            For a single simulation, it can be either:\n",
    "              - 2D: shape (n_time_points, grid_size) for a 1D grid, or\n",
    "              - 3D: shape (n_time_points, n_grid, n_grid) for a 2D grid.\n",
    "            For batched simulations, the shape is:\n",
    "              - 3D: (batch_size, n_time_points, grid_size) or\n",
    "              - 4D: (batch_size, n_time_points, n_grid, n_grid).\n",
    "            In such cases, only the first simulation (i.e. first batch element) is visualized.\n",
    "        title_prefix (str, list): Prefix for subplot titles.\n",
    "        cmap (str): Colormap for imshow when visualizing 2D grid outputs.\n",
    "    \"\"\"\n",
    "    # If a batch dimension is present, select the first simulation.\n",
    "    if sim_output.ndim == 4:\n",
    "        # (batch_size, n_time_points, n_grid, n_grid)\n",
    "        sim_output = sim_output[0]\n",
    "\n",
    "    # Determine number of time points.\n",
    "    n_time_points = sim_output.shape[0]\n",
    "\n",
    "    # Automatically choose grid layout (approximate square).\n",
    "    n_cols = n_time_points\n",
    "    n_rows = 1\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "    # Flatten axes array in case it's 2D.\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for i in range(n_time_points):\n",
    "        ax = axes[i]\n",
    "        # Check if the grid is 1D or 2D.\n",
    "        # 2D grid: shape (n_time_points, n_grid, n_grid)\n",
    "        im = ax.imshow(sim_output[i], cmap=cmap, vmin=sim_output.min(), vmax=sim_output.max())\n",
    "        if isinstance(title_prefix, list):\n",
    "            ax.set_title(title_prefix[i])\n",
    "        else:\n",
    "            ax.set_title(f\"{title_prefix} {i}\")\n",
    "        fig.colorbar(im, ax=ax)\n",
    "\n",
    "    # Hide any unused subplots.\n",
    "    for j in range(n_time_points, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "test = prior.sample_full(1)\n",
    "simulator_test = Simulator()\n",
    "sim_test = simulator_test(test)['observable']\n",
    "visualize_simulation_output(sim_test)"
   ],
   "id": "626ee61da45e5d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def positional_encoding(t, d_model, max_t=1000.0):\n",
    "    \"\"\"\n",
    "    Computes the sinusoidal positional encoding for a given time t.\n",
    "\n",
    "    Args:\n",
    "        t (torch.Tensor): The input time tensor of shape (batch_size, 1).\n",
    "        d_model (int): The dimensionality of the embedding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The positional encoding of shape (batch_size, d_model).\n",
    "    \"\"\"\n",
    "    half_dim = d_model // 2\n",
    "    div_term = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=t.device) *\n",
    "                         -(math.log(max_t) / (half_dim - 1)))\n",
    "    t_proj = t * div_term\n",
    "    pos_enc = torch.cat([torch.sin(t_proj), torch.cos(t_proj)], dim=-1)\n",
    "    return pos_enc\n",
    "\n",
    "class ConditionalResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, cond_dim, dropout=0.1):\n",
    "        super(ConditionalResidualBlock, self).__init__()\n",
    "        # First linear layer that takes [hidden state; conditioning]\n",
    "        self.fc1 = nn.Linear(hidden_dim + cond_dim, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        # Second linear layer also takes [hidden state; conditioning]\n",
    "        self.fc2 = nn.Linear(hidden_dim + cond_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.activation = nn.SiLU()  # same as swish\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Apply spectral normalization\n",
    "        self.fc1 = nn.utils.parametrizations.spectral_norm(self.fc1)\n",
    "        self.fc2 = nn.utils.parametrizations.spectral_norm(self.fc2)\n",
    "\n",
    "    @staticmethod\n",
    "    def swish(x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "    def forward(self, h, cond):\n",
    "        # Concatenate the hidden state with the conditioning vector\n",
    "        x = torch.cat([h, cond], dim=-1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        # Inject conditioning again before the second transformation\n",
    "        out = self.fc2(torch.cat([out, cond], dim=-1))\n",
    "        out = self.norm2(out)\n",
    "        # Add the original hidden state (skip connection) and apply activation\n",
    "        return self.activation(out + h)\n",
    "\n",
    "class ScoreModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Neural network model that computes score estimates.\n",
    "\n",
    "        Args:\n",
    "            input_dim_theta (int): Input dimension for theta.\n",
    "            input_dim_x (int): Input dimension for x.\n",
    "            input_dim_condition (int): Input dimension for the condition. Can be 0 for global score.\n",
    "            hidden_dim (int): Hidden dimension for theta network.\n",
    "            time_embed_dim (int, optional): Dimension of time embedding. Defaults to 4.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim_theta, input_dim_x, input_dim_condition,\n",
    "                 hidden_dim,\n",
    "                 time_embed_dim=16):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "\n",
    "        # Define the dimension of the conditioning vector\n",
    "        cond_dim = input_dim_x + input_dim_condition + time_embed_dim\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        # Project the concatenation of theta and the condition into hidden_dim\n",
    "        self.input_layer = nn.Linear(input_dim_theta + cond_dim, hidden_dim)\n",
    "\n",
    "        # Create a sequence of conditional residual blocks\n",
    "        self.block1 = ConditionalResidualBlock(hidden_dim, cond_dim)\n",
    "        self.block2 = ConditionalResidualBlock(hidden_dim, cond_dim)\n",
    "        self.block3 = ConditionalResidualBlock(hidden_dim, cond_dim)\n",
    "\n",
    "        # Create a sequence of residual blocks\n",
    "        #self.block1 = ResidualBlock(input_dim_theta + cond_dim, hidden_dim)\n",
    "        #self.block2 = ResidualBlock(hidden_dim, hidden_dim)\n",
    "        #self.block3 = ResidualBlock(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Final layer to get back to the theta dimension\n",
    "        self.final_linear = nn.Linear(hidden_dim, input_dim_theta)\n",
    "\n",
    "        # Apply spectral normalization\n",
    "        self.final_linear = nn.utils.parametrizations.spectral_norm(self.final_linear)\n",
    "\n",
    "    def forward(self, theta, t, x, conditions=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the ScoreModel.\n",
    "\n",
    "        Args:\n",
    "            theta (torch.Tensor): Input theta tensor of shape (batch_size, input_dim_theta).\n",
    "            t (torch.Tensor): Input time tensor of shape (batch_size, 1).\n",
    "            x (torch.Tensor): Input x tensor of shape (batch_size, input_dim_x).\n",
    "            conditions (torch.Tensor, optional): Input condition tensor of shape (batch_size, input_dim_condition).\n",
    "                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the score model.\n",
    "        \"\"\"\n",
    "        # Compute a time embedding (shape: [batch, time_embed_dim])\n",
    "        t_emb = positional_encoding(t, self.time_embed_dim)\n",
    "\n",
    "        # Form the conditioning vector. If conditions is None, only x and time are used.\n",
    "        if conditions is not None:\n",
    "            cond = torch.cat([x, conditions, t_emb], dim=-1)\n",
    "        else:\n",
    "            cond = torch.cat([x, t_emb], dim=-1)\n",
    "\n",
    "        # Concatenate theta with the conditioning vector as the initial input\n",
    "        h = torch.cat([theta, cond], dim=-1)\n",
    "        h = self.input_layer(h)\n",
    "\n",
    "        # Pass through each residual block, injecting the same cond at each layer\n",
    "        h = self.block1(h, cond)\n",
    "        h = self.block2(h, cond)\n",
    "        h = self.block3(h, cond)\n",
    "\n",
    "        theta_emb = self.final_linear(h)\n",
    "        return theta_emb\n",
    "\n",
    "\n",
    "class HierarchicalScoreModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim_theta_local, input_dim_theta_global, input_dim_x,\n",
    "                 hidden_dim,\n",
    "                 time_embed_dim=16):\n",
    "        super(HierarchicalScoreModel, self).__init__()\n",
    "        self.summary_net = nn.GRU(\n",
    "            input_size=input_dim_x,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.n_params_global = input_dim_theta_global\n",
    "        self.global_model = ScoreModel(\n",
    "            input_dim_theta=input_dim_theta_global,\n",
    "            input_dim_x=hidden_dim,\n",
    "            input_dim_condition=0,\n",
    "            hidden_dim=hidden_dim,\n",
    "            time_embed_dim=time_embed_dim\n",
    "        )\n",
    "        self.n_params_local = input_dim_theta_local\n",
    "        self.local_model = ScoreModel(\n",
    "            input_dim_theta=input_dim_theta_local,\n",
    "            input_dim_x=hidden_dim,\n",
    "            input_dim_condition=input_dim_theta_global,\n",
    "            hidden_dim=hidden_dim,\n",
    "            time_embed_dim=time_embed_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, theta, t, x):\n",
    "        theta_global, theta_local = torch.split(theta, [self.n_params_global, self.n_params_local], dim=-1)\n",
    "        _, x_emb = self.summary_net(x)\n",
    "        x_emb = x_emb[0]  # only one layer, not bidirectional\n",
    "        global_out = self.global_model.forward(theta=theta_global, t=t, x=x_emb, conditions=None)\n",
    "        local_out = self.local_model.forward(theta=theta_local, t=t, x=x_emb, conditions=theta_global)\n",
    "        return torch.cat([global_out, local_out], dim=-1)\n",
    "\n",
    "    def forward_local(self, theta_local, theta_global, t, x):\n",
    "        _, x_emb = self.summary_net(x)\n",
    "        x_emb = x_emb[0]  # only one layer, not bidirectional\n",
    "        local_out = self.local_model.forward(theta=theta_local, t=t, x=x_emb, conditions=theta_global)\n",
    "        return local_out\n",
    "\n",
    "    def forward_global(self, theta_global, t, x):\n",
    "        _, x_emb = self.summary_net(x)\n",
    "        x_emb = x_emb[0]  # only one layer, not bidirectional\n",
    "        global_out = self.global_model.forward(theta=theta_global, t=t, x=x_emb, conditions=None)\n",
    "        return global_out"
   ],
   "id": "6d13bcb1279b6f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cosine_schedule_diffusion_time(t, max_t, s=0):\n",
    "    return torch.cos(((t/max_t + s) / (1 + s)) * (np.pi / 2)) ** 2\n",
    "\n",
    "\n",
    "def generate_diffusion_time_old(max_t, steps, random=False, device=None):\n",
    "    if random:\n",
    "        time = torch.rand(steps+1, dtype=torch.float32, device=device) * max_t\n",
    "        time = torch.sort(time)[0]\n",
    "    else:\n",
    "        time = torch.linspace(0, max_t, steps+1, dtype=torch.float32, device=device)\n",
    "\n",
    "    # gamma called alpha_t in paper\n",
    "    f_0 = 1#cosine_schedule_diffusion_time(torch.tensor(0, dtype=torch.float32, device=device), max_t)\n",
    "    gamma = cosine_schedule_diffusion_time(time, max_t)\n",
    "    beta_t = 1 - torch.cat((gamma[0:1], gamma[1:] / gamma[:-1]), dim=0)\n",
    "    # clip to avoid numerical instability\n",
    "    beta_t = torch.clamp(beta_t, max=0.999)\n",
    "    return time, gamma, beta_t"
   ],
   "id": "3b8f4255beedcc57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_old, gamma, beta_t = generate_diffusion_time_old(10, 400)\n",
    "\n",
    "plt.plot(time_old, torch.log(gamma/(1 - gamma)), label='snr')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time_old, beta_t, label='weighting')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time_old, torch.sqrt(1 - gamma), label='noise')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time_old, beta_t/(1-gamma), label='actual weights')\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "ac9289cce0d91160",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time_old",
   "id": "4a5a3ea38d3b536e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "S_SHIFT_COSINE = 0.#05\n",
    "LAMBDA_0 = 10\n",
    "LAMBDA_1 = -LAMBDA_0\n",
    "\n",
    "def cosine_schedule_signal_to_noise_inv(snr):\n",
    "    return (2/torch.pi)*torch.arctan(torch.exp(-snr/2 - S_SHIFT_COSINE))\n",
    "\n",
    "T_0 = cosine_schedule_signal_to_noise_inv(torch.tensor(LAMBDA_0, dtype=torch.float32, device=torch_device))\n",
    "T_1 = cosine_schedule_signal_to_noise_inv(torch.tensor(LAMBDA_1, dtype=torch.float32, device=torch_device))\n",
    "\n",
    "def cosine_schedule_signal_to_noise(t, max_t):\n",
    "    \"\"\"\n",
    "    Cosine schedule for the log signal-to-noise ratio.\n",
    "    t is assumed to be in the interval [0, max_t].\n",
    "    \"\"\"\n",
    "    # Map t in [0, max_t] to the interval [T_0, T_1]\n",
    "    t_truncated = T_0 + (T_1 - T_0) * (t / max_t)\n",
    "    return -2 * torch.log(torch.tan(torch.pi * t_truncated / 2)) + 2 * S_SHIFT_COSINE\n",
    "\n",
    "def sech(x):\n",
    "    \"\"\"Compute the hyperbolic secant of x.\"\"\"\n",
    "    return 1 / torch.cosh(x)\n",
    "\n",
    "def compute_continuous_weights(t, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Compute continuous weight schedule for diffusion models.\n",
    "\n",
    "    Args:\n",
    "        t: Time values in range [0, 1]\n",
    "        epsilon: Small value to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "        weights: Continuous weight values matching the discrete implementation\n",
    "    \"\"\"\n",
    "    # Compute signal-to-noise ratio\n",
    "    snr = -2 * torch.log(torch.tan(torch.pi * t / 2))\n",
    "\n",
    "    # Compute gamma (sqrt of sigmoid of SNR)\n",
    "    gamma = torch.sqrt(torch.sigmoid(snr))\n",
    "\n",
    "    # Compute time shift for ratio (equivalent to looking at t+dt)\n",
    "    dt = epsilon\n",
    "    t_next = torch.clamp(t + dt, 0, 1)\n",
    "\n",
    "    # Compute gamma for shifted time\n",
    "    snr_next = -2 * torch.log(torch.tan(torch.pi * t_next / 2))\n",
    "    gamma_next = torch.sqrt(torch.sigmoid(snr_next))\n",
    "\n",
    "    # Compute ratio (equivalent to gamma[1:] / gamma[:-1])\n",
    "    ratio = gamma_next / (gamma + epsilon)\n",
    "\n",
    "    # Special handling for t=0 (equivalent to gamma[0:1])\n",
    "    ratio = torch.where(t < epsilon, gamma, ratio)\n",
    "\n",
    "    # Compute weights as 1 - ratio\n",
    "    weights = 1 - ratio\n",
    "\n",
    "    return weights\n",
    "\n",
    "def cosine_schedule_signal_to_noise_density(snr):\n",
    "    \"\"\"Density of the log signal-to-noise ratio.\"\"\"\n",
    "    p = sech(snr / 2 - S_SHIFT_COSINE) / (2 * torch.pi * (T_1 - T_0))\n",
    "    # Truncate the density outside the [LAMBDA_1, LAMBDA_0] interval\n",
    "    p[snr > LAMBDA_0] = 0\n",
    "    p[snr < LAMBDA_1] = 0\n",
    "    return p\n",
    "\n",
    "def weighting_function(snr, device=None):\n",
    "    #return torch.ones_like(snr, dtype=torch.float32, device=device)\n",
    "    #return torch.tanh(snr)\n",
    "    return torch.sigmoid(-snr + 2)\n",
    "\n",
    "def generate_diffusion_time(max_t, size, return_batch=False, device=None):\n",
    "    \"\"\"\n",
    "    Generates diffusion time values along with their corresponding\n",
    "    log signal-to-noise ratio, weighting function, and density.\n",
    "\n",
    "    The time is generated uniformly in [0, max_t] and then mapped to [T_0, T_1].\n",
    "    \"\"\"\n",
    "    if not return_batch:\n",
    "        time = torch.linspace(0, max_t, steps=size, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        # t_i = \\mod (u_0 + i/k, 1)\n",
    "        u0 = torch.rand(1, dtype=torch.float32, device=device)\n",
    "        i = torch.arange(0, size, dtype=torch.float32, device=device)  # i as a tensor of indices\n",
    "        time = ((u0 + i / size) % 1) * (T_1*max_t - T_0) + T_0\n",
    "\n",
    "    snr = cosine_schedule_signal_to_noise(time, max_t)\n",
    "    #sigma2_noise = torch.sigmoid(-snr)\n",
    "    #weight_snr = cosine_schedule_signal_to_noise(T_1*max_t-time, max_t)\n",
    "    #weight_snr = torch.ones_like(snr, dtype=torch.float32, device=device) #weighting_function(snr, device=device)\n",
    "    snr_density = cosine_schedule_signal_to_noise_density(snr) #/ cosine_schedule_signal_to_noise_density(torch.tensor(LAMBDA_0))#* 800\n",
    "    #snr_density = torch.sigmoid(-snr)\n",
    "    weight_snr = weighting_function(snr, device=device)\n",
    "    #weight_snr = weight_snr / sigma2_noise\n",
    "\n",
    "    #gamma = torch.sqrt(torch.sigmoid(snr))\n",
    "    #weight_snr = 1 - torch.cat((gamma[0:1], gamma[1:] / gamma[:-1]), dim=0)  # beta\n",
    "    #weight_snr = compute_continuous_weights(time/ (max_t*T_1) + T_0)\n",
    "\n",
    "    #alpha_sampling_noise = torch.sqrt(torch.sigmoid(snr))\n",
    "    #alpha_t = torch.cat((alpha_sampling_noise[0:1], alpha_sampling_noise[1:] / alpha_sampling_noise[:-1]), dim=0)\n",
    "    #weight_snr = 1 - alpha_t\n",
    "\n",
    "    if return_batch:\n",
    "        # Add a new dimension so that each tensor has shape (size, 1)\n",
    "        return time.unsqueeze(1), snr.unsqueeze(1), weight_snr.unsqueeze(1), snr_density.unsqueeze(1)\n",
    "    else:\n",
    "        return time, snr, weight_snr, snr_density"
   ],
   "id": "b506683f73d8dd26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "T_0, T_1",
   "id": "33af6a6c8c317d25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "snr = torch.tensor(np.linspace(LAMBDA_1, LAMBDA_0, 100))\n",
    "snr_density = cosine_schedule_signal_to_noise_density(snr)\n",
    "\n",
    "plt.plot(snr, snr_density, label='Signal-To-Noise Ratio')\n",
    "#plt.plot(time / 400, weight/p_noise, label='weight')\n",
    "#plt.plot(time / 400, -1 / torch.sqrt(1 - gamma) * delta_t, label='sampling step')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "8627b95492ee5adc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time, snr, weight, snr_density = generate_diffusion_time(10, 400)\n",
    "\n",
    "plt.plot(time, snr, label='log snr')\n",
    "plt.plot(time_old[:-1], torch.log(gamma/(1 - gamma))[:-1], label='log snr old', alpha=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time, weight, label='weighting function')\n",
    "plt.plot(time_old, beta_t, label='weighting function old', alpha=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time, snr_density, label='snr density')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time, torch.sqrt(torch.sigmoid(snr)), label='mean noise')\n",
    "plt.plot(time_old, torch.sqrt(gamma), label='mean noise old', alpha=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time, torch.sqrt(torch.sigmoid(-snr)), label='noise')\n",
    "plt.plot(time_old, torch.sqrt(1 - gamma), label='noise old', alpha=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time, -1/torch.sqrt(torch.sigmoid(-snr)) , label='scaling')\n",
    "plt.plot(time_old, -1 / torch.sqrt(1 - gamma), label='scaling old', alpha=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(time, weight/snr_density, label='actual weights')\n",
    "plt.plot(time_old, beta_t/(1-gamma), label='actual weights old', alpha=0.5)\n",
    "plt.xlabel('Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "7bb7a3e5508f208a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Loss function for weighted MSE\n",
    "def weighted_mse_loss(inputs, targets, weights):\n",
    "    return torch.mean(weights * (inputs - targets) ** 2)\n",
    "\n",
    "\n",
    "def compute_score_loss(x_batch, theta_prime_batch, model, diffusion_time, alpha_noise, sigma_noise, weights_noise, device=None):\n",
    "    # sample from the Gaussian kernel, just learn the noise\n",
    "    epsilon = torch.randn_like(theta_prime_batch, dtype=torch.float32, device=device)\n",
    "    theta_batch = alpha_noise * theta_prime_batch + sigma_noise * epsilon\n",
    "    # calculate the score for the sampled theta\n",
    "    score_pred = model(theta=theta_batch, t=diffusion_time, x=x_batch)\n",
    "    # calculate the loss\n",
    "    loss = 0.5 * weighted_mse_loss(score_pred, epsilon, weights=weights_noise)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop for Score Model\n",
    "def train_score_model(model, dataloader, dataloader_valid=None,\n",
    "                      T=400, epochs=100, lr=1e-3, device=None):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Add Cosine Annealing Scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    loss_history = np.zeros((epochs, 2))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        # for each sample in the batch, calculate the loss for a random diffusion time\n",
    "        for theta_global_prime_batch, theta_local_prime_batch, x_batch in dataloader:\n",
    "            # Generate diffusion time and step size\n",
    "            diffusion_time, snr, weights, snr_density = generate_diffusion_time(max_t=T, size=x_batch.shape[0],\n",
    "                                                                             return_batch=True, device=device)\n",
    "            alpha_noise = torch.sqrt(torch.sigmoid(snr))\n",
    "            sigma_noise = torch.sqrt(torch.sigmoid(-snr))\n",
    "            weights_noise = weights / snr_density\n",
    "\n",
    "            # initialize the gradients\n",
    "            optimizer.zero_grad()\n",
    "            theta_prime_batch = torch.concat([theta_global_prime_batch, theta_local_prime_batch], dim=-1)\n",
    "            # calculate the loss\n",
    "            loss = compute_score_loss(x_batch=x_batch, theta_prime_batch=theta_prime_batch,\n",
    "                                      model=model, diffusion_time=diffusion_time,\n",
    "                                      alpha_noise=alpha_noise, sigma_noise=sigma_noise, weights_noise=weights_noise,\n",
    "                                      device=device)\n",
    "            loss.backward()\n",
    "            # gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # validate the model\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        if dataloader_valid is not None:\n",
    "            for theta_global_prime_batch, theta_local_prime_batch, x_batch in dataloader_valid:\n",
    "                # Generate diffusion time and step size\n",
    "                diffusion_time, snr, weights, snr_density = generate_diffusion_time(max_t=T, size=x_batch.shape[0],\n",
    "                                                                                    return_batch=True, device=device)\n",
    "                alpha_noise = torch.sqrt(torch.sigmoid(snr))\n",
    "                sigma_noise = torch.sqrt(torch.sigmoid(-snr))\n",
    "                weights_noise = weights / snr_density\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    theta_prime_batch = torch.concat([theta_global_prime_batch, theta_local_prime_batch], dim=-1)\n",
    "                    loss = compute_score_loss(x_batch, theta_prime_batch=theta_prime_batch,\n",
    "                                              model=model, diffusion_time=diffusion_time,\n",
    "                                              alpha_noise=alpha_noise, sigma_noise=sigma_noise, weights_noise=weights_noise,\n",
    "                                              device=device)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "        loss_history[epoch] = [np.median(total_loss), np.median(valid_loss)]\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {np.median(total_loss):.4f}, \"\n",
    "              f\"Valid Loss: {np.median(valid_loss):.4f}\", end='\\r')\n",
    "    return loss_history"
   ],
   "id": "3e4488af88ad4738",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "n_samples = 10000\n",
    "batch_size = 128\n",
    "T = 400\n",
    "\n",
    "score_model = HierarchicalScoreModel(\n",
    "    input_dim_theta_global=n_params_global,\n",
    "    input_dim_theta_local=n_params_local,\n",
    "    input_dim_x=1,\n",
    "    hidden_dim=64\n",
    ")\n",
    "score_model.to(torch_device)\n",
    "\n",
    "# Create model and dataset\n",
    "thetas_global, thetas_local, xs = generate_synthetic_data(n_samples, device=torch_device)\n",
    "# Normalize data\n",
    "thetas_global = (thetas_global - prior.prior_global_mean) / prior.prior_global_std\n",
    "thetas_local = (thetas_local - prior.prior_local_mean) / prior.prior_local_std\n",
    "xs = (xs - prior.x_mean) / prior.x_std\n",
    "\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(thetas_global, thetas_local, xs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# create validation data\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(1000, device=torch_device)\n",
    "valid_data = (valid_data - prior.x_mean) / prior.x_std\n",
    "valid_prior_global = (valid_prior_global - prior.prior_global_mean) / prior.prior_global_std\n",
    "valid_prior_local = (valid_prior_local - prior.prior_local_mean) / prior.prior_local_std\n",
    "dataset_valid = TensorDataset(valid_prior_global, valid_prior_local, valid_data)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size)"
   ],
   "id": "32dfab946faf1c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "loss_history = train_score_model(score_model, dataloader, dataloader_valid=dataloader_valid,\n",
    "                                 T=T, epochs=100, lr=1e-3, device=torch_device)"
   ],
   "id": "ab5eef7cdb5ec27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "score_model.global_model.final_linear.weight",
   "id": "7b8a27d43c3debd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "torch.save(score_model.state_dict(), \"score_model.pt\")",
   "id": "70dd78d9bf8db4e3"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "score_model.load_state_dict(torch.load(\"score_model.pt\", weights_only=True))\n",
    "score_model.eval();"
   ],
   "id": "bec5a801baa5bda3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot loss history\n",
    "plt.figure(figsize=(6, 3), tight_layout=True)\n",
    "plt.plot(loss_history[:, 0], label='Train')\n",
    "plt.plot(loss_history[:, 1], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "aca578e1fb265da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diffusion_time, noise, weights, p_noise = generate_diffusion_time(max_t=T, size=100)\n",
    "alpha_noise = torch.sqrt(torch.sigmoid(noise))  # sometimes called \\sqrt(1-\\beta_t)\n",
    "sigma_noise = torch.sqrt(torch.sigmoid(-noise))  # sometimes \\sqrt(\\beta_t)\n",
    "plt.plot(diffusion_time/T, alpha_noise, label='alpha')\n",
    "#plt.plot(1-alpha_noise**2, label='alpha')\n",
    "plt.plot(diffusion_time/T, sigma_noise, label='sigma')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "1c233e8d031ce5e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DDPM Sampling from Ho. et al. (2020)\n",
    "def ddpm_sampling(model, x_obs, n_post_samples, conditions=None, diffusion_steps=100, device=None):\n",
    "    x_obs_norm = (x_obs - prior.x_mean) / prior.x_std  # assumes x_obs is not standardized\n",
    "    x_obs_norm = x_obs_norm.reshape(x_obs_norm.shape[0], -1)\n",
    "    n_obs = x_obs_norm.shape[-1]\n",
    "    n_time_steps = x_obs_norm.shape[0]\n",
    "    x_obs_norm = x_obs_norm.T[:, :, np.newaxis]\n",
    "\n",
    "    # Ensure x_obs_norm is a PyTorch tensor\n",
    "    if not isinstance(x_obs_norm, torch.Tensor):\n",
    "        x_obs_norm = torch.tensor(x_obs_norm, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Initialize parameters\n",
    "    if conditions is None:  # global\n",
    "        n_params = n_params_global\n",
    "        theta = torch.randn(n_post_samples, n_params_global, dtype=torch.float32, device=device) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32, device=device))\n",
    "        conditions_exp = None\n",
    "    else:\n",
    "        # Ensure conditions is a PyTorch tensor\n",
    "        if not isinstance(conditions, torch.Tensor):\n",
    "            conditions = torch.tensor(conditions, dtype=torch.float32, device=device)\n",
    "\n",
    "        n_params = n_params_local*n_obs\n",
    "        theta = torch.randn(n_post_samples, n_obs, n_params_local, dtype=torch.float32, device=device)\n",
    "        conditions = (conditions - prior.prior_global_mean) / prior.prior_global_std\n",
    "        conditions_exp = conditions.unsqueeze(0).expand(n_post_samples, n_obs, -1).reshape(-1, n_params_global)\n",
    "\n",
    "    # Generate diffusion time parameters\n",
    "    diffusion_time, snr, _, _ = generate_diffusion_time(max_t=T, size=diffusion_steps, device=device)\n",
    "    alpha_sampling_noise = torch.sigmoid(snr)  # sometimes called 1-\\beta_t\n",
    "    sigma_sampling_noise = torch.sqrt(torch.sigmoid(-snr))  # sometimes \\sqrt(\\beta_t)\n",
    "\n",
    "    # Expand x_obs_norm to match the number of posterior samples\n",
    "    x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, n_obs, n_time_steps, -1)  # Shape: (n_post_samples, n_obs, n_time_steps, d)\n",
    "    x_expanded = x_exp.reshape(n_post_samples*n_obs, n_time_steps, -1)\n",
    "\n",
    "    # Reverse iterate over diffusion times and step sizes\n",
    "    with torch.no_grad():\n",
    "        for t in tqdm(reversed(range(diffusion_steps)), total=diffusion_steps):\n",
    "            # Create tensor for current time step\n",
    "            t_tensor = torch.full((n_post_samples, 1), diffusion_time[t], dtype=torch.float32, device=device)\n",
    "            t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "\n",
    "            # Compute model scores\n",
    "            if conditions is None:\n",
    "                theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, n_params_global)\n",
    "                model_scores_noise = model.forward_global(theta_global=theta_exp, t=t_exp, x=x_expanded)\n",
    "                # Sum over observations\n",
    "                model_scores_noise = model_scores_noise.reshape(n_post_samples, n_obs, -1).sum(dim=1)\n",
    "            else:\n",
    "                theta_exp = theta.reshape(-1, n_params_local)\n",
    "                model_scores_noise = model.forward_local(theta_local=theta_exp, t=t_exp, x=x_expanded, theta_global=conditions_exp)\n",
    "                model_scores_noise = model_scores_noise.reshape(n_post_samples, n_obs, -1)\n",
    "\n",
    "            # scaling since we learned the scaled score network (noise)\n",
    "            model_scores = -model_scores_noise  / sigma_sampling_noise[t]\n",
    "\n",
    "            # Compute updated scores\n",
    "            if conditions is None:\n",
    "                # Compute prior score\n",
    "                prior_score = prior.score_global_batch(theta)\n",
    "                w_scores = (1 - n_obs) * (T - diffusion_time[t]) / T * prior_score + model_scores\n",
    "            else:\n",
    "                w_scores = model_scores\n",
    "\n",
    "            # these are be the marginals to get from one time point to the next one p(x_{t-1}|x_t), not p(x_0|x_t)\n",
    "            if t == 0:\n",
    "                alpha_t = torch.tensor(1, dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                alpha_t = alpha_sampling_noise[t] / alpha_sampling_noise[t-1]\n",
    "            beta_t = 1 - alpha_t\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "\n",
    "            # Sample Gaussian noise\n",
    "            eps = torch.randn_like(theta, dtype=torch.float32, device=device) # set to 0 for the last step\n",
    "            # Make update\n",
    "            theta = 1 / torch.sqrt(alpha_t) * (theta - (1-alpha_t) * w_scores) + sigma_t * eps\n",
    "            if torch.isnan(theta).any():\n",
    "                print(\"NaNs in theta\")\n",
    "                break\n",
    "    # correct for normalization\n",
    "    if conditions is None:\n",
    "        theta = theta * prior.prior_global_std + prior.prior_global_mean\n",
    "    else:\n",
    "        theta = theta * prior.prior_local_std + prior.prior_local_mean\n",
    "    # convert to numpy\n",
    "    theta = theta.detach().numpy().reshape(n_post_samples, n_params)\n",
    "    return theta"
   ],
   "id": "c5a8acb1162985b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def euler_maruyama_step(z, score, t, dt):\n",
    "    \"\"\"\n",
    "    Perform one Euler-Maruyama update step for the SDE\n",
    "      dz = [f(z,t)- g(t)^2*s(z,t)] dt + g(t) dW_t,\n",
    "    with\n",
    "      f(z,t) = -0.5 * (d/dt log(1+e^{-lambda(t)})) * z,\n",
    "      g(t)^2 = d/dt log(1+e^{-lambda(t)}),\n",
    "    and\n",
    "      lambda(t) = -2 log(tan(pi t/2)) + 2s.\n",
    "\n",
    "    Parameters:\n",
    "        z: Current state (PyTorch tensor)\n",
    "        score: Score network output (PyTorch tensor)\n",
    "        t: Current time (scalar tensor or float)\n",
    "        dt: Time step size (float)\n",
    "\n",
    "    Returns:\n",
    "        z_next: Updated state after time dt.\n",
    "    \"\"\"\n",
    "    # Compute lambda(t) = -2 log(tan(pi*t/2)) + 2s.\n",
    "    lambda_t = cosine_schedule_signal_to_noise(t, max_t=T)\n",
    "    if torch.isnan(lambda_t).any():\n",
    "        print(\"NaNs in lambda_t\")\n",
    "\n",
    "    # Compute g_t2 = 2 pi / sin(pi t) * sigmoid(-lambda(t))\n",
    "    g_t2 = (2 * torch.pi / torch.sin(torch.pi * t)) * torch.sigmoid(-lambda_t)\n",
    "    if torch.isnan(g_t2).any():\n",
    "        print(\"NaNs in g_t2\")\n",
    "\n",
    "    # Drift: f(z,t) = -1/2 * g_t2 * z.\n",
    "    f_z = -0.5 * g_t2 * z\n",
    "    drift = f_z - 0.5 * g_t2 * score\n",
    "\n",
    "    # Diffusion: g(t) = sqrt(g_t2)\n",
    "    diffusion = torch.sqrt(g_t2)\n",
    "\n",
    "    # Sample Gaussian noise (same shape as z)\n",
    "    noise = torch.randn_like(z, dtype=z.dtype, device=z.device)\n",
    "\n",
    "    # Eulerâ€“Maruyama update:\n",
    "    if torch.isnan(drift * dt).any():\n",
    "        print(\"NaNs in drift\")\n",
    "    if torch.isnan(diffusion  * torch.sqrt(dt) * noise).any():\n",
    "        print(\"NaNs in diffusion\")\n",
    "    z_next = z + drift * dt + diffusion * torch.sqrt(dt) * noise\n",
    "    return z_next"
   ],
   "id": "61887926b5939d74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Euler-Maruyama Sampling from Song et al. (2021)\n",
    "def euler_maruyama_sampling(model, x_obs, n_post_samples, conditions=None, diffusion_steps=100, device=None):\n",
    "    x_obs_norm = (x_obs - prior.x_mean) / prior.x_std  # assumes x_obs is not standardized\n",
    "    x_obs_norm = x_obs_norm.reshape(x_obs_norm.shape[0], -1)\n",
    "    n_obs = x_obs_norm.shape[-1]\n",
    "    n_time_steps = x_obs_norm.shape[0]\n",
    "    x_obs_norm = x_obs_norm.T[:, :, np.newaxis]\n",
    "\n",
    "    # Ensure x_obs_norm is a PyTorch tensor\n",
    "    if not isinstance(x_obs_norm, torch.Tensor):\n",
    "        x_obs_norm = torch.tensor(x_obs_norm, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Initialize parameters\n",
    "    if conditions is None:  # global\n",
    "        n_params = n_params_global\n",
    "        theta = torch.randn(n_post_samples, n_params_global, dtype=torch.float32, device=device) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32, device=device))\n",
    "        conditions_exp = None\n",
    "    else:\n",
    "        # Ensure conditions is a PyTorch tensor\n",
    "        if not isinstance(conditions, torch.Tensor):\n",
    "            conditions = torch.tensor(conditions, dtype=torch.float32, device=device)\n",
    "\n",
    "        n_params = n_params_local*n_obs\n",
    "        theta = torch.randn(n_post_samples, n_obs, n_params_local, dtype=torch.float32, device=device)\n",
    "        conditions = (conditions - prior.prior_global_mean) / prior.prior_global_std\n",
    "        conditions_exp = conditions.unsqueeze(0).expand(n_post_samples, n_obs, -1).reshape(-1, n_params_global)\n",
    "\n",
    "    # Generate diffusion time parameters\n",
    "    diffusion_time, snr, _, _ = generate_diffusion_time(max_t=T, size=diffusion_steps, device=device)\n",
    "    sigma_sampling_noise = torch.sqrt(torch.sigmoid(-snr))  # sometimes \\sqrt(\\beta_t)\n",
    "\n",
    "    # Expand x_obs_norm to match the number of posterior samples\n",
    "    x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, n_obs, n_time_steps, -1)  # Shape: (n_post_samples, n_obs, n_time_steps, d)\n",
    "    x_expanded = x_exp.reshape(n_post_samples*n_obs, n_time_steps, -1)\n",
    "\n",
    "    # Reverse iterate over diffusion times and step sizes\n",
    "    with torch.no_grad():\n",
    "        for t in tqdm(reversed(range(diffusion_steps)), total=diffusion_steps):\n",
    "            # Create tensor for current time step\n",
    "            t_tensor = torch.full((n_post_samples, 1), diffusion_time[t], dtype=torch.float32, device=device)\n",
    "            t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "\n",
    "            # Compute model scores\n",
    "            if conditions is None:\n",
    "                theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, n_params_global)\n",
    "                model_scores_noise = model.forward_global(theta_global=theta_exp, t=t_exp, x=x_expanded)\n",
    "                # Sum over observations\n",
    "                model_scores_noise = model_scores_noise.reshape(n_post_samples, n_obs, -1).sum(dim=1)\n",
    "            else:\n",
    "                theta_exp = theta.reshape(-1, n_params_local)\n",
    "                model_scores_noise = model.forward_local(theta_local=theta_exp, t=t_exp, x=x_expanded, theta_global=conditions_exp)\n",
    "                model_scores_noise = model_scores_noise.reshape(n_post_samples, n_obs, -1)\n",
    "\n",
    "            # scaling since we learned the scaled score network (noise)\n",
    "            model_scores = -model_scores_noise / sigma_sampling_noise[t]\n",
    "            print('Scores', model_scores)\n",
    "\n",
    "            # Compute updated scores\n",
    "            if conditions is None:\n",
    "                # Compute prior score\n",
    "                prior_score = prior.score_global_batch(theta)\n",
    "                w_scores = (1 - n_obs) * (T - diffusion_time[t]) / T * prior_score + model_scores\n",
    "            else:\n",
    "                w_scores = model_scores\n",
    "\n",
    "            # Make Euler-Maruyama step\n",
    "            #print(diffusion_time[t] - diffusion_time[t-1], theta, w_scores)\n",
    "            if t == 0:\n",
    "                pass\n",
    "            else:\n",
    "                theta = euler_maruyama_step(theta, score=w_scores, t=t_tensor, dt=diffusion_time[t] - diffusion_time[t-1])\n",
    "            if torch.isnan(theta).any():\n",
    "                print(\"NaNs in theta\")\n",
    "                break\n",
    "    # correct for normalization\n",
    "    if conditions is None:\n",
    "        theta = theta * prior.prior_global_std + prior.prior_global_mean\n",
    "    else:\n",
    "        theta = theta * prior.prior_local_std + prior.prior_local_mean\n",
    "    # convert to numpy\n",
    "    theta = theta.detach().numpy().reshape(n_post_samples, n_params)\n",
    "    return theta"
   ],
   "id": "ffbd8811e27ede9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Annealed Langevin Dynamics for Sampling\n",
    "def langevin_sampling(model, x_obs, n_post_samples, conditions=None, steps=1, diffusion_steps=400, device=None):\n",
    "    x_obs_norm = (x_obs - prior.x_mean) / prior.x_std  # assumes x_obs is not standardized\n",
    "    x_obs_norm = x_obs_norm.reshape(x_obs_norm.shape[0], -1)\n",
    "    n_obs = x_obs_norm.shape[-1]\n",
    "    n_time_steps = x_obs_norm.shape[0]\n",
    "    x_obs_norm = x_obs_norm.T[:, :, np.newaxis]\n",
    "\n",
    "    # Ensure x_obs_norm is a PyTorch tensor\n",
    "    if not isinstance(x_obs_norm, torch.Tensor):\n",
    "        x_obs_norm = torch.tensor(x_obs_norm, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Initialize parameters\n",
    "    if conditions is None:  # global\n",
    "        n_params = n_params_global\n",
    "        theta = torch.randn(n_post_samples, n_params_global, dtype=torch.float32, device=device) / torch.sqrt(torch.tensor(n_obs, dtype=torch.float32, device=device))\n",
    "        conditions_exp = None\n",
    "    else:\n",
    "        # Ensure conditions is a PyTorch tensor\n",
    "        if not isinstance(conditions, torch.Tensor):\n",
    "            conditions = torch.tensor(conditions, dtype=torch.float32, device=device)\n",
    "\n",
    "        n_params = n_params_local*n_obs\n",
    "        theta = torch.randn(n_post_samples, n_obs, n_params_local, dtype=torch.float32, device=device)\n",
    "        conditions = (conditions - prior.prior_global_mean) / prior.prior_global_std\n",
    "        conditions_exp = conditions.unsqueeze(0).expand(n_post_samples, n_obs, -1).reshape(-1, n_params_global)\n",
    "\n",
    "    # Generate diffusion time parameters\n",
    "    diffusion_time, snr, weight_snr, _ = generate_diffusion_time(max_t=T, size=diffusion_steps, device=device)\n",
    "    scaling = torch.sqrt(torch.sigmoid(-snr))  # sometimes \\sqrt(\\beta_t)\n",
    "    delta_t = weight_snr / torch.sigmoid(-snr)  # w = beta_t/(1-gamma)\n",
    "\n",
    "    #diffusion_time, gamma, delta_t = generate_diffusion_time(max_t=T, steps=steps_time, device=device)\n",
    "\n",
    "    # Expand x_obs_norm to match the number of posterior samples\n",
    "    x_exp = x_obs_norm.unsqueeze(0).expand(n_post_samples, n_obs, n_time_steps, -1)  # Shape: (n_post_samples, n_obs, n_time_steps, d)\n",
    "    x_expanded = x_exp.reshape(n_post_samples*n_obs, n_time_steps, -1)\n",
    "\n",
    "    # Reverse iterate over diffusion times and step sizes\n",
    "    for step_size, t, scale in tqdm(zip(delta_t.flip(0), diffusion_time.flip(0), scaling.flip(0)), total=T):\n",
    "        # Create tensor for current time step\n",
    "        t_tensor = torch.full((n_post_samples, 1), t, dtype=torch.float32, device=device)\n",
    "        t_exp = t_tensor.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, 1)\n",
    "\n",
    "        for _ in range(steps):\n",
    "            # Sample Gaussian noise\n",
    "            eps = torch.randn_like(theta, dtype=torch.float32, device=device)\n",
    "\n",
    "            if conditions is None:\n",
    "                # Compute prior score\n",
    "                prior_score = prior.score_global_batch(theta)\n",
    "            else:\n",
    "                prior_score = 0\n",
    "\n",
    "            # Compute model scores\n",
    "            if conditions is None:\n",
    "                theta_exp = theta.unsqueeze(1).expand(-1, n_obs, -1).reshape(-1, n_params_global)\n",
    "                model_scores = model.forward_global(theta_global=theta_exp, t=t_exp, x=x_expanded)\n",
    "                # Sum over observations\n",
    "                model_scores = model_scores.reshape(n_post_samples, n_obs, -1).sum(dim=1)\n",
    "            else:\n",
    "                theta_exp = theta.reshape(-1, n_params_local)\n",
    "                model_scores = model.forward_local(theta_local=theta_exp, t=t_exp, x=x_expanded, theta_global=conditions_exp)\n",
    "                model_scores = model_scores.reshape(n_post_samples, n_obs, -1)\n",
    "\n",
    "            # Compute updated scores and perform Langevin step\n",
    "            # scaling since we learned the scaled score network (noise)\n",
    "            scores = (1 - n_obs) * (T - t) / T * prior_score + scale * model_scores\n",
    "            theta = theta + (step_size / 2) * scores + torch.sqrt(step_size) * eps\n",
    "        if torch.isnan(theta).any():\n",
    "            print(\"NaNs in theta\")\n",
    "            break\n",
    "    # correct for normalization\n",
    "    if conditions is None:\n",
    "        theta = theta * prior.prior_global_std + prior.prior_global_mean\n",
    "    else:\n",
    "        theta = theta * prior.prior_local_std + prior.prior_local_mean\n",
    "    # convert to numpy\n",
    "    theta = theta.detach().numpy().reshape(n_post_samples, n_params)\n",
    "    return theta"
   ],
   "id": "2112764f9c4ea368",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validation",
   "id": "157900e315686835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_grid = 8\n",
    "valid_prior_global, valid_prior_local, valid_data = generate_synthetic_data(2, n_grid=n_grid, full_grid=True, device=torch_device)\n",
    "n_post_samples = 20"
   ],
   "id": "28d63d6aa09d9245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_global_samples_valid = np.array([ddpm_sampling(score_model, vd, n_post_samples=n_post_samples,\n",
    "                                                                   diffusion_steps=T,\n",
    "                                                          device=torch_device)\n",
    "                                        for vd in valid_data])"
   ],
   "id": "d700cbffbf4d35c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_global_samples_valid",
   "id": "be398ecac64f872e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diagnostics.plot_recovery(posterior_global_samples_valid, np.array(valid_prior_global), param_names=[r'$\\mu$', r'$\\log \\tau$']);",
   "id": "afe9df5c1ca137da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_global_samples_valid, np.array(valid_prior_global),\n",
    "                          difference=True, param_names=[r'$\\mu$', r'$\\log \\tau$']);"
   ],
   "id": "c12c415e037a9277",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conditions_global = valid_prior_global #np.median(posterior_global_samples_valid, axis=0)\n",
    "posterior_local_samples_valid = np.array([euler_maruyama_sampling(score_model, vd, n_post_samples=n_post_samples, conditions=c,\n",
    "                                                             diffusion_steps=10,\n",
    "                                                          device=torch_device)\n",
    "                                        for vd, c in zip(valid_data, conditions_global)])"
   ],
   "id": "fb0c12e449219594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_recovery(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          param_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "44320e5b15ea6349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics.plot_sbc_ecdf(posterior_local_samples_valid.reshape(valid_data.shape[0], n_post_samples, -1),\n",
    "                          np.array(valid_prior_local).reshape(valid_data.shape[0], -1),\n",
    "                          difference=True, stacked=True,\n",
    "                          param_names=['$\\\\theta_{'+str(i)+'}$' for i in range(n_grid**2)]);"
   ],
   "id": "1708cd7d0c011284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_id = 1\n",
    "print('Data')\n",
    "visualize_simulation_output(valid_data[valid_id])\n",
    "print('Global Estimates')\n",
    "print('mu:', np.median(posterior_global_samples_valid[valid_id, :, 0]), np.std(posterior_global_samples_valid[valid_id, :, 0]))\n",
    "print('log tau:', np.median(posterior_global_samples_valid[valid_id, :, 1]), np.std(posterior_global_samples_valid[valid_id, :, 1]))\n",
    "print('True')\n",
    "print('mu:', valid_prior_global[valid_id][0].item())\n",
    "print('log tau:', valid_prior_global[valid_id][1].item())"
   ],
   "id": "c4bb5620888ab007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "med = np.median(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "std = np.std(posterior_local_samples_valid[valid_id].reshape(n_post_samples, n_grid, n_grid), axis=0)\n",
    "cat = np.stack((med, std, valid_prior_local[valid_id]))\n",
    "visualize_simulation_output(cat, title_prefix=['Posterior Median\\n', 'Posterior Std\\nUncertainty', 'True\\n'])"
   ],
   "id": "c9daf84cada1b0a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fb502aa27eeab6e6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
